{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/intro/title_page.png\" style=\"display: block; margin: 0 auto; width: 100%;\">\n",
        "</figure>\n",
        "\n",
        "### Probability Concepts\n",
        "\n",
        "Michael J. Pyrcz, Professor, The University of Texas at Austin \n",
        "\n",
        "[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
        "\n",
        "Chapter of e-book \"Applied Machine Learning in Python: a Hands-on Guide with Code\". \n",
        "\n",
        "```{admonition} Cite this e-Book as:\n",
        ":class: remove-from-content-only\n",
        "\n",
        "Pyrcz, M. J., 2024, Applied Machine Learning in Python: A Hands-on Guide with Code. GitHub repository. Zenodo. DOI: 10.5281/zenodo.15169138 [![DOI](https://zenodo.org/badge/863274676.svg)](https://doi.org/10.5281/zenodo.15169138) \n",
        "```\n",
        "\n",
        "The workflows in this book and more are available here:\n",
        "\n",
        "```{admonition} Cite the MachineLearningDemos GitHub Repository as:\n",
        ":class: remove-from-content-only\n",
        "\n",
        "Pyrcz, M.J., 2024, MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository (0.0.1). Zenodo. DOI: 10.5281/zenodo.13835312  [![DOI](https://zenodo.org/badge/862519860.svg)](https://zenodo.org/doi/10.5281/zenodo.13835312)\n",
        "```\n",
        "\n",
        "By Michael J. Pyrcz <br />\n",
        "&copy; Copyright 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This chapter is a summary of **Probability Concepts** including essential concepts:\n",
        "\n",
        "* Motivation and Application to Model Uncertainty\n",
        "* Approaches to Calculate Probability\n",
        "* Probability Operators\n",
        "* Marginal, Conditional and Joint Probability\n",
        "* Independence Checks\n",
        "* Bayesian Updating\n",
        "\n",
        "**YouTube Lecture**: check out my lecture on:\n",
        "\n",
        "* [Probability and Statistics](https://youtu.be/jl14s8jvXcc?si=TA1YAG_LVWAXMeik)\n",
        "\n",
        "I also have more comprehensive probability content from my Data Analytics and Geostatistics course:\n",
        "\n",
        "* [Probability](https://youtu.be/IGPayWv1BBM?si=K2zI0qBQV3FvJ9fM)\n",
        "* [Frequentist Probability](https://youtu.be/NSvyljWT4mw?si=c0HepAkQwLDx3TCb)\n",
        "* [Bayesian Probability](https://youtu.be/Ppwfr8H177M?si=NYBOi8zTCAxJEpGl)\n",
        "* [Joint, Marginal, and Conditional Probability](https://youtu.be/kxjnPVyuuo8?si=FI3Tiu72Wc5Neunm)\n",
        "* [Joint, Marginal, and Conditional Probability](https://youtu.be/kxjnPVyuuo8?si=FI3Tiu72Wc5Neunm)\n",
        "* [Bayesian Coin Example](https://youtu.be/D1UKZGOYDOg?si=uFSAB0xLWsr80TYj)\n",
        "\n",
        "For convenience here's a summary of the salient points.\n",
        "\n",
        "#### Motivation for Probability\n",
        "\n",
        "Why cover probability at the beginning of an e-book or course on machine learning?\n",
        "\n",
        "1. **Model Formulation** - many of our machine learning models are formulated with probability concepts, for example, naive Bayes classification is derived from Bayes\u2019 Theorem and Bayesian linear regression estimates the probability distributions for our model parameters.\n",
        "\n",
        "2. **Data Cleaning and Preparation** - is 90% of any machine learning workflow and we cannot complete these steps without understanding our data distributions and statistics and all of these are based on probability.\n",
        "\n",
        "3. **Loss Functions and Optimization** - many of our machine learning models are trained through optimization of a loss function that relies on probability for stochastic steps or even directly in the loss function as is the case for maximum likelihood estimation.\n",
        "\n",
        "4. **Tuning Machine Learning Models** - machine learning model tuning to reduce model overfit is based on the concept of expected model performance in the presence of various uncertainty sources, and probability is the language of statistical expectation and uncertainty.\n",
        "\n",
        "5. **Machine Learning Model Choice** - You will make choices between frequentist and Bayesian predictive machine learning models and their differences are the result of two distinct approaches to calculate probability and build models.\n",
        "\n",
        "6. **Real World Applications** - to make the best choices in machine learning workflows design and to use our results to support decisions we must integrate probability concepts Using our models in the real world.\n",
        "\n",
        "Let us build our machine learning skills on a solid foundation of probability!\n",
        "\n",
        "```{admonition} Probability\n",
        ":class: remove-from-content-only\n",
        "Probability is an essential prerequisite for machine learning.\n",
        "```\n",
        "\n",
        "Now we get started by defining probability and then we will be ready to talk about ways to calculate it.\n",
        "\n",
        "#### What is Probability?\n",
        "\n",
        "To understand what is probability consider Kolmogorov's 3 axioms for probabilities, i.e., the rules that any measure of probability must honor,\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/kolmogorov.png\" style=\"display: block; margin: 0 auto; width: 30%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Andrey Kolmogorov (1903 \u2013 1987), Soviet mathematician, photo taken 1972 and from image from https://culturemath.ens.fr/thematiques/biographie/life-and-work-kolmogorov).\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "1. **non-negativity** - probability of an event is a non-negative number\n",
        "\n",
        "$$\n",
        "P(\ud835\udc34) \\ge 0\n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; imagine negative probability!\n",
        "\n",
        "2. **normalization** - probability of the entire sample space is one (unity), also known as probability closure\n",
        "\n",
        "$$\n",
        "P(\\Omega) = 1\n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; something happens!\n",
        "\n",
        "3. **additivity** - addition of probability of mutually exclusive events for unions\n",
        "\n",
        "$$\n",
        "P\\left(\u22c3_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i)\n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; e.g., probability of $A_1$ and $A_2$ mutual exclusive events is, $Prob(A_1 + A_2) = Prob(A_1) + Prob(A_2)$\n",
        "\n",
        "* note our concise notation, $P(\\cdot)$, for probability of event in the bracket occurring.\n",
        "\n",
        "This is a good place to start for valid probabilities, we will refine this later. Now we can ask,\n",
        "\n",
        "#### How to Calculate Probability?\n",
        "\n",
        "There are 3 probability perspectives that can be applied to calculate probabilities,\n",
        "\n",
        "1. **Probability by long-term frequencies** (Frequentist Probability),\n",
        "\n",
        "* probability as ratio of outcomes from an experiment\n",
        "\n",
        "* requires repeated observations from a controlled experiment\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for example, you flip a coin 100 times and count the number of outcomes with heads $n(\\text{heads})$ and then calculate this ratio,\n",
        "\n",
        "$$\n",
        "P(\\text{heads}) = \\frac{n(\\text{heads})}{n}\n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; this is the frequentist approach to calculate probabilities. The experiment is the set of coin tosses. \n",
        "\n",
        "* one issue with frequentist probabilities is, can we now use this probability of heads outside the experiment? For example, \n",
        "\n",
        "    * on another day? \n",
        "    \n",
        "    * a different person tossing the coin?\n",
        "    \n",
        "    * a different coin?\n",
        "    \n",
        "2. **Probability by physical tendencies or propensities** (Engineering Probability),\n",
        "\n",
        "* probability calculated from knowledge about the system\n",
        "\n",
        "* we could know the probability of coin toss outcomes without the experiment\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; this is the engineering approach to probability, model the system and use this model, i.e., the physics of the system to calculate probability of outcomes\n",
        "\n",
        "* did you know that there is a $\\frac{1}{6,000}$ probability of a coin landing and staying upright on its edge? This could  be lower depending of manner of tossing, coin thickness and characteristics of the surface.\n",
        "\n",
        "3. **Probability by Degrees of belief** (Bayesian Probability), \n",
        "\n",
        "* first we specify the scientific concept of \"belief\" as your opinion based on all your knowledge and experience. \n",
        "\n",
        "* our model integrates our certainty about a result and data\n",
        "\n",
        "* this is very flexible, we can assign probability to any event, and includes a framework for updating with new information\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if this is your approach, then you are using the Bayesian approach for probability. If not, before you dismiss this approach let me make a couple arguments,\n",
        "\n",
        "* you may be bothered by this idea of belief, as it may seem subjective compared to the probabilities objectively measured from a frequentist's experiment but, \n",
        "\n",
        "    * to use the frequentist probability you have to make the subjective decision to apply it outside the experiment, i.e., we need to go beyond a single coin!\n",
        "    \n",
        "    * the Bayesian probability approach includes objective probabilities from experiments, but it also allows for integration of our expert knowledge\n",
        "\n",
        "#### A Warning about Calculating Probability\n",
        "\n",
        "Statistics can be misused or even abused, leading to flawed conclusions and poor decision-making. When probabilities are calculated improperly or misinterpreted, it can result in significant consequences. Here are a few examples of what can go wrong:\n",
        "\n",
        "1. **Insufficient sampling** - there are various rules of thumb about what constitutes a small sample size, i.e., too small for inference about the population parameters and too small to train a reliable prediction model. Some say 7, some say 30, indubitably there is a minimum sample size.\n",
        "\n",
        "* in general, our models will communicate increasing uncertainty as the number of data decreases, but at some point these models break! \n",
        "\n",
        "* consider bootstrap, yes as the number of samples decrease the uncertainty in the statistic reported by bootstrap increases, but this uncertainty distribution is centered on the sample statistic! In other words, we need enough samples to have a reasonable estimate of the uncertainty model's expectation in the first place.\n",
        "\n",
        "* of course, there is minimum number of samples to complete a mathematical operation, for example principal components analysis can only calculate $n-1$ principal components (where $n$ is the number of samples) and we cannot fit a linear regression model to $n=1$ samples. Best practice is to stay very far away (have many more samples) from any of these algorithm limitations.\n",
        "\n",
        "2. **Biased sampling** - in general our probability calculations will not automatically debias the sample data. Any bias in the samples will pass bias through to the probabilities representing our uncertainty model. \n",
        "\n",
        "* for example, a biased sample mean will bias simple kriging estimates away from data. Geostatistical simulation reproduces the entire input feature distribution, so any bias in any part of the distribution will be passed to the spatial models.\n",
        "\n",
        "* also, there is bad data. When we use Bayesian methods and rely on expert experience, that is not a license to say anything and defend it as belief. On the contrary, we must rigorously document and defend all our choices\n",
        "\n",
        "3. **Unskilled practice and a lack of rigor** - there are mistakes that can be made with probabilities and some of them are shockingly common. \n",
        "\n",
        "* our first line of defense is to understand what our methods are doing under the hood! This will help us recognize logical inconsistencies as we build our workflows. \n",
        "\n",
        "* our second line of defense is to check every step in our workflows. Like an accountant we must close the loops with all our probability calculations, a process that accountants call reconciliation. Like a software engineer we must unit test every operation to ensure we don't introduce errors as we update our probability workflows.\n",
        "\n",
        "* for example, if we perform primary feature cosimulation with the same random number seed as the secondary feature simulation we will introduce artificial correlation between the simulated primary and secondary features that will dramatically change the conditional and joint probabilities.\n",
        "\n",
        "* in another example, if we use information from the likelihood probabilities to inform the prior probabilities, we will significantly under-estimate the uncertainty. This error is a form of information leakage, descriptively we state it as \"double dipping\" from the information.\n",
        "\n",
        "Please remember these warnings as you proceed below and onward as you build your own data science workflows. \n",
        "\n",
        "\n",
        "#### Venn Diagrams\n",
        "\n",
        "Venn Diagrams are a tool to communicate probability.\n",
        "\n",
        "* representing the possible events or outcomes ($A, B,\\ldots$) of an experiment within the collection of all possible events or outcomes, the sample space ($\\Omega$).\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Simple example Venn diagram.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "What do we learn from this Venn diagram?\n",
        "\n",
        "1. the relative size of regions for each event within is the relative probability of occurrence, probability of $B$ is greater than probability of $A$\n",
        "\n",
        "2. the overlap over events is the probability of joint occurrence, there is 0.0 probability of $A$ and $B$ occurring together\n",
        "\n",
        "Let us include a more practical Venn diagram to ensure this concept is not too abstract. Here's a Venn diagram from 3,000 core samples with interpreted facies\n",
        "\n",
        "* the events are sandstone (Sm) and mudstone (Fm)\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_rock.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing the facies assignments for 3,000 core samples.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "What do we learn from this Venn diagram?\n",
        "\n",
        "* mudstone is more likely than sandstone over these core samples\n",
        "\n",
        "* there are a lot of samples that are neither sandstone nor mudstone, the white space within $\\Omega$\n",
        "\n",
        "* there are samples that are interpreted as both sandstone and mudstone, i.e., interbedded sandstone-mudstone\n",
        "\n",
        "* do not forget to draw and label the $\\Omega$ box or we can't understand the relative probability of the events\n",
        "\n",
        "* we can use any convenient shape to represent an event\n",
        "\n",
        "In summary, Venn diagrams are an excellent tool to visualize probability, so we will use them to visualize and teach probability here.\n",
        "\n",
        "#### Frequentist Probability\n",
        "\n",
        "We now provide an extended definition for the frequentist approach for probability. A measure of the likelihood that an event will occur based on frequencies observed from an experiment.\n",
        "\n",
        "* For random experiments and well-defined settings, we calculate probability as: \n",
        "\n",
        "$$\n",
        "P(A) = \\lim_{n \\to \\infty} \\frac{n(A)}{n}\n",
        "$$\n",
        "\n",
        "where: \n",
        "\n",
        "$n(A)$ = number of times event $A$ occurred\n",
        "$n$ = number of trails\n",
        "\n",
        "* we use limit notation above to indicate sufficient sampling and that the solution converges and improves accuracy as we introduce more samples from our experiment\n",
        "\n",
        "For example, \n",
        "\n",
        "* probability of drilling a dry hole, encountering sandstone, and exceeding a rock porosity of $15\\%$ at a location ($\\bf{u}_{\\alpha}$) based on historical results for drilling in this reservoir\n",
        "\n",
        "Now we walk-through various probability operations from the frequentist perspective, may I ask for patience from my Bayesian friends as we will later return to the Bayesian perspective for probability.\n",
        "\n",
        "#### Probability Operations\n",
        "\n",
        "**Union of Events** - for example, all outcomes in the sample space that belong to either event $A$ or $B$\n",
        "\n",
        "* for the union of events operator, we use the word \"or\" and the mathematics symbol, $cup$, using set notation we state the samples in the union $A$ or $B$ as,\n",
        "\n",
        "$$\n",
        "A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\}\n",
        "$$\n",
        "\n",
        "* and the probability notation for a union as,\n",
        "\n",
        "$$\n",
        "P(A \\cup B)\n",
        "$$\n",
        "\n",
        "Here's a Venn diagram illustrating the union $P(A \\cup B)$. \n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_union.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing union probability operator for events \\(A\\) or \\(B\\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "**Intersection of Events** - for example, all outcomes in the sample space that belong to both events $A$ and $B$\n",
        "\n",
        "* for the intersection of events operator, we use the word \"and\" and the mathematics symbol $cap$, using set notation we state the samples in the intersection $A$ and $B$ as,\n",
        "\n",
        "$$\n",
        "A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\}\n",
        "$$\n",
        "\n",
        "* and the probability notation for an intersection as,\n",
        "\n",
        "$$\n",
        "P(A \\cap B) \n",
        "$$ \n",
        "\n",
        "* or with the common probability shorthand as,\n",
        "\n",
        "$$\n",
        "P(A,B)\n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; we will call this a joint probability later. Here is a Venn diagram illustrating the intersection $P(A \\cap B)$. \n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_intersection.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing intersection probability operator for events \\(A\\) and \\(B\\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "**Compliment of Events** - for example, all outcomes in the sample space that do not belong to event $A$\n",
        "\n",
        "* for the compliment of event(s) operator we use the word \"not\" and the mathematics symbol $^c$, using set notation, we state the samples in the compliment of $A$ as,\n",
        "\n",
        "$$\n",
        "A^c = \\{x: x \\notin A\\}\n",
        "$$\n",
        "\n",
        "* and the probability notation for a compliment as,\n",
        "\n",
        "$$\n",
        "P(A^c) \n",
        "$$ \n",
        "\n",
        "Here's a Venn diagram illustrating the compliment $P(A^c)$.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_compliment.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing probability compliment operator for events \\(A\\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "**Mutually Exclusive Events** - for example, events do not intersect or do not have any common outcomes, $A$ and $B$ do not occur at the same time. \n",
        "\n",
        "* using set notation, we state events $A$ and $B$ are mutually exclusive as,\n",
        "\n",
        "$$\n",
        "A \\cap B = \\{x: x \\in A \\text{ and } x \\in B \\} = \\emptyset\n",
        "$$\n",
        "\n",
        "* and the probability notation for mutually exclusive as,\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = 0.0\n",
        "$$ \n",
        "\n",
        "* or with the common probability shorthand as,\n",
        "\n",
        "$$\n",
        "P(A,B) = 0.0\n",
        "$$ \n",
        "\n",
        "Here's a Venn diagram illustrating mutual exclusive events $A$ and $B$.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing events \\(A\\) and \\(B\\) as mutually exclusive.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "**Exhaustive, Mutually Exclusive Events** the sequence of events whose union is equal to the sample space, all-possible events ($\\Omega$) and there is no intersection between the events:\n",
        "\n",
        "\n",
        "* using set notation, we state events $A$ and $B$ are exhaustive as,\n",
        "\n",
        "$$\n",
        "\\{x: x \\in A \\text{ or } x \\in B \\} = \\Omega\n",
        "$$\n",
        "\n",
        "* and events $A$ and $B$ are mutually exclusive as,\n",
        "\n",
        "$$\n",
        "\\{x: x \\in A \\text{ and } x \\in B \\} = \\emptyset\n",
        "$$\n",
        "\n",
        "* and the probability notation for exhaustive events as,\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = 1.0\n",
        "$$ \n",
        "\n",
        "* and for mutually exhaustive events as,\n",
        "\n",
        "$$\n",
        "P(A,B) = 0.0\n",
        "$$ \n",
        "\n",
        "Here's a Venn diagram illustrating mutual exclusive, exhaustive events $A$ and $B$.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_mutual.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing events \\(A\\) and \\(B\\) as exhaustive, mutually exclusive.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "**Combinations of Operators** - we can use these probability operators with any number of events to communicate complicated probability cases. For example, let us define these events,\n",
        "\n",
        "* Event $A$: oil present ($A^c$: dry hole)\n",
        "\n",
        "* Event $B$: \ud835\udc46\ud835\udc5a ($B^c$: \ud835\udc39\ud835\udc5a)\n",
        "\n",
        "* Event $C$: porosity \u2265 15% ($C^c$: porosity < 15%)\n",
        "\n",
        "What is the probability of dry hole with massive sandstone (\ud835\udc46\ud835\udc5a) and porosity \u2265 15%? \n",
        "\n",
        "$$\n",
        "P(A^c \\cap B \\cap C) = \\frac{\\text{Area}(A^c \\cap B \\cap C)}{\\text{Area}(\\Omega)}\n",
        "$$\n",
        "\n",
        "Here's a Venn diagram representing this case.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_complicated.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing a more complicated case with 3 events, \\(A, B, C\\), with compliment and intersection probability operators.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "#### Constraints on Probability\n",
        "\n",
        "Now that we have defined probability notation and probability operations, now we return the idea of permissible probability values expressed by Kolmogorov. We can now build on Kolmogorov's probability axioms with this set of probability constraints,\n",
        "\n",
        "Non-negativity, Normalization constraints include,\n",
        "\n",
        "* Probability is bounded,\n",
        "\n",
        "$$\n",
        "0.0 \\le P(A) \\le 1.0\n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; probability must be between 0.0 and 1.0 (including 0.0 and 1.0)\n",
        "\n",
        "* Probability closure,\n",
        "\n",
        "$$\n",
        "P(\\Omega) = 1.0\n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; probability of any event is 1.0\n",
        "\n",
        "$$\n",
        "P(A) + P(A^c) = 1.0 \n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; probability of A or not A is 1.0\n",
        "\n",
        "* Probability for null sets,\n",
        "\n",
        "$$\n",
        "P(\\emptyset) = 0.0\n",
        "$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; probability of nothing happens is zero\n",
        "\n",
        "Now we use these probability concepts and notation to append more essential complicated probability concepts.\n",
        "\n",
        "#### Probability Addition Rule\n",
        "\n",
        "What is the probability for a union of events? Remember that union is an \"or\" operation represented by the $\\cup$ notation. Consider,\n",
        "\n",
        "$$\n",
        "P(A \\cup B)\n",
        "$$\n",
        "\n",
        "inspection of the previously shown Venn diagram indicates that we can not calculate the union , $P(A \\cup B)$, by just summing $P(A)$ and $P(B)$.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_intersection.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing events \\(A\\) and \\(B\\), showing the intersection of \\(A\\) and \\(B\\) that will be double counted if we calculate the union and \\(A\\) or \\(B\\) as the sum of \\(A\\) and \\(B\\). Note, the legend on the top right is included to clarify the plot but is not part of the Venn diagram.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "The sum of the probabilities $P(A)$ and $P(B)$ will double count the intersection, $P(A \\cap B)$, so we must subtract it,\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n",
        "$$\n",
        "\n",
        "Yes, there is a general expression for any number of events to calculate the probability of the union,\n",
        "\n",
        "* I'm not going to include it here but suffice it to say that it is a book keeping nightmare given the combinatorial of intersections that can be counted too many times!\n",
        "\n",
        "There is a much simpler case. If the events are mutually exclusive then there is no intersection, $P(A,B) = 0.0$, and we can just sum the probabilities. \n",
        "\n",
        "* for the general case of mutually exclusive for any number of events,\n",
        "\n",
        "$$\n",
        "A_i \\cap A_j = \\emptyset, \\quad \\forall \\quad i \\ne j\n",
        "$$\n",
        "\n",
        "then we can write this general equation for the addition rule for any number of mutually exclusive events,\n",
        "\n",
        "$$\n",
        "P\\left( \\bigcup_{i=1}^k A_i \\right) = \\sum_{i=1}^k P(A_i)\n",
        "$$\n",
        "\n",
        "Here's a Venn diagram showing 4 mutual exclusive events, $A_1, A_2, A_3, A_4$.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_addition.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A Venn diagram representing 4 events, \\(A_1, A_2, A_3, A_4\\), that are all mutually exclusive. We can calculate the probability of the union of these events as the sum of probabilities of each.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "#### Conditional Probability \n",
        "\n",
        "What is the probability of an event given another event has occurred? To discuss this, let us get more specific, what is the probability of event $B$ given event $A$ has occurred? \n",
        "\n",
        "* to express this we use the notation, $P(B|A)$\n",
        "\n",
        "* we read this notation as, \"probability of B given A\"\n",
        "\n",
        "This is an example of a conditional probability. We calculate conditional probability with this equation,\n",
        "\n",
        "$$\n",
        "P(B|A) = \\frac{P(A \\cap B)}{P(A)}\n",
        "$$\n",
        "\n",
        "This may seem complicated, but we can easily visualize and understand this equation from our Venn diagram. \n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_conditional1.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> To calculate conditional probability we \"shrink our universe\" to the given condition. For the probability of $B$ given $A$ we shrink our Venn diagram to \\(A\\).\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "after we shrink our universe, our $\\Omega$ is only event $A$! Now we can easily see that the conditional probability is simply the probability of the intersection of $A$ and $B$ divided by the probability of $A$.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_conditional2.png\" style=\"display: block; margin: 0 auto; width: 40%;\">\n",
        "  <figcaption style=\"text-align: center;\"> After we \"shrink our universe\" to the condition, we can see conditional probability equation clearly as the intersection of \\(A\\) and \\(B\\) divided by the probability of \\(A\\).\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "Now we introduce a couple examples to test our knowledge of conditional probability,\n",
        "\n",
        "**Conditional Probability Example \\#1** - for this Venn diagram provide the conditional probabilities, $P(A|B)$ and $P(B|A)$, in terms of $A$ and $B$. \n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_conditional_ex1.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Venn diagram to check your understanding of conditional probability.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "This is simple to solve, the answer is 0.0 for both conditional probability, because to calculate conditional probability the numerator is the probability, $P(A,B)$, and this is 0.0 if there is no overlap in the Venn diagram.\n",
        "\n",
        "* also, since $P(B,A) = P(B,A)$ the numerator is the same for both conditional probabilities,\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B,A)}{P(B)} = \\frac{0.0}{P(B)} = 0.0\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(B|A) = \\frac{P(A,B)}{P(A)} = \\frac{0.0}{P(A)} = 0.0\n",
        "$$\n",
        "\n",
        "Now we attempt a more challenging example.\n",
        "\n",
        "**Conditional Probability Example \\#2** - for this Venn diagram provide the conditional probabilities, $P(A|B)$ and $P(B|A)$, in terms of $A$ and $B$. \n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_conditional_ex2.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Venn diagram to check your understanding of conditional probability.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "This one is more interesting. Let us look at each conditional probability one at a time.\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B,A)}{P(B)}\n",
        "$$\n",
        "\n",
        "notice that $P(B,A) = P(B)$ since all $B$ is inside $A$, so we can substitute $P(B)$ for $P(B,A)$ above,\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B)}{P(B)} = 1.0\n",
        "$$\n",
        "\n",
        "this makes sense, given $B$ then $A$ must occur, the conditional probability is 1.0. Now we calculate the other conditional probability,\n",
        "\n",
        "$$\n",
        "P(B|A) = \\frac{P(A,B)}{P(A)} = \\frac{P(B,A)}{P(A)} \n",
        "$$\n",
        "\n",
        "just a reminder that $P(B,A) = P(A,B)$ so we can use our previous result and substitute, $P(B)$ for $P(B,A)$ above.\n",
        "\n",
        "$$\n",
        "P(B|A) = \\frac{P(A,B)}{P(A)} = \\frac{P(B,A)}{P(A)} = \\frac{P(B)}{P(A)}\n",
        "$$ \n",
        "\n",
        "If you understand these two examples, then you have a good foundation in conditional probability,\n",
        "\n",
        "* and I can challenge you with a more complicated without conditional probabilities and learn an interesting general case! \n",
        "\n",
        "* let us take this Venn diagram with 3 events ($A, B, C$) and all possible event intersections labeled.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/venn_conditional3.png\" style=\"display: block; margin: 0 auto; width: 50%;\">\n",
        "  <figcaption style=\"text-align: center;\"> A more complicated case of conditional probability dealing with 3 events, \\(A, B, C\\).\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "now we calculate the conditional probability,\n",
        "\n",
        "$$\n",
        "P(C|B \\cap A) = \\frac{P(A \\cap B \\cap C}{P(A \\cap B}\n",
        "$$\n",
        "\n",
        "Recalling the definition of conditional probability, \n",
        "\n",
        "$$\n",
        "P(B|A) = \\frac{P(A \\cap B)}{P(A)}\n",
        "$$\n",
        "\n",
        "so we can reorder this to get,\n",
        "\n",
        "$$\n",
        "\\frac{P(A \\cap B)} = P(B|A) \\cdot {P(A)}\n",
        "$$\n",
        "\n",
        "spoiler alert, later we will define this at the multiplication rule, but for now we can substitute this as the denominator for the conditional probability, $P(C|B \\cap A)$, to get,\n",
        "\n",
        "$$\n",
        "P(C|B \\cap A) = \\frac{P(A \\cap B \\cap C}{P(B|A) \\cdot P(A)}\n",
        "$$\n",
        "\n",
        "now we can reorder this as,\n",
        "\n",
        "$$\n",
        "P(A \\cap B \\cap C) = P(C|B \\cap A) \\cdot P(B|A) \\cdot P(A)\n",
        "$$\n",
        "\n",
        "Do you see the pattern? We can calculate high order probability intersections as a sequential set, product sum of marginal probability and then growing conditional probabilities. Let me state it with words,\n",
        "\n",
        "* probability of $A$, $B$ and $C$ is the probability of $A$ times the probability of $B$ given $A$, times the probability of $C$ given $B$ and $A$.\n",
        "\n",
        "In fact, we can generalize this for any number of events as,\n",
        "\n",
        "$$\n",
        "P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = P(A_n|A_{n-1} \\cap \\ldots \\cap A_1) \\cdot P(A_{n-1}|A_{n-2} \\cap \\ldots \\cap A_1) \\cdot \\ldots \\cdot P(A_2|A_1) \\cdot P(A_1)\n",
        "$$\n",
        "\n",
        "or more compactly,\n",
        "\n",
        "$$\n",
        "P(A_1, A_2, \\ldots , A_n) = P(A_n|A_{n-1}, \\ldots , A_1) \\cdot P(A_{n-1}|A_{n-2}, \\ldots , A_1) \\cdot \\ldots \\cdot P(A_2|A_1) \\cdot P(A_1)\n",
        "$$\n",
        "\n",
        "Are we just showing off now? No! This example teaches us a lot about how conditional probabilities work.\n",
        "\n",
        "* also, this is exactly the concept used with sequential Gaussian simulation where we sample sequentially from conditional distributions instead of from the high order joint probability, an impossible task without the sequential approach!\n",
        "\n",
        "Now we are ready to summarize marginal, conditional and joint probabilities together.\n",
        "\n",
        "#### Marginal, Conditional and Joint Probabilities\n",
        "\n",
        "Now we define marginal, conditional and joint probability, provide notation and discuss how to calculate them:\n",
        "\n",
        "**Marginal Probability** - probability of an event, irrespective of any other event,\n",
        "\n",
        "$$\n",
        "P(A), P(B)\n",
        "$$\n",
        "\n",
        "* marginal probabilities are calculated as,\n",
        "\n",
        "$$\n",
        "P(A) = \\frac{n(A)}{n(\\Omega)}\n",
        "$$\n",
        "\n",
        "* marginal probabilities may be calculated from joint probabilities through the process of marginalization,\n",
        "\n",
        "$$\n",
        "P(A) = \\int_{-\\infty}^{\\infty} P(A,B) dB\n",
        "$$\n",
        "\n",
        "* where we integrate over all cases of the other events, $B$, to remove them.\n",
        "\n",
        "* given discrete cases of event $B$ we can simply sum the probabilities over all cases of $B$,\n",
        "\n",
        "$$\n",
        "P(A) = \\sum_{i=1}^{k_B} P(A,B)\n",
        "$$\n",
        "\n",
        "**Conditional Probability** - probability of an event, given another event has already occurred,\n",
        "\n",
        "$$\n",
        "P(A \\text{ given } B), P(B \\text{ given } A)\n",
        "$$\n",
        "\n",
        "* or with compaction probability notation, \n",
        "\n",
        "$$\n",
        "P(A|B), P(B|A)\n",
        "$$\n",
        "\n",
        "* see the previous section for more information, but for symmetry we repeat the method to calculate conditional probability as,\n",
        "\n",
        "$$\n",
        "P(B|A) = \\frac{P(A,B)}{P(A)}\n",
        "$$\n",
        "\n",
        "* of course, as we saw in our conditional probability examples above, we cannot switch the order of the events,\n",
        "\n",
        "$$\n",
        "P(B|A) \\ne P(A|B)\n",
        "$$\n",
        "\n",
        "\n",
        "**Joint Probability** - probability of multiple events occurring together,\n",
        "\n",
        "$$\n",
        "P(A \\text{ and } B), P(B \\text{ and } A)\n",
        "$$\n",
        "\n",
        "* or with compaction probability notation, \n",
        "\n",
        "$$\n",
        "P(A \\cap B), P(B \\cap A)\n",
        "$$\n",
        "\n",
        "* or even more compact as,\n",
        "\n",
        "$$\n",
        "P(A,B), P(B,A)\n",
        "$$\n",
        "\n",
        "* of course, the ordering does not matter for joint probabilities, \n",
        "\n",
        "$$\n",
        "P(A,B) = P(B,A)\n",
        "$$\n",
        "\n",
        "* we calculate joint probabilities as,\n",
        "\n",
        "$$\n",
        "P(A,B) = \\frac{n(A,B)}{n(\\Omega)}\n",
        "$$\n",
        "\n",
        "To clarify the concept of marginal, conditional and joint probability (and distributions), I have coded a set of interactive Python dashboards, [Interactive_Marginal_Joint_Conditional_Probability](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Marginal_Joint_Conditional_Probability.ipynb), \n",
        "\n",
        "* that provides a dataset and interactively calculates and compares marginal, conditional and joint probabilities and visualizes and compares the marginal and conditional distributions.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/interactive_joint.png\" style=\"display: block; margin: 0 auto; width: 100%;\">\n",
        "  <figcaption style=\"text-align: center;\"> One of the interactive Python dashboards from my series on marginal, conditional and joint probability and distributions.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "#### Probability Multiplication Rule\n",
        "\n",
        "As shown above, we can calculate the joint probability of $A$ and $B$ as the product of the conditional probability of $B$ given $A$ with the marginal probability of $A$,\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A,B) = P(B|A) \\cdot P(A)\n",
        "$$\n",
        "\n",
        "and of course, we can also state,\n",
        "\n",
        "$$\n",
        "P(B \\cup A) = P(B,A) = P(A|B) \\cdot P(B)\n",
        "$$\n",
        "\n",
        "this is know as the multiplication rule, we use the multiplication rule to develop the definition of independence.\n",
        "\n",
        "#### Independent Events\n",
        "\n",
        "Given the probability multiplication rule,\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A,B) = P(B|A) \\cdot P(A)\n",
        "$$\n",
        "\n",
        "now we ask the question, what is the conditional probability $P(B|A)$ if events $A$ and $B$ are independent? Given independence between $A$ and $B$ we would expect that knowing about $A$ will not impact the outcome of $B$,\n",
        "\n",
        "$$\n",
        "P(B|A) = P(B)\n",
        "$$\n",
        "\n",
        "if we substitute this into $P(A \\cup B) = P(A,B) = P(B|A) \\cdot P(A)$ we get,\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(B) \\cdot P(A)\n",
        "$$\n",
        "\n",
        "by the same logic we can show,\n",
        "\n",
        "$$\n",
        "P(A|B) = P(A)\n",
        "$$\n",
        "\n",
        "Now we can summarize this as, events $A$ and $B$ are independent if and only if the following relations are true,\n",
        "\n",
        "1. $P(A \\cap B) = P(A) \\cdot P(B)$ - joint probability is the product of the marginal probabilities\n",
        "\n",
        "2. $P(A|B) = P(A)$ - conditional is the marginal probability\n",
        "\n",
        "3. $P(B|A) = P(B)$ - conditional probability is the marginal probability\n",
        "\n",
        "If any of these are violated, we can suspect that there exists some form of relationship.\n",
        "\n",
        "* we leave the significance of this result outside the scope for this chapter on probability\n",
        "\n",
        "* certainly, if we can assume independence this simplifies our data science workflows\n",
        "\n",
        "**Independence Example \\#1** - check independence for the following dataset. \n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/ind_table.png\" style=\"display: block; margin: 0 auto; width: 80%;\">\n",
        "  <figcaption style=\"text-align: center;\"> If event \\(A_1\\) is Fm facies in Middle Unit (blue circles) and event \\(A_2\\) is Sm facies in Bottom Unit (red circles), and joint cases of event \\(A_1\\) and \\(A_2\\) (black rectangles for joint \\(A_1\\) and \\(A_2\\) occur together) are \\(A_1\\) and \\(A_2\\) independent events?\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "To check for independence, we calculate the marginal and joint probabilities,\n",
        "\n",
        "$$\n",
        "P(A_1) = \\frac{5}{10} \\quad P(A_2) = \\frac{6}{10} \\quad P(A_1,A_2) = \\frac{2}{10}\n",
        "$$\n",
        "\n",
        "now we check one of the conditions for independence, as the joint probability as product of the marginal probabilities,\n",
        "\n",
        "* $P(A_1 \\cap A_2) = P(A_1) \\cdot P(A_2)$ $\\rightarrow$ $0.2 \\ne 0.5 \\cdot 0.6$, $\\therefore$ we suspect a relationship between events $A_1$ and $A_2$\n",
        "\n",
        "#### Bayesian Probability\n",
        "\n",
        "Now we finally dive into the Bayesian approach to probability, a very flexible approach that can be applied to assign probability to anything, and includes a framework for updating with new information.\n",
        "\n",
        "When I say that the Bayesian approach can assign probability to anything, I realize this needs some further explanation,\n",
        "\n",
        "* of course, remember my statements in the \"A Warning about Calculating Probability\" section above, there are times when we do not have enough data, and there are workflows that are incorrect.\n",
        "\n",
        "I mean that any type of information can be encoded into the Bayesian framework, including belief \n",
        "\n",
        "Let us discuss this supported by a great example from the introduction chapter of Sivia (1996), \"What is the Mass of Jupyter?\".\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/Jupiter.png\" style=\"display: block; margin: 0 auto; width: 80%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Jupyter image from New Horizons Long Range Imager (LORRI), taken at 57 million km on January 2007. Image from https://en.wikipedia.org/wiki/Jupiter#/media/File:Jupiter_New_Horizons.jpg.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "What would be frequentist and Bayesian approaches to calculate the probabilities for the uncertainty model of the mass of Jupiter?\n",
        "\n",
        "* Frequentist perspective - calculate the cumulative distribution function by measuring the mass of enough Jupiter-like planets from many star systems.\n",
        "\n",
        "* Bayesian - form a prior probability and update with any available information.\n",
        "\n",
        "Now to anyone saying, the frequentist approach is possible, remember the first exoplanets were discovered in 1995 by astronomers Michel Mayor and Didier Queloz. \n",
        "\n",
        "* not coincidentally, the first discovered exoplanet was a Jupyter-mass hot, gas giant, as these are easier to see with the Doppler-based wobble method, because near-to-star gas giants have greater pull on their star resulting in a relatively  high amplitude and frequency that can be readily observed.\n",
        "\n",
        "* when Sivia was authoring his book, there were no known exoplanets.\n",
        "\n",
        "So, we didn't have other Jupiter like planets to apply the frequentist approach, but we could use our knowledge about the formation of solar systems to formulate a prior model. Then we could use any available observations or measurements from Jupiter to update this prior. We could do something!\n",
        "\n",
        "* but I am getting ahead of myself, we must properly introduce the Bayesian approach to probability, by starting with Bayes' theorem.\n",
        "\n",
        "#### Bayes' Theorem\n",
        "\n",
        "Once again, here's the multiplication rule,\n",
        "\n",
        "$$\n",
        "P(B \\cup A) = P(B,A) = P(A|B) \\cdot P(B)\n",
        "$$\n",
        "\n",
        "and it follows that we can also express it as,\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A,B) = P(B|A) \\cdot P(A)\n",
        "$$\n",
        "\n",
        "If follows that,\n",
        "\n",
        "$$\n",
        "P(B \\cup A) = P(A \\cup B)\n",
        "$$\n",
        "\n",
        "therefore, we can substitute the right-hand sides of the multiplication rules above into this equality as,\n",
        "\n",
        "$$\n",
        "P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A)\n",
        "$$\n",
        "\n",
        "this is Bayes' theorem. We can make a simple modification to get the popular form of Bayes' theorem,\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "To better explain Bayes' theorem, we replace the $A$ with \"Model\" and $B$ with \"New Data\", where \"New Data\" is the new data that we are updating with. \n",
        "\n",
        "$$\n",
        "P(\\text{Model}|\\text{New Data}) = \\frac{P(\\text{New Data}|\\text{Model}) \\cdot P(\\text{Model})}{P(\\text{New Data})}\n",
        "$$\n",
        "\n",
        "now we can see Bayesian updating as taking a model and updating it with new data.\n",
        "\n",
        "Now we make some observations about Bayes' theorem,\n",
        "\n",
        "1. We are reversing conditional probabilities to get $P(A|B)$ from $P(B|A)$, this often comes in handy because we readily have access to one but not the other. \n",
        "\n",
        "2. The probabilities in Bayes' theorem are known as:\n",
        "\n",
        "* $P(B)$ - evidence \n",
        "\n",
        "* $P(A)$ - prior\n",
        "\n",
        "* $P(B|A)$ - likelihood \n",
        "\n",
        "* $P(A|B)$ - posterior\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; we can substitute the probabilities with their labels,\n",
        "\n",
        "$$\n",
        "\\text{Posterior} = \\frac{\\text{Likelihood} \\cdot \\text{Prior}}{\\text{Evidence}}\n",
        "$$\n",
        "\n",
        "3. Prior should have no information from likelihood. \n",
        "\n",
        "* the prior probability is estimated prior to the collection of the new information in the likelihood. If the prior and likelihood includes new information, then this is \"double accounting\" of the new information!\n",
        "\n",
        "4. Evidence term is usually just a standardization to ensure probability closure.\n",
        "\n",
        "* evidence probability is often calculated with marginalization,\n",
        "\n",
        "$$\n",
        "P(B) = \\int_{A} P(B|A) \\cdot P(A) dA\n",
        "$$\n",
        "\n",
        "* for the example or two mutually exclusive, exhaustive outcomes, $A$ and $A^c$, then this marginalization can be applied to calculate $P(B)$.\n",
        "\n",
        "$$\n",
        "P(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(B^c)\n",
        "$$\n",
        "\n",
        "* and by substitution we get this expanded but usually easier to calculate form of Bayes' theorem,\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(B^c)}\n",
        "$$\n",
        "\n",
        "* we can generalize this marginalization for any number of discrete, mutually exclusive, exhaustive events as,\n",
        "\n",
        "$$\n",
        "P(B) = \\sum_{i=1}^{m} P(B|A_i) \\cdot P(A_i), \\quad \\forall \\quad i = 1,\\ldots,m\n",
        "$$\n",
        "\n",
        "* in some cases, like Markov chain Monte Carlo with the Metropolis-Hastings algorithm, we eliminate the need for the evidence term because we calculate a probability ratio of the proposed step vs. the current state and the evidence probability cancels out.\n",
        "\n",
        "5. If the prior is na\u00efve then the posterior is equal to the likelihood. This is logical, if we know nothing prior to collecting the new data then we completely rely on the likelihood probability.\n",
        "\n",
        "* a na\u00efve prior is a maximum uncertainty prior, like the uniform probability with all outcomes equal probable\n",
        "\n",
        "* under the assumption of standard normal, global Gaussian distribution with a mean of 0.0 and standard deviation of 1.0, a na\u00efve prior is the standard normal distribution\n",
        "\n",
        "6. If the likelihood probability is na\u00efve then the posterior is equal to the prior. Once again, this is logical, if the new data provides no information, then we should continue to rely on the prior probability and the original model is not updated.\n",
        "\n",
        "#### Bayesian Probability Example Problems\n",
        "\n",
        "An efficient way to improve your understanding of Bayesian probability is through solving illustrative problems. Here's a great group of problems,\n",
        "\n",
        "* you conduct a test and get a positive test result for something we can't directly observe happening, but what is the probability of the thing actually happening given the positive test?\n",
        "\n",
        "Here's some examples of tests that fit in this class of problems,\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/tests.png\" style=\"display: block; margin: 0 auto; width: 80%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Example cases of a tests, the underlying event is happening vs. a test that indicates the underlying event is happening. \n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "and we can rewrite Bayes' theorem for the case of one of these tests,\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/Bayes_tests.png\" style=\"display: block; margin: 0 auto; width: 80%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Bayes' theorem rewritten for the case of tests. \n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "**Bayesian Updating Example \\#1** - prior information at a site suggests a deepwater channel reservoir exists at a given location with a probability of 0.6. We consider further investigation with a 3D seismic survey.\n",
        "\n",
        "3D seismic survey will indicate a channelized reservoir:\n",
        " \n",
        "\u2010 is present with 0.9 probability if it really is present\n",
        "\n",
        "\u2010 is not present with a probability 0.7 if it really is not\n",
        "\n",
        "We have our test, seismic survey, that will offer new data, and we have our prior information (prior to collecting the new seismic data). Now we define our events, \n",
        "\n",
        "* $A$ = the deepwater channel is present, then $P(A)$ is the probability the thing is happening before collecting the new data \n",
        "\n",
        "* $B$ = new seismic shows a deepwater channel, then $P(B)$ is the probability of a positive test for the thing happening\n",
        "\n",
        "It follows that the compliments are,\n",
        "\n",
        "* $A^c$ = the deepwater channel not present, then $P(A^c)$ is the probability the thing is not happening before collecting the new data \n",
        "\n",
        "* $B^c$ = new seismic does not show a deepwater channel, then $P(B^c)$ is the probability of a negative test for the thing happening\n",
        "\n",
        "Now we write out Bayes' theorem, the regular and expanded evidence term by marginalization,\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(B^c)}\n",
        "$$\n",
        "\n",
        "What do we know? From the question above we have,\n",
        "\n",
        "* $P(A) = 0.6$ - \"prior information at a site suggests a deepwater channel reservoir exists at a given location with a probability of 0.6\"\n",
        "\n",
        "* $P(B|A) = 0.9$ - \"is present with 0.9 probability if it really is present\"\n",
        "\n",
        "* $P(B^c|A^c) = 0.7$ - \"is not present with a probability 0.7 if it really is not\"\n",
        "\n",
        "but we also need $P(A^c)$ and $P(B|A^c)$. We calculate these from closure as,\n",
        "\n",
        "* $P(A^c) = 1.0 - P(A) = 1.0 - 0.6 = 0.4$\n",
        "\n",
        "* $P(B|A^c) = 1.0 - P(B^c|A^c) = 1.0 - 0.7 = 0.3$\n",
        "\n",
        "we have all the information that we need to substitute,\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)} = \\frac{0.9 \\cdot 0.6}{0.9 \\cdot 0.6 + 0.3 \\cdot 0.4} = 0.82\n",
        "$$\n",
        "\n",
        "Given a positive test, seismic indicating a deepwater channel is present, we have posterior probability of 0.82. \n",
        "\n",
        "Is seismic data useful? How do we access this?\n",
        "\n",
        "* consider the change from prior probability, 0.6, to posterior probability, 0.82, showing a reduction in uncertainty. By integrating the value and potential loss of this decision it is possible to now assign the value of information for seismic data. \n",
        "\n",
        "* I leave decision analysis out of scope for this e-book, but it is a fascinating topic, and I recommend anyone involved in data science to learn at least the basics to ensure you add value by impacting the decision(s)!   \n",
        "\n",
        "It is quite instructive to conduct a sensitivity on this example problem to assess the behavior of Bayesian updating. You could download and run my [interactive Bayesian updating dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb) to accomplish this.\n",
        "\n",
        "* it is a custom dashboard to visualize Bayesian updating.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/interactive_Bayesian.png\" style=\"display: block; margin: 0 auto; width: 100%;\">\n",
        "  <figcaption style=\"text-align: center;\"> My interactive Python dashboard demonstrating Bayesian updating. Note, \\(H\\) represents the thing is happening (\\(A\\) in our example), and \\(+\\) represents a positive test and \\(-\\) represents a negative test (\\(B\\) and \\(B^c\\) respectively in our example). \n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "We have improved the test precision, probability of seismic detecting a channel given it is present, $P(B|A)$ from 0.9 to 0.99.\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)} = \\frac{0.99 \\cdot 0.6}{0.99 \\cdot 0.6 + 0.3 \\cdot 0.4} = 0.83\n",
        "$$\n",
        "\n",
        "we have almost no change in the posterior probability. Now we have improved our ability to detect no channel given the channel is not present, $P(B|A^c)$ from 0.3 to 0.03.\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)} = \\frac{0.9 \\cdot 0.6}{0.9 \\cdot 0.6 + 0.03 \\cdot 0.4} = 0.98\n",
        "$$\n",
        "\n",
        "we change the posterior probability from 0.82 to 0.98! What happened?\n",
        "\n",
        "* our test was being impacted by a relatively high probability of false positive, seismic showing a channel when there is no channel present, $P(B|A^c) = 0.3$ paired with a relatively high rate of no channel present, $P(A^c) = 0.4$.\n",
        "\n",
        "now we switch gears and look at another illustrative example of Bayesian updating with machines on a production line.\n",
        "\n",
        "**Bayesian Updating Example \\#2** - You have 3 machines making the same product (imagine a large production line). They have different volumes and errors.\n",
        "\n",
        "* Note we assume the products are mutually exclusive (only come from a single machine), and exhaustive (all products come from one of the 3 machines).\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/Bayesian_machines.png\" style=\"display: block; margin: 0 auto; width: 80%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Machines 1, 2 and 3, produce the same product that are then mixed in the assembly line. They each have different production rates (percentage of total and error rates).\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "We want to know given an error in an individual product, what is the probability that the product came from a specific machine? First, we define our variables, \n",
        "\n",
        "* $Y$ = the product has an error \n",
        "\n",
        "* $X_1, X_2, X_3$ = the product came from machines 1, 2 or 3 respectively.\n",
        "\n",
        "our Bayesian formulation for probability of defective (product with error) product coming from a specific machine is,\n",
        "\n",
        "$$\n",
        "P(X_i|Y) = \\frac{P(Y|X_i) \\cdot P(X_i)}{P(Y)} \\quad \\forall \\quad i = 1, 2, 3\n",
        "$$\n",
        "\n",
        "we will need to calculate our evidence term, P(Y) to solve this for any product. Also, we only must do this once, the evidence term is the same for all products. \n",
        "\n",
        "We will apply marginalization to accomplish this by substituting our variables into the previously introduced general equation as, \n",
        "\n",
        "$$\n",
        "P(Y) = \\sum_{i=1}^{m} P(Y|X_i) \\cdot P(X_i), \\quad \\forall \\quad i = 1,\\ldots,3\n",
        "$$\n",
        "\n",
        "our specific form for our problem is,\n",
        "\n",
        "$$\n",
        "P(Y) = P(Y|X_1) \\cdot P(X_1) + P(Y|X_2) \\cdot P(X_2) + P(Y|X_3) \\cdot P(X_3) \n",
        "$$\n",
        "\n",
        "we substitute the probabilities from the problem statement as,\n",
        "\n",
        "$$\n",
        "P(Y) = 0.2 \\cdot 0.05 + 0.3 \\cdot 0.03 + 0.5 \\cdot 0.01 = 0.024\n",
        "$$\n",
        "\n",
        "Given this evidence probability, now we can calculate the posterior probabilities of the product coming from each machine given the product is defective,\n",
        "\n",
        "$$\n",
        "P(X_1|Y) = \\frac{P(Y|X_1) \\cdot P(X_1)}{P(Y)} = \\frac{0.05 \\cdot 0.2}{0.024} = 0.41\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(X_2|Y) = \\frac{P(Y|X_2) \\cdot P(X_2)}{P(Y)} = \\frac{0.03 \\cdot 0.3}{0.024} = 0.38\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(X_3|Y) = \\frac{P(Y|X_3) \\cdot P(X_3)}{P(Y)} = \\frac{0.01 \\cdot 0.5}{0.024} = 0.21\n",
        "$$\n",
        "\n",
        "Let us close the loop by checking closure, we expect these posterior probabilities to sum to 1.0 over all machines (once again assuming products are mutually exclusive, and exhaustive over the machines),\n",
        "\n",
        "$$\n",
        "P(X_1|Y) + P(X_2|Y) + P(X_3|Y) = 0.41 + 0.38 + 0.21 = 1.0\n",
        "$$\n",
        "\n",
        "and we have closure.\n",
        "\n",
        "These two examples are very helpful to understand Bayesian updating. In the classroom I get my students to calculate the additional posteriors such as the probability of the event happening given a negative test, etc. \n",
        "\n",
        "* my second dashboard in this Jupyter notebook [interactive Bayesian updating dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb) includes these examples. \n",
        "\n",
        "* I invite you to play with Bayesian updating.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/interactive_Bayesian_all.png\" style=\"display: block; margin: 0 auto; width: 100%;\">\n",
        "  <figcaption style=\"text-align: center;\"> My more complete interactive Python dashboard demonstrating Bayesian updating for all possible posterior probabilities. Note, \\(H\\) represents the thing is happening \\(A\\) in our example, and \\(+\\) represents a positive test and \\(-\\) represents a negative test, \\(B\\) and \\(B^c\\) respectively in our example. \n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "#### Bayesian Updating with Gaussian Distributions\n",
        "\n",
        "Sivia (1996) provided an analytical approach for Bayesian updating under the assumption of a standard normal global distribution, Gaussian distributed with mean of 0.0 and variance of 1.0. We calculate,\n",
        "\n",
        "* the mean of the posterior distribution from the prior and likelihood mean and variance.\n",
        "\n",
        "$$\n",
        "\\mu_{\\text{posterior}} = \\frac{\\mu_{\\text{likelihood}} \\cdot \\sigma^2_{\\text{prior}} + \\mu_{\\text{prior}} \\cdot \\sigma^2_{\\text{likelihood}}}{\\left[1.0 \u2212 \\sigma^2_{\\text{likelihood}} \\right] \\cdot \\left[\\sigma^2_{\\text{prior}} \u2212 1.0 \\right]+1.0}\n",
        "$$\n",
        "\n",
        "* the variance of the posterior distribution from the prior and likelihood variances\n",
        "\n",
        "$$\n",
        "\\sigma^2_{\\text{posterior}} = \\frac{\\sigma^2_{\\text{prior}} \\cdot \\sigma^2_{\\text{likelihood}}}{\\left[1.0 \u2212 \\sigma^2_{\\text{likelihood}} \\right] \\cdot \\left[\\sigma^2_{\\text{prior}} \u2212 1.0 \\right]+1.0}\n",
        "$$\n",
        "\n",
        "* consistent with the multivariate Gaussian distribution this posterior is homoscedastic, the prior or likelihood means are not in the equation for posterior variance.\n",
        "\n",
        "* my third dashboard in this Jupyter notebook [interactive Bayesian updating dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb) includes Bayesian updating with Gaussian distributions.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/probability/interactive_Bayesian_gaussian.png\" style=\"display: block; margin: 0 auto; width: 100%;\">\n",
        "  <figcaption style=\"text-align: center;\"> My interactive Python dashboard demonstrating Bayesian updating under the assumption of standard normal global distribution.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "This example helps my students understand the concept of Bayesian updating for building uncertainty distributions. Here are some things to try out,\n",
        "\n",
        "1. a na\u00efve prior or likelihood and observe the posterior is the same as the likelihood or prior, respectively.\n",
        "\n",
        "2. a very certain, low variance prior. Start with a large prior variance and as you reduce the variance, the posterior is pulled towards the prior. The influence of prior and likelihood is proportional to their relative certainty.\n",
        "\n",
        "3. a contradiction between prior and likelihood with very different means. You may be able to even introduce extrapolation, a low prior positive mean with higher positive likelihood mean can result in an even higher positive posterior mean. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comments\n",
        "\n",
        "This was a basic treatment of probability. Much more could be done and discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links at the start of this chapter with resource links in the videos' descriptions.\n",
        "\n",
        "I hope this is helpful,\n",
        "\n",
        "*Michael*\n",
        "\n",
        "#### About the Author\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "  <img src=\"_static/intro/michael_pyrcz_officeshot_jacket.jpg\" style=\"display: block; margin: 0 auto; width: 70%;\">\n",
        "  <figcaption style=\"text-align: center;\"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p), and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/), at [The University of Texas at Austin](https://www.utexas.edu/), where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,\n",
        "\n",
        "* the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics) freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin\n",
        "\n",
        "* an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board), and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board), the International Association for Mathematical Geosciences. \n",
        "\n",
        "Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en), a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics, co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) and author of two recently released e-books, [Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).\n",
        "\n",
        "All of Michael\u2019s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures) with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy), to support any interested students and working professionals with evergreen content. To find out more about Michael\u2019s work and shared educational resources visit his [Website](www.michaelpyrcz.com).\n",
        "\n",
        "#### Want to Work Together?\n",
        "\n",
        "I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.\n",
        "\n",
        "* Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I'd be happy to drop by and work with you! \n",
        "\n",
        "* Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!\n",
        "\n",
        "* I can be reached at mpyrcz@austin.utexas.edu.\n",
        "\n",
        "I'm always happy to discuss,\n",
        "\n",
        "*Michael*\n",
        "\n",
        "Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin\n",
        "\n",
        "More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}