

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Machine Learning Concepts &#8212; Applied Machine Learning in Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'MachineLearning_concepts';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Machine Learning Workflow Construction and Coding" href="MachineLearning_worklfow_construction.html" />
    <link rel="prev" title="Applied Machine Learning in Python: a Hands-on Guide with Code" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/AppliedMachineLearning.jpg" class="logo__image only-light" alt="Applied Machine Learning in Python - Home"/>
    <script>document.write(`<img src="_static/AppliedMachineLearning.jpg" class="logo__image only-dark" alt="Applied Machine Learning in Python - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Applied Machine Learning in Python: a Hands-on Guide with Code
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Machine Learning Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_worklfow_construction.html">Workflow Construction and Coding</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_probability.html">Probability Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_plotting_data_models.html">Loading and Plotting Data and Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_univariate_analysis.html">Univariate Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_multivariate_analysis.html">Multivariate Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_feature_transformations.html">Feature Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_feature_ranking.html">Feature Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_clustering.html">Cluster Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_density-based_clustering.html">Density-based Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_spectral_clustering.html">Spectral Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_PCA.html">Principal Components Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_multidimensional_scaling.html">Multidimensional Scaling</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_ridge_regression.html">Ridge Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_LASSO_regression.html">LASSO Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html">Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_naive_Bayes.html">Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_polynomial_regression.html">Polynomial Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_knearest_neighbours.html">k-Nearest Neighbours</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_decision_tree.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_ensemble_trees.html">Bagging Tree and Random Forest</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_gradient_boosting.html">Gradient Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_support_vector_machines.html">Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_time_series.html">Time Series Analysis and Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusions.html">Conclusions</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/GeostatsPyDemos_Book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/GeostatsPyDemos_Book/issues/new?title=Issue%20on%20page%20%2FMachineLearning_concepts.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/MachineLearning_concepts.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine Learning Concepts</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-machine-learning-concepts">Motivation for Machine Learning Concepts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#big-data">Big Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-geostatistics-and-data-analytics">Statistics, Geostatistics and Data Analytics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-science">Data Science</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cheap-and-available-compute">Cheap and Available Compute</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#availability-of-big-data">Availability of Big Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-science-and-subsurface-resources">Data Science and Subsurface Resources</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-prerequisite-definitions">Machine Learning Prerequisite Definitions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#population">Population</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample">Sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-feature">Variable / Feature</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-and-response-features">Predictor and Response Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-tuning-machine-learning-models">Training and Tuning Machine Learning Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parameters-and-model-hyperparameters">Model Parameters and Model Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-and-classification">Regression and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sources-of-predictive-machine-learning-testing-error">Sources of Predictive Machine Learning Testing Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underfit-and-overfit-models">Underfit and Overfit Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-about-training-and-testing-splits">More About Training and Testing Splits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-and-professional-practice-concerns-with-data-science">Ethical and Professional Practice Concerns with Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">Comments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-author">The Author:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-work-together">Want to Work Together?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-resources-available-at-twitter-github-website-googlescholar-geostatistics-book-youtube-applied-geostats-in-python-e-book-applied-machine-learning-in-python-e-book-linkedin">More Resources Available at: Twitter | GitHub | Website | GoogleScholar | Geostatistics Book | YouTube  | Applied Geostats in Python e-book | Applied Machine Learning in Python e-book | LinkedIn</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="machine-learning-concepts">
<h1>Machine Learning Concepts<a class="headerlink" href="#machine-learning-concepts" title="Permalink to this heading">#</a></h1>
<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with Code‚Äù.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, Applied Machine Learning in Python: a Hands-on Guide with Code, <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book">https://geostatsguy.github.io/MachineLearningDemos_Book</a>.</p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository (0.0.1). Zenodo. <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="https://zenodo.org/badge/862519860.svg" /></a></p>
</div>
<p>By Michael J. Pyrcz <br />
¬© Copyright 2024.</p>
<p>This chapter is a summary of <strong>Machine Learning Concepts</strong> including essential concepts:</p>
<ul class="simple">
<li><p>Statistics and Data Analytics</p></li>
<li><p>Inferential Machine Learning</p></li>
<li><p>Predictive Machine Learning</p></li>
<li><p>Machine Learning Model Training and Tuning</p></li>
<li><p>Machine Learning Model Overfit</p></li>
</ul>
<p><strong>YouTube Lecture</strong>: check out my lecture on <a class="reference external" href="https://youtu.be/zOUM_AnI1DQ?si=Gi2KZTfPa5xQ2Qb6">Introduction to Machine Learning</a>. For your convenience here‚Äôs a summary of salient points.</p>
<section id="motivation-for-machine-learning-concepts">
<h2>Motivation for Machine Learning Concepts<a class="headerlink" href="#motivation-for-machine-learning-concepts" title="Permalink to this heading">#</a></h2>
<p>You could just open up a Jupyter notebook in Python and start building machine learning models. The <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn docs</a> are quite good and for every machine learning function there is a short code example that you could copy and paste to repeat their work. Also, you could Google a question about using a specific machine learning algorithm in Python and the top results will include <a class="reference external" href="https://stackoverflow.com/">StackOverflow</a> questions and responses, it is truly amazing how much experienced coders are willing to give back and share their knowledge. We truly have an amazing scientific community with the spirit of knowledge sharing and open-source development. Respect. Of course, you could learn a lot about machine learning from a machine learning large language model (LLM) like <a class="reference external" href="https://chatgpt.com/">ChatGPT</a>. Not only will ChatGPT answer your questions, but it will also provide codes and help you debug them when you tell it what went wrong. On way of the other and you received and added this code to you data science workflow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_neighbours</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="n">weights</span> <span class="o">=</span> <span class="s2">&quot;distance&quot;</span><span class="p">;</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbours</span><span class="p">,</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span> 
</pre></div>
</div>
<p>et voil√†, you have a trained predictive machine learning model that could be applied to make predictions for new cases.</p>
<ul class="simple">
<li><p>But, is this a good model?</p></li>
<li><p>How good is it?</p></li>
<li><p>Could it be better?</p></li>
</ul>
<p>Without knowledge about basic machine learning concepts we can‚Äôt answer these questions and build the best possible models. In general, I‚Äôm not an advocate for black box modeling, because it is:</p>
<ol class="arabic simple">
<li><p>likely to lead to mistakes that may be difficult to detect and correct</p></li>
<li><p>incompatible with the expectations for compentent practice for professional engineers. I gave a talk on <a class="reference external" href="https://youtu.be/W_ZDg1Wb2vM?si=DF-n1E4-ik2ukLfF">Applying Machine Learning as a Compentent Engineer or Geoscientist</a></p></li>
</ol>
<p>To help out this chapter provides you with the basic knowledge to answer these questions and to make better, more reliable machine learning models. Let‚Äôs start building this essential foundation with some definitions.</p>
</section>
<section id="big-data">
<h2>Big Data<a class="headerlink" href="#big-data" title="Permalink to this heading">#</a></h2>
<p>Everyone hears that machine learning needs a lot of data. In fact, so much data that it is called ‚Äúbig data‚Äù, but how do you know if you are working with big data? The criteria for big data are these ‚ÄòV‚Äôs, if you answer yes for at least some of these, then you are working with big data:</p>
<ul class="simple">
<li><p><strong>Volume</strong>: many data samples, difficult to handle and visualize</p></li>
<li><p><strong>Velocity</strong>: high rate collection, continuous relative to decision making cycles</p></li>
<li><p><strong>Variety</strong>: data form various sources, with various types and scales</p></li>
<li><p><strong>Variability</strong>: data acquisition changes during the project</p></li>
<li><p><strong>Veracity</strong>: data has various levels of accuracy</p></li>
</ul>
<p>In my experience, most subsurface engineers and geoscientists answer yes to all of these questions. So I proudly say that we in the subsurface have been big data long before the tech sector learned about big data. In fact, I state that we in the subsurface resource industries are the original data scientists. I‚Äôm getting ahead of myself, more on this in a bit. Don‚Äôt worry if I get carried away in hubris, rest assured this e-book is written for anyone interested to learn about machine learning. You can skip the short sections on subsurface data science or read along if interested. Now that we know big data, let‚Äôs talk about the big topics.</p>
</section>
<section id="statistics-geostatistics-and-data-analytics">
<h2>Statistics, Geostatistics and Data Analytics<a class="headerlink" href="#statistics-geostatistics-and-data-analytics" title="Permalink to this heading">#</a></h2>
<p>Statistics is collecting, organizing, and interpreting data, as well as drawing conclusions and making decisions. If you look up the definition of data analytics you will find criteria that include statistical analysis, and data visualization to support decision making. I‚Äôm going to call it, data analytics and statistics are the same thing. Now we can append, geostatistics as a branch of applied statistics that accounts for:</p>
<ol class="arabic simple">
<li><p>the spatial (geological) context</p></li>
<li><p>the spatial relationships</p></li>
<li><p>volumetric support</p></li>
<li><p>uncertainty</p></li>
</ol>
<p>Remember all those statistics classes with the assumption of i.i.d., independent, identically distributed. Spatial phenomenon don‚Äôt do that, so we developed a unique branch of statistics to address this. By our assumption above, we can state that geostatistics is the same as spatial data analytics. Now, let‚Äôs use a Venn diagram to visualize statistics / data analytics and geostatistics / spatial data analytics:</p>
<figure style="text-align: center;">
  <img src="_static/concepts/Venn_stats.png" style="display: block; margin: 0 auto; width: 40%;">
  <figcaption style="text-align: center;">Venn diagram for statistics and geostatistics.</figcaption>
</figure>
<p>Now we can add our previously discussed big data to our Venn diagram resulting in big data analytics and spatial big data analytics.</p>
<figure style="text-align: center;">
  <img src="_static/concepts/Venn_big_data.png" style="display: block; margin: 0 auto; width: 40%;">
  <figcaption style="text-align: center;">Venn diagram for statistics and geostatistics with big data added.</figcaption>
</figure>
</section>
<section id="data-science">
<h2>Data Science<a class="headerlink" href="#data-science" title="Permalink to this heading">#</a></h2>
<p>If no one else has said this to you, let me have the honor of welcoming you to the fourth paradigm for scientific discovery, data-driven scientific discovery or just call it data science. The paradigms for scientific discovery are distinct stages or approaches for humanity to develop science. To understand the fourth paradigm let‚Äôs first consider the previous three paradigms. Here‚Äôs all of the four paradigms with dates for important developments with each.:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>1st Paradigm Empirical Science</p></th>
<th class="head"><p>2nd Paradigm Theoretical Science</p></th>
<th class="head"><p>3rd Paradigm Computational Science</p></th>
<th class="head"><p>4th Paradigm Data Science</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Experiments</p></td>
<td><p>Models and Laws</p></td>
<td><p>Numerical Simulation</p></td>
<td><p>Learning from Data</p></td>
</tr>
<tr class="row-odd"><td><p>430 BC Empedocles proved air has substance</p></td>
<td><p>1011 AD al-Haytham Book of Optics</p></td>
<td><p>1942 Manhattan Project</p></td>
<td><p>2009 Hey et al. Data-Intensive Book</p></td>
</tr>
<tr class="row-even"><td><p>230 BC Eratosthenes measure Earth‚Äôs diameter</p></td>
<td><p>1687 AD Newton Principia</p></td>
<td><p>1980 ‚Äì Global Forecast System (GFS)</p></td>
<td><p>2015 AlphaGo beats a professional Go player</p></td>
</tr>
</tbody>
</table>
<p>Of course, we can argue about the boundaries between the paradigms for scientific discovery, i.e., when did a specific paradigm begin? Certainly the Mesopotamians (4000 BC - 3500 BC) conducted many first paradigm experiments that supported their development of the wheel, plow, chariot, weaving loom, and irrigation and on the other side we can trace the development of artificial neural networks to McColloch and Pitts in 1943. Also, the adoption of a new scientific paradigm is a major societal shift that does not occur uniformly around the globe.</p>
<p>So what caused the fourth paradigm to start some time between 1943 and 2009? The fundamental mathematics of data driven models has been available for a long time considering the following mathematical and statistical developments in history:</p>
<ul class="simple">
<li><p><strong>Calculus</strong> - Isaac Newton and Gottfried Wilhelm Leibniz independently developed the math to find minimums and maximums during the 1600s with ‚ÄúMethods of Fluxions‚Äù 1671 (published posthumously in 1736), and with ‚ÄúNova Methodus pro Maximis et Minimis‚Äù published in 1684.</p></li>
<li><p><strong>Bayesian Probability</strong> - introduced by Reverend Thomas Bayes with Bayes‚Äô Theorem enshrined in ‚ÄúAn Essay Towards Solving a Problem in the Doctrine of Chances‚Äù published posthumously in 1763</p></li>
<li><p><strong>Linear Regression</strong> - formalized by Marie Legendre in 1805</p></li>
<li><p><strong>Discriminant Analysis</strong> - developed by Ronald Fisher in his 1939 paper ‚ÄúThe Use of Multiple Measurements in Taxonomic Problems‚Äù</p></li>
<li><p><strong>Monte Carlo Simulation</strong> - pioneered by Stanislaw Ulam and John von Neumann in the early 1940‚Äôs as part of the Manhattan Project</p></li>
</ul>
<p>Upon reflection, one could ask, why didn‚Äôt the fourth paradigm start before the 1940‚Äôs and even in the 1800s? What changed? The answer is that other critical developments provided the fertile ground for data science, including cheap and available compute and big data.</p>
<section id="cheap-and-available-compute">
<h3>Cheap and Available Compute<a class="headerlink" href="#cheap-and-available-compute" title="Permalink to this heading">#</a></h3>
<p>Consider the following developments in computers.</p>
<ul class="simple">
<li><p><strong>Charles Babbage‚Äôs Analytical Engine</strong> (1837) is often credited as the first computer, yet is was a mechanical device the attempted to implement many modern concepts such as arithmetic logic, control flow and memory, but it was never completed given the challenges with the available technology.</p></li>
<li><p><strong>Konrad Zuse‚Äôs Z3</strong> (1941) and <strong>ENIAC</strong> (1945) the first digital, programmable computers, but they were programmed by labor-intensive and time-consuming rewiring of plug boards with machine language as there was no high level programming language, the memory could fit 1,000 words limiting the program complexity and with 1000s of vacuum tubes cooling and maintenance was a big challenge. These unreliable machines were very slow with only about 5,000 additions per second.</p></li>
<li><p><strong>Transistors</strong> were invented in 1947 the transistor replaced the vacuum tubes greatly improving energy-efficiency, miniaturization, speed and reliability. Resulting in the second generation computers such as <strong>IBM 7090</strong> and <strong>UNIVAC 1108</strong>.</p></li>
<li><p><strong>Integrated Circuits</strong> were developed in the 1960s allowed for multiple transistors to be placed on a single chip leading to third generation computers.</p></li>
<li><p><strong>Microprocessors</strong> - developed in 1971 integrated the functions of a computer‚Äôs central processing unit (CPU) enabling smaller, cheaper computers, resulting in the personal, home computer revolution including the <strong>Apple II</strong> (1977) and <strong>IBM PC</strong> (1981). Yes, when I was in grade 1 in elementary school we had a Apple II in my classroom, and it amazed all of us with the monochrome (orange and black pixels) monitor, floppy disk loaded programs (there was no hard drive) and beeps and clicks from the speaker!</p></li>
</ul>
<p>We live in a society with more compute power in our pockets (cell phones) than used to send the first astronauts to the moon and a SETI screen saver that uses home computers‚Äô idle time to search for extraterrestrial life! Now we are surrounded by cheap and reliable compute that our grandparents (and perhaps parents) could never have imagined.</p>
<p>As you will learn in this e-book, machine learning methods requires a lot of compute. In fact, training most of these machine learning methods rely on a large number of iterations, matrix or parallel operations, bootstrapping models, stochastic descent, and tuning the models requires a lot of trained models requiring even another loop of iteration. Cheap and available compute is an essential prerequisite for the fourth paradigm. Now consider the development of data science has been largely a crowd-source, open-source effort. There cannot be data science without many people having easy access to compute.</p>
</section>
<section id="availability-of-big-data">
<h3>Availability of Big Data<a class="headerlink" href="#availability-of-big-data" title="Permalink to this heading">#</a></h3>
<p>With small data we tend to rely on other sources of information such as physics, engineering and geoscience principals and then calibrate these models to the few available observations, as is common in applications of the second and third paradigm for scientific discovery. With big data we can learn the full range of behaviors of natural systems from the data itself. In fact, with big data we often see the limitations of the second and third paradigm models due to missing complexity in our solutions.</p>
<p>Therefore, big data:</p>
<ul class="simple">
<li><p>provides sufficient sampling to support the data-driven fourth paradigm</p></li>
<li><p>and may actually preclude exclusive use of the second and third paradigm.</p></li>
</ul>
<p>These days big data is everywhere, with so much data now open-source and available online. Here‚Äôs some great examples:</p>
<ul class="simple">
<li><p>satellite data with time lapse is widely available on <a class="reference external" href="https://www.google.com/maps">Google Earth</a> and other platforms. Yes, this isn‚Äôt the most up to date and highest resolution, but it is certainly sufficient for many land use, geomorphology and surface evolution studies. My team of graduate students use it.</p></li>
<li><p>river gages data over your state are generally available to the public. We use it for hydrologic analysis and also to determine great locations and times to paddle! Check out the <a class="reference external" href="https://waterdata.usgs.gov/nwis">USGS‚Äôs National Water Information System</a></p></li>
<li><p>government databases, including the <a class="reference external" href="https://data.census.gov/">United States Census</a> to <a class="reference external" href="https://www.rrc.texas.gov/oil-and-gas/research-and-statistics/production-data/">oil and gas production data</a> are available to all</p></li>
<li><p><a class="reference external" href="https://trek.nasa.gov/moon/#v=0.1&amp;x=0&amp;y=0&amp;z=1&amp;p=urn%3Aogc%3Adef%3Acrs%3AEPSG%3A%3A104903&amp;d=&amp;locale=&amp;b=moon&amp;e=-337.4999937044213%2C-167.87109061860536%2C337.4999937044213%2C167.87109061860536&amp;sfz=&amp;w=">Moon Trek</a>: online portal to visualize and download digital images from Lunar Reconnaissance Orbiter (LRO), SELENE and Clementine.</p></li>
</ul>
<p>This is open, public data, but consider the fact that many industries are embracing smart, intelligent systems with enhanced connectivity, monitoring and control. The result is an explosion of data supported by faster and cheaper computation, processing and storage. We are all swimming in data that the generation before us could not have imagined.</p>
<p>We could also talk about improved algorithms and hardware architectures optimized for data science, but I‚Äôll leave that out of scope for this e-book. All of these developments have provided the fertile ground for the seeds of data science to impact all sectors or our economy and society.</p>
</section>
</section>
<section id="data-science-and-subsurface-resources">
<h2>Data Science and Subsurface Resources<a class="headerlink" href="#data-science-and-subsurface-resources" title="Permalink to this heading">#</a></h2>
<p>Spoiler alert, I‚Äôm going to boast a bit in the section. I often hear students say, ‚ÄúI can‚Äôt believe this data science course is in the <a class="reference external" href="https://pge.utexas.edu/">Hildebrand Department of Petroleum and Geosystems Engineering</a>!‚Äù or ‚ÄúWhy are you teaching machine learning in <a class="reference external" href="https://eps.jsg.utexas.edu/">Department of Earth and Planetary Sciences</a>?‚Äù My response is,</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">We in the subsurface are the original data scientists!</p>
<p>We are the original data-driven scientists, we have been big data long before tech learned about big data!</p>
</div>
<p>This may sound a bit arrogant, but let me back this up with this timeline:</p>
<figure style="text-align: center;">
  <img src="_static/concepts/history.png" style="display: block; margin: 0 auto; width: 90%;">
  <figcaption style="text-align: center;">Timeline of data science development from the perspective of subsurface engineering and geoscience.</figcaption>
</figure>
<p>Shortly after Kolmogorov developed the fundamental probability axioms, Danie Krige developed a set of statistical, spatial, i.e., data-driven tools for making estimates in space while accounting for spatial continuity and scale. These tools were formalized with theory developed by Professor Matheron during the 1960s in a new science called geostatistics. Over the 1970s - 1990s the geostatistical methods and applications expanded from mining to address oil and gas, environmental, agriculture, fisheries, etc. with many important open source developments.</p>
<p>Why was subsurface engineering and geoscience earlier in the development of data science? Because, necessity is the mother of invention! Complicated, heterogeneous, sparsely sampled, vast systems with complicated physics and high value decisions drove us to data-driven methods. There are many other engineering fields that:</p>
<ul class="simple">
<li><p>work with homogeneous phenomenon that does not have significant spatial heterogeneity, continuity, nor uncertainty</p></li>
<li><p>have exhaustive sampling of the population relative to the modeling purpose and do not need an estimation model with uncertainty</p></li>
<li><p>have well understood physics and can model the entire system with second and third paradigm phenomenon</p></li>
</ul>
<p>As a result, many of us subsurface engineers and geoscientists are learning, applying and teaching data science.</p>
</section>
<section id="machine-learning">
<h2>Machine Learning<a class="headerlink" href="#machine-learning" title="Permalink to this heading">#</a></h2>
<p>Now we are ready to define machine learning, a <a class="reference external" href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning from Wikipedia</a> article definition may be summarized and dissected as follows. Machine learning is the study of a:</p>
<ol class="arabic simple">
<li><p><strong>toolbox</strong> - algorithms and mathematical models that computer systems use</p></li>
<li><p><strong>learning</strong> - to progressively improve their performance on a specific task with</p></li>
<li><p><strong>training data</strong> - machine learning algorithms train a mathematical model to sample data,</p></li>
<li><p><strong>general</strong> - in order to make predictions or decisions without being explicitly programmed to perform the task.</p></li>
</ol>
<p>Let‚Äôs highlight some important concepts. Machine learning is a large set of algorithms, (a numerical toolbox), that adapt to the problem (learning), these algorithms learn from training data (training data) and a single algorithm may be trained to and applied to many problems (general).</p>
<p>Near the end of the article there is an important statement that many stop short and miss.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Use machine learning,</p>
<p>Where it is infeasible to develop an algorithm of specific instructions for performing the task</p>
</div>
<p>In other words, if you know the engineering theory, the geoscience principals, the physics, the chemical reaction, the free body diagram, etc. use that and don‚Äôt jump to use data science as a crutch instead of learning the fundamental concepts (paraphrased from personal communication with Professor Carlos Torres-Verdin).</p>
</section>
<section id="machine-learning-prerequisite-definitions">
<h2>Machine Learning Prerequisite Definitions<a class="headerlink" href="#machine-learning-prerequisite-definitions" title="Permalink to this heading">#</a></h2>
<p>To understand data science we need to first differentiate between the population and the sample.</p>
<section id="population">
<h3>Population<a class="headerlink" href="#population" title="Permalink to this heading">#</a></h3>
<p>Exhaustive, finite list of property of interest over area of interest. Generally the entire population is not accessible, e.g. exhaustive set of porosity at each location within a gas reservoir</p>
</section>
<section id="sample">
<h3>Sample<a class="headerlink" href="#sample" title="Permalink to this heading">#</a></h3>
<p>The set of values, locations that have been measured, e.g. porosity data from well-logs within a reservoir.</p>
<p>What are we measuring in our samples? Each distinct type of measure is called a variable or feature.</p>
</section>
<section id="variable-feature">
<h3>Variable / Feature<a class="headerlink" href="#variable-feature" title="Permalink to this heading">#</a></h3>
<p>Any property measured / observed in a study, and in data science only the term feature is used. Note, all of us statisticians we are more accustomed with the term ‚Äúvariable‚Äù, but it is the same. Here‚Äôs some examples of features:</p>
<ul class="simple">
<li><p>porosity measured from 1.5 inch diameter, 2 inch long core plugs extracted from the Miocene-aged Tahiti field in the Gulf of Mexico</p></li>
<li><p>permeability modeled from porosity (neutron density well log) and rock facies (interpreted fraction of shale logs) at 0.5 foot resolution along the well bore in the Late Devonian Leduc formation in the Western Canadian Sedimentary Basin.</p></li>
<li><p>blast hole cuttings nickel grade aggregated over 8 inch diameter 10 meter blast holes at Voisey‚Äôs Bay Mine, Proterozoic gneissic complex.</p></li>
</ul>
<p>Did you see what I did? I specified what was measured, how it was measured, and over what scale was it measured. This is important because how the measure is made changes the veracity (level of certainty in the measure) and different methods actually may results in different results so we may need to reconcile multiple measurement methods of the same thing, so we store each vintage as separate features. Also, the scale is very important due to volume variance effect, with increasing support volume (sample size) the variance reduces due to volumetric averaging resulting in regression to the mean. I have a <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/GeostatsPy_volume_variance.html">chapter on volume-variance</a> in my Applied Geostatistics in Python e-book for those interested in more details.</p>
<p>Additionally, our subsurface measures often requires significant analysis, interpretation, etc. We don‚Äôt just hold a tool up to the rock and get the number, we have a thick layer of engineering and geoscience interpretation to map from measurement to a useable feature. Consider this carbonate thin section from Bureau of Economic Geology, The University of Texas at Austin from <a class="reference external" href="http://www.beg.utexas.edu/lmod/_IOL-CM07/old-4.29.03/cm07-step05.htm">geoscience course</a> by F. Jerry Lucia.</p>
<figure style="text-align: center;">
  <img src="_static/concepts/thin_section.png" style="display: block; margin: 0 auto; width: 50%;">
  <figcaption style="text-align: center;">Carbonate thin section image (from <a href="http://www.beg.utexas.edu/lmod/_IOL-CM07/old-4.29.03/cm07-step05.htm">this link</a> by F. Jerry Lucia).</figcaption>
</figure>
<p>Note, the blue dye indicates the void space in the rock, but is porosity the blue area divided by the total area of the sample? That would be the ‚Äútotal porosity‚Äù and not all of it may contribute to fluid flow in the rock as it is not sufficiently connected; therefore, we need to go from total porosity to ‚Äúeffective porosity‚Äù, requiring interpretation. Even porosity one of the most simple (observable in well logs and linearly averaging) features doesn‚Äôt escape this necessary interpretation.</p>
</section>
<section id="predictor-and-response-features">
<h3>Predictor and Response Features<a class="headerlink" href="#predictor-and-response-features" title="Permalink to this heading">#</a></h3>
<p>To understand the difference between predictor and response features let‚Äôs look at the most concise, simple expression of a machine learning model.</p>
<figure style="text-align: center;">
  <img src="_static/concepts/ML_equation.png" style="display: block; margin: 0 auto; width: 50%;">
  <figcaption style="text-align: center;">The fundamental predictive machine learning model that maps from predictor to response features.</figcaption>
</figure>
<p>The predictors (or independent) features (or variables) the model inputs, i.e., the <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, response (or dependent) feature(s) (or variable(s)) are the model output, <span class="math notranslate nohighlight">\(y\)</span>, and there is an error term, <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p>Machine Learning is all about estimating such models, <span class="math notranslate nohighlight">\(\hat{ùëì}\)</span>, for two purposes, inference or prediction.</p>
</section>
<section id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this heading">#</a></h3>
<p>Inference must precede prediction, because inference is going from the limited sample to a model of the population. Inference is learning about the system, <span class="math notranslate nohighlight">\(\hat{f}\)</span>, for example,</p>
<ul class="simple">
<li><p>what is the relationship between the features?</p></li>
<li><p>what feature are most important?</p></li>
<li><p>what are the complicated interactions between all the features?</p></li>
</ul>
<p>A good example of a inferential problem is imagine if I walk in the room and pull a coin out of my pocket and flip it 10 times and get 3 heads and 7 tails and then ask you, ‚ÄúIs this is a fair coin?‚Äù.</p>
<ul class="simple">
<li><p>you take the sample, the results from 10 coin tosses, to make an inference about the population, i.e., whether the coin is fair or not? Note, in this example the coin is the population!</p></li>
</ul>
</section>
<section id="prediction">
<h3>Prediction<a class="headerlink" href="#prediction" title="Permalink to this heading">#</a></h3>
<p>Once we have completed our inference to model the population from the sample, we are ready to use the model of the population to predict future samples. This is prediction, going from a model of the population to estimating new samples.</p>
<ul class="simple">
<li><p>the focus of prediction is to get the most accurate estimates of future samples.</p></li>
</ul>
<p>Carrying on with our coin example, if you declare that my coin was biased towards tails, now you are able to predict the outcome for the next 10 coin tosses, before I toss the coin again. This is prediction.</p>
<p>How do you know if you are doing inferential or predictive machine learning. In general, inferential machine learning is conducted only with the predictor features, also known as unsupervised machine learning (because there are no response labels). This includes:</p>
<ul class="simple">
<li><p><strong>cluster analysis</strong> - that learns ways to split the data into multiple distinct populations to improve the subsequent models</p></li>
<li><p><strong>dimensionality reduction</strong> - that projects the predictor features to a small number of new features that better describe the system with less noise</p></li>
</ul>
<p>When you‚Äôre doing predictive machine learning you have both the predictor and the response feature(s), also known as supervised learning, and the model is trained and tuned to predict new samples.</p>
</section>
</section>
<section id="training-and-tuning-machine-learning-models">
<h2>Training and Tuning Machine Learning Models<a class="headerlink" href="#training-and-tuning-machine-learning-models" title="Permalink to this heading">#</a></h2>
<p>The predictive machine learning we apply a general model training and testing workflow that is illustrated here.</p>
<figure style="text-align: center;">
  <img src="_static/concepts/ML_workflow.png" style="display: block; margin: 0 auto; width: 100%;">
  <figcaption style="text-align: center;">Standard predictive machine learning modeling workflow.</figcaption>
</figure>
<p>Let‚Äôs walk through the steps,</p>
<ol class="arabic simple">
<li><p><strong>Train and Test Split</strong> the available data into train and test mutually exclusive, exhaustive subsets. Typically 15% - 30% of the data are withheld and assigned to test and the remainder to the train.</p></li>
<li><p><strong>Remove the Test Split</strong> from the analysis. The test data cannot be used to train the predictive model. Any information from test data to inform the model training is considered information leakage and compromises the accuracy of the model.</p></li>
<li><p><strong>Train a Very Simple Model</strong> with the training data. Set the hyperparameters such that the model is as simple as reasonable and then train the model parameters with the available training data.</p></li>
<li><p><strong>Train Models from Simple to Complicated</strong> by repeating the previous step with the model hyperparameters incrementally increasing the model complexity. The result is a suite of models of variable complexity all fit, trained model parameters to minimize misfit with the training data.</p></li>
<li><p><strong>Access Performance with the Withheld Testing Data</strong> by retrieving the testing data and summarizing the error over all these withheld data.</p></li>
<li><p><strong>Select the Hyperparameters</strong> that minimize the testing error by picking the model from the previous step with the minimum testing error. The associated hyperparameters are known as the tuned hyperparameters.</p></li>
<li><p><strong>Retrain the Model</strong> with all of the data (train and test combined) with the tuned hyperparameters.</p></li>
</ol>
<p>You may have some questions, let‚Äôs anticipate and answer them to better defend and explain this workflow.</p>
<p><strong>What is the outcome from the empirical approach expressed in steps 1 through 6?</strong> Just the tuned hyperparameters. We would never use the selected model in step 6 as it would omit valuable data used for testing.</p>
<p><strong>Why not just train the model with all of the data?</strong> We can always minimize the training error by selecting hyperparameters that result in a very complicated model. More complicated models are more flexible, and with enough flexibility will be able to perfectly fit all of the training data, but that model would not do a very good job with making predictions for predictor feature values not used to train the model. The most complicated, overfit model would always win (more on overfit soon).</p>
<p>In other words, this approach is an attempt to simulate the model use, a form of dress rehearsal, to find the level of complexity that does the best job making new predictions. I‚Äôve said model parameters and model hyperparameters a bunch of times, so I owe you their definitions.</p>
</section>
<section id="model-parameters-and-model-hyperparameters">
<h2>Model Parameters and Model Hyperparameters<a class="headerlink" href="#model-parameters-and-model-hyperparameters" title="Permalink to this heading">#</a></h2>
<p>Model parameters are fit during training phase to minimize error at the training data, i.e., model parameters are trained with training data and control model fit to the data. For the polynomial predictive machine learning model from the machine learning workflow example above, the model parameters are the polynomial coefficients, e.g., <span class="math notranslate nohighlight">\(b_3\)</span>, <span class="math notranslate nohighlight">\(b_2\)</span>, <span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(c\)</span> (often called <span class="math notranslate nohighlight">\(b_0\)</span>) for the third order polynomial model.</p>
<figure style="text-align: center;">
  <img src="_static/concepts/parameters.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;">Model parameters are adjusted to fit of the model to the data, i.e., model parameters are trained to minimize error over the training data (x markers).</figcaption>
</figure>
<p>Model hyperparameters are very different. They do not constrain the model fit to the data directly, instead they constrain the model complexity.
The model hyperparameters are selected (call tuning) to minimize error at the withheld testing data. Going back to our polynomial predictive machine learning example, the choice of polynomial order is the model hyperparameter.</p>
<figure style="text-align: center;">
  <img src="_static/concepts/hyperparameters.png" style="display: block; margin: 0 auto; width: 100%;">
  <figcaption style="text-align: center;">Model hyperparameters are adjusted to change the model complexity / flexibilty, i.e., model hyperparameters are tuned to minimize error over the withheld testing data (solid circles).</figcaption>
</figure>
</section>
<section id="regression-and-classification">
<h2>Regression and Classification<a class="headerlink" href="#regression-and-classification" title="Permalink to this heading">#</a></h2>
<p>Before we proceed we need to define regression and classification.</p>
<ul class="simple">
<li><p><strong>Regression</strong> - a predictive machine learning model where the response feature(s) is continuous.</p></li>
<li><p><strong>Classification</strong> - a predictive machine learning model where the response feature(s) is categorical.</p></li>
</ul>
<p>It turns out that for each of these we need to build different models and use different methods to score these models. For the remainder of this discussion we will focus on regression, but in later chapters we introduce classification models as well. Now, to better understand predictive machine learning model tuning we need to understand the sources of testing error.</p>
</section>
<section id="sources-of-predictive-machine-learning-testing-error">
<h2>Sources of Predictive Machine Learning Testing Error<a class="headerlink" href="#sources-of-predictive-machine-learning-testing-error" title="Permalink to this heading">#</a></h2>
<p>Mean square error (MSE) is a standard way to express error. Mean squared error is known as a norm because we at taking a vector of error (over all of the data) and summarizing with a single, non-negative value, and specifically as the L2 norm because the errors are squared before they are summed,</p>
<div class="math notranslate nohighlight">
\[
MSE = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual observation of the response feature and <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the model estimate over data indexed, <span class="math notranslate nohighlight">\(i = 1,\ldots,n\)</span>. The <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is estimated with our predictive machine learning model with a general form,</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_i = \hat{f}(x^1_i, \ldots , x^m_i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{f}\)</span> is our predictive machine learning model and <span class="math notranslate nohighlight">\(x^1_i, \ldots , x^m_i\)</span> are the predictor feature values for the <span class="math notranslate nohighlight">\(i^{th}\)</span> data.</p>
<p>Of course, MSE can be calculated over training data and this is often the loss function that we minimize for training the model parameters for regression models.</p>
<div class="math notranslate nohighlight">
\[
MSE_{train} = \frac{1}{n_{train}} \sum_{i=1}^{n_{train}} \left( y_i - \hat{y}_i \right)^2
\]</div>
<p>and MSE can be calculated over the withheld testing data for hyperparameter tuning of regression models. Note, the L2 norm has a lot of nice properties including a continuous error function that can be differentiated over all values of error, but it is sensitive to data outliers that may have a disproportionate impact on the sum of the squares.</p>
<p>If we take the <span class="math notranslate nohighlight">\(MSE_{test}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
MSE_{test} = \frac{1}{n_{test}} \sum_{i=1}^{n_{test}} \left( y_i - \hat{f}(x^1_i, \ldots , x^m_i) \right)^2
\]</div>
<p>but we pose it as the expected test square error we get this form,</p>
<div class="math notranslate nohighlight">
\[
E \left[ \left( y_0 - \hat{f}(x^1_0, \ldots , x^m_0) \right)^2 \right]
\]</div>
<p>where we use the <span class="math notranslate nohighlight">\(_0\)</span> notation to indicate data samples not in the training dataset split, but in the withheld testing split. We can expand the quadratic and group the terms to get this convenient decomposition of expect test square error into three additive sources (derivation is available in <a class="reference external" href="https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr_1_1?crid=32MDGH9EIGR9T&amp;dib=eyJ2IjoiMSJ9.p0bVgWPuChRIt10algkzSRLwDHzW4bCihIh1RA6GGZDYFtIQN37sOnIqDS5rCJ4fF5dgqsleBiGbmgJUSXISlcmayLc6C0aOcXVX8iCtyZElt9qVbd-Dvq9P3x4KTBlzHCFHDtjz0ImJUAd3LhT6D6KhOtiHOAveaz8xiE4jU1ah6LlDo0xzGoQVDXzNE6ODFzysbfBvzJMRGXhLbQBD292ixuH_tTBTPZwOzNGzpIw.TpRqJ1llQqMdsDXdXhWr0WQIhzTn6rpjDKSAzA10E_I&amp;dib_tag=se&amp;keywords=hastie+machine+learning&amp;qid=1728683359&amp;s=books&amp;sprefix=hastie+machine+learn%2Cstripbooks%2C187&amp;sr=1-1">Hastie et al., 2009</a>,</p>
<figure style="text-align: center;">
  <img src="_static/concepts/three_error.png" style="display: block; margin: 0 auto; width: 90%;">
  <figcaption style="text-align: center;">Model error in testing with three additive components, model variance, model bias and irreducible error.</figcaption>
</figure>
<p>Let‚Äôs explain these three additive sources of error over test data, i.e., the error we expect in the real-world use of our model to predict for cases not used to train the model,</p>
<ol class="arabic simple">
<li><p><strong>Model Variance</strong> - is error due to sensitivity to the dataset, for example a simple model like linear regression does not change much if we change the training data, but a more complicated model like a fifth order polynomial model will jump around a lot as we change the training data. More model complexity tends to increase model variance.</p></li>
<li><p><strong>Model Bias</strong> -  is error due to using an approximate model, i.e., the model is too simple to fit the natural phenomenon. A very simple model is inflexible and will generally have higher model bias while a more complicated model is flexible enough to fit the data and will have lower model bias.</p></li>
<li><p><strong>Irreducible Error</strong> - is due to missing and incomplete data. There may be important features that were not sampled, or ranges of feature values that were not sampled. This is the error due to data limitations that cannot be address by the machine learning model hyperparameter tuning for optimum complexity; therefore, irreducible error is constant over the range of model complexity.</p></li>
</ol>
<p>Now we can take these three additive sources of error and produce an instructive plot,</p>
<figure style="text-align: center;">
  <img src="_static/concepts/variance_bias_tradeoff.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;">Model error in testing vs. model complexity with three additive error components, model variance, model bias and irreducible error.</figcaption>
</figure>
<p>showing hyperparameter tuning is an optimization of the model variance and bias trade off. We select the model complexity via hyperparameters that results in a model that is not too simple (too inflexible) is underfit or not too complicated (too flexible) is overfit. Now, let‚Äôs talk a bit more about under and overfit.</p>
</section>
<section id="underfit-and-overfit-models">
<h2>Underfit and Overfit Models<a class="headerlink" href="#underfit-and-overfit-models" title="Permalink to this heading">#</a></h2>
<p>First visualize under- and overfit first with a simple prediction problem with one predictor feature and one response feature, here‚Äôs a model that is too simple beside a model that is too complicated.</p>
<figure style="text-align: center;">
  <img src="_static/concepts/under_overfit.png" style="display: block; margin: 0 auto; width: 100%;">
  <figcaption style="text-align: center;">Example underfit model (left), a model that is too simple / inflexible and overfit model (right), a model that is too complicated / flexible.</figcaption>
</figure>
<p>It is clear that this simple model is not sufficiently flexible to fit the data, this is likely an underfit model while the overfit model perfectly fits all of the training data and is likely overfit. To better understand the difference, let‚Äôs plot the train error with the test error we showed previously.</p>
<figure style="text-align: center;">
  <img src="_static/concepts/overfit.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;">Model error in training and testing vs. model complexity with illustration of under- and overfit model regions.</figcaption>
</figure>
<ul class="simple">
<li><p><strong>Train Error</strong> - as we increase model complexity it will continue to decrease and at some point reach zero when the model is flexible enough to perfectly fit the training data.</p></li>
<li><p><strong>Test Error</strong> - as shown above is the result of model variance, model bias and irreducible error, and we expect the model variance-bias trade-off to result in an optimum model complexity that minimizes the error.</p></li>
</ul>
<p>From these observations we can define underfit and overfit as follows,</p>
<ul class="simple">
<li><p><strong>underfit</strong> models are too simple and continue to reduce testing error as complexity increases. These models are too conservative and lack the flexibility to fit the natural phenomenon.</p></li>
<li><p><strong>overfit</strong> models are too complicated and although they continue to reduce training error as complexity increases the testing error is actually increasing. These models may deceive us into believing that we know more than we actually do.</p></li>
</ul>
</section>
<section id="more-about-training-and-testing-splits">
<h2>More About Training and Testing Splits<a class="headerlink" href="#more-about-training-and-testing-splits" title="Permalink to this heading">#</a></h2>
<p>Here are some more information on training and testing splits,</p>
<ul class="simple">
<li><p><strong>Proportion withheld for testing</strong> - meta studies have identified that between 15% and 30% is typically optimum. To understand the underlying trade-off imagine if we only withhold 2% of the data for test, then we will have a lot of training data to train the model as well as possible, but the model will not be well-tested over a wide range of prediction cases. Now image we withhold 70% of the data for test, then with only 30% of the data available for training we will do a very good job testing our somewhat poor model. It is a trade off between build a good model and testing this model well.</p></li>
<li><p><strong>k-fold Cross Validation</strong> - the train and test workflow above known as a cross validation approach, but there are many other methods. With k-fold cross validation we divide the data into k mutually exclusive, exhaustive equal size sets (called folds) for repeat the model train and test k times with each fold having a turn being withheld. Then we average the error norm over the folds to provide a single error for hyperparameter tuning. This method removes the sensitivity to the exact train and test split so it is often seen as more robust. The proportion of testing is implicit to the choice of k, for k = 3, 33% of data are withheld for each fold and for k=5, 20% of data are withheld for each fold.</p></li>
<li><p><strong>Train, validate and test</strong> - there is an alternative approach with three exhaustive, mutually exclusive segments. The train data subset is the same as train above and is used to train the model parameters, validate is like test above and is used to tune the model hyperparameters, and the test data subset is applied to check the model trained on all the data with the tuned hyperparameters. The philosophy is that this final check is performed with data that was not at all involved with model construction, including training model parameters nor tuning model hyperparameters. There‚Äôs two reasons that I push back on this method. Firstly, what do we do if we don‚Äôt quite like the performance in this testing phase, do we have a fourth subset for another test? Also, we can never deploy a model that is not trained with all available data; therefore, we will still have to train with the test subset.</p></li>
<li><p><strong>Spatial Fair Train and Test Splits</strong> - Dr. Julian Salazar suggested that for spatial prediction problems that random train and test split may not fair. He proposed a <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0920410521015023">fair train and test split method</a> for spatial prediction models that splits the data based on the difficulty of the planned use of the model. Prediction difficulty is related to kriging variance that accounts for spatial continuity and distance offset. If the model will be used to impute data with small offsets from available data then construct a train and test split with train data close to test data, and if the model will be used to predict a large distance offsets then perform splits the result is large offsets between train and test data. With this method the tuned model may vary based on the planned use for the model.</p></li>
</ul>
</section>
<section id="ethical-and-professional-practice-concerns-with-data-science">
<h2>Ethical and Professional Practice Concerns with Data Science<a class="headerlink" href="#ethical-and-professional-practice-concerns-with-data-science" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1602.04938.pdf">Rideiro et al. (2016)</a> trained a logistic regression classifier with 20 wolf and dog images to detect the difference between wolves and dogs. The input is a photo of a dog or wolf and the output is the probability of dog and the compliment, the probability of wolf. The model worked well until this example here (see the left side below),</p>
<figure style="text-align: center;">
  <img src="_static/concepts/dog_and_wolf.png" style="display: block; margin: 0 auto; width: 80%;">
  <figcaption style="text-align: center;">Example dog misclassified as a wolf (left) and pixels that resulted in this misclassification (right), image taken from Rideiro et al. (2016).</figcaption>
</figure>
<p>where this results in a high probability of wolf. Fortunately the authors were able to interrogate the model and determine the pixels that had the greatest influence of the model‚Äôs determination of wolf (see the right side above). What happened? They trained a model to check for snow in the background. As a Canadian I can assure you that many photos of wolves are in our snow filled northern regions of our country, while many dogs are photographed in grassy yards. The problem with machine learning is:</p>
<ul class="simple">
<li><p>interpretability may be low with complicated models</p></li>
<li><p>application of machine learning may become routine and trusted</p></li>
</ul>
<p>This is a dangerous combination, as the machine may become a trusted unquestioned authority. Rideiro and others state that they developed the problem to demonstrate this, but does it actually happen? Yes. I advised a team of students that attempted to automatically segment urban vs. rural environments from time lapse satellite photographs to build models of urban development. The model looked great, but I asked for an additional check with a plot of the by-pixel classification vs. pixel color. The results was a 100% correspondence, i.e., the model with all of it‚Äôs complicated convolution, activation, pooling was only looking for grey and tan pixels often associated with roads and buildings.</p>
<p>I‚Äôm not going to say, ‚ÄòSkynet‚Äô, oops I just did, but just consider these thoughts:</p>
<ul class="simple">
<li><p>New power and distribution of wealth by concentrating rapid inference with big data as more data is being shared</p></li>
<li><p>Trade-offs that matter to society while maximizing a machine learning objective function may be ignored, resulting in low interpretability</p></li>
<li><p>Societal changes, disruptive technologies, post-labor society</p></li>
<li><p>Non-scientific results such as <a class="reference external" href="https://en.wikipedia.org/wiki/Clever_Hans">Clever Hans</a> effect with models that learn from tells in the data rather than really learning to perform the task resulting catastrophic failures</p></li>
</ul>
<p>I don‚Äôt want to be too negative and to take this too far. Full disclosure, I‚Äôm the old-fashioned professor that thinks we should put of phones in our pockets, walk around with our heads up so we can greet each other and observe our amazing environments and societies. One thing that is certain, data science is changing society in so many ways and as Neil Postman in <a class="reference external" href="https://www.amazon.com/Technopoly-Surrender-Technology-Neil-Postman-ebook/dp/B004ZZJBW4/ref=sr_1_1?crid=E9SP0DFNP9JO&amp;dib=eyJ2IjoiMSJ9.ELnF0aIjkOw11vdTEQ3Tpg.NSaPUOIOz6m6i7XxpEMUJvZobANzU5baE6DlEa40Uzs&amp;dib_tag=se&amp;keywords=technolopoly&amp;qid=1728667409&amp;sprefix=technolopo%2Caps%2C127&amp;sr=8-1">Technopoly</a>)</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Neil Postman‚Äôs Quote for Technopoly</p>
<p>‚ÄúOnce a technology is admitted, it plays out its hand.‚Äù</p>
</div>
</section>
<section id="comments">
<h2>Comments<a class="headerlink" href="#comments" title="Permalink to this heading">#</a></h2>
<p>This was a basic introduction to machine learning. Much more could be done, I have other demonstrations on basics of working with DataFrames, ndarrays and many other workflows available at <a class="github reference external" href="https://github.com/GeostatsGuy/PythonNumericalDemos">GeostatsGuy/PythonNumericalDemos</a> and <a class="github reference external" href="https://github.com/GeostatsGuy/GeostatsPy">GeostatsGuy/GeostatsPy</a>.</p>
<p>I hope this was helpful,</p>
<p><em>Michael</em></p>
</section>
<section id="the-author">
<h2>The Author:<a class="headerlink" href="#the-author" title="Permalink to this heading">#</a></h2>
<p>Michael Pyrcz, Professor, The University of Texas at Austin
<em>Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions</em></p>
<p>With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers‚Äô and geoscientists‚Äô impact in subsurface resource development.</p>
<p>For more about Michael check out these links:</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?<a class="headerlink" href="#want-to-work-together" title="Permalink to this heading">#</a></h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz&#37;&#52;&#48;austin&#46;utexas&#46;edu">mpyrcz<span>&#64;</span>austin<span>&#46;</span>utexas<span>&#46;</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
</section>
<section id="more-resources-available-at-twitter-github-website-googlescholar-geostatistics-book-youtube-applied-geostats-in-python-e-book-applied-machine-learning-in-python-e-book-linkedin">
<h2>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a><a class="headerlink" href="#more-resources-available-at-twitter-github-website-googlescholar-geostatistics-book-youtube-applied-geostats-in-python-e-book-applied-machine-learning-in-python-e-book-linkedin" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Applied Machine Learning in Python: a Hands-on Guide with Code</p>
      </div>
    </a>
    <a class="right-next"
       href="MachineLearning_worklfow_construction.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine Learning Workflow Construction and Coding</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-machine-learning-concepts">Motivation for Machine Learning Concepts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#big-data">Big Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-geostatistics-and-data-analytics">Statistics, Geostatistics and Data Analytics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-science">Data Science</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cheap-and-available-compute">Cheap and Available Compute</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#availability-of-big-data">Availability of Big Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-science-and-subsurface-resources">Data Science and Subsurface Resources</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-prerequisite-definitions">Machine Learning Prerequisite Definitions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#population">Population</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample">Sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-feature">Variable / Feature</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-and-response-features">Predictor and Response Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-tuning-machine-learning-models">Training and Tuning Machine Learning Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parameters-and-model-hyperparameters">Model Parameters and Model Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-and-classification">Regression and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sources-of-predictive-machine-learning-testing-error">Sources of Predictive Machine Learning Testing Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underfit-and-overfit-models">Underfit and Overfit Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-about-training-and-testing-splits">More About Training and Testing Splits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-and-professional-practice-concerns-with-data-science">Ethical and Professional Practice Concerns with Data Science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">Comments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-author">The Author:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-work-together">Want to Work Together?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-resources-available-at-twitter-github-website-googlescholar-geostatistics-book-youtube-applied-geostats-in-python-e-book-applied-machine-learning-in-python-e-book-linkedin">More Resources Available at: Twitter | GitHub | Website | GoogleScholar | Geostatistics Book | YouTube  | Applied Geostats in Python e-book | Applied Machine Learning in Python e-book | LinkedIn</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael J. Pyrcz
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024 CC-BY-SA 4.0.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>