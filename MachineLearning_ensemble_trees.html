

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Bagging and Random Forest &#8212; Applied Machine Learning in Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'MachineLearning_ensemble_trees';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Gradient Boosting Trees" href="MachineLearning_gradient_boosting.html" />
    <link rel="prev" title="Decision Tree" href="MachineLearning_decision_tree.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/AppliedMachineLearning.jpg" class="logo__image only-light" alt="Applied Machine Learning in Python - Home"/>
    <script>document.write(`<img src="_static/AppliedMachineLearning.jpg" class="logo__image only-dark" alt="Applied Machine Learning in Python - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_concepts.html">Machine Learning Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_training_tuning.html">Training and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_workflow_construction.html">Workflow Construction and Coding</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_probability.html">Probability Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_plotting_data_models.html">Loading and Plotting Data and Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_univariate_analysis.html">Univariate Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_multivariate_analysis.html">Multivariate Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_feature_transformations.html">Feature Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_feature_ranking.html">Feature Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_feature_imputation.html">Feature Imputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_clustering.html">Cluster Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_density-based_clustering.html">Density-based Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_spectral_clustering.html">Spectral Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_PCA.html">Principal Components Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_multidimensional_scaling.html">Multidimensional Scaling</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_random_projection.html">Random Projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_ridge_regression.html">Ridge Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_LASSO_regression.html">LASSO Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html">Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_naive_Bayes.html">Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_polynomial_regression.html">Polynomial Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_knearest_neighbours.html">k-Nearest Neighbours</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_decision_tree.html">Decision Trees</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bagging Tree and Random Forest</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_gradient_boosting.html">Gradient Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_support_vector_machines.html">Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_ANN.html">Artificial Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_time_series.html">Time Series Analysis and Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusions.html">Conclusions</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/GeostatsPyDemos_Book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/GeostatsPyDemos_Book/issues/new?title=Issue%20on%20page%20%2FMachineLearning_ensemble_trees.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/MachineLearning_ensemble_trees.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bagging and Random Forest</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivations-for-bagging-and-random-forest">Motivations for Bagging and Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-model-formulation">Tree Model Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-loss-functions">Tree Loss Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-tree-model">Training the Tree Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-the-tree-model">Tuning the Tree Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-methods">Ensemble Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrap">Bootstrap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging-models">Bagging Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-tuning-bagging-models">Training and Tuning Bagging Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#out-of-bag-cross-validation">Out-of-Bag Cross Validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#number-of-estimators">Number of Estimators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimator-complexity">Estimator Complexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-bagging">Tree Bagging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest">Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-required-libraries">Load the Required Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#declare-functions">Declare Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-the-working-directory">Set the working directory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-data">Loading Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering">Feature Engineering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-add-random-noise-to-the-response-feature">Optional: Add Random Noise to the Response Feature</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-the-correlation-matrix-and-correlation-with-response-ranking">Calculate the Correlation Matrix and Correlation with Response Ranking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-and-test-split">Train and Test Split</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-the-dataframe">Visualize the DataFrame</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-statistics-for-tabular-data">Summary Statistics for Tabular Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-the-distributions">Visualize the Distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-tree-method-tree-bagging-regression">Ensemble Tree Method - Tree Bagging Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration-of-bagging-by-hand">Demonstration of Bagging by-Hand</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration-of-bagging-with-increasing-number-of-trees">Demonstration of Bagging with Increasing Number of Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-variance-vs-ensemble-model-averaging">Model Variance vs. Ensemble Model Averaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-performance-by-out-of-bag-and-feature-importance">Model Performance by Out-of-Bag and Feature Importance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-pipelines-for-clean-compact-machine-learning-code">Machine Learning Pipelines for Clean, Compact Machine Learning Code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">Comments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-author">The Author:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-work-together">Want to Work Together?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-resources-available-at-twitter-github-website-googlescholar-book-youtube-applied-geostats-in-python-e-book-linkedin">More Resources Available at: Twitter | GitHub | Website | GoogleScholar | Book | YouTube  | Applied Geostats in Python e-book | LinkedIn</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <figure style="text-align: center;">
  <img src="_static/intro/title_page.png" style="display: block; margin: 0 auto; width: 100%;">
</figure>
<section id="bagging-and-random-forest">
<h1>Bagging and Random Forest<a class="headerlink" href="#bagging-and-random-forest" title="Permalink to this heading">#</a></h1>
<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book “Applied Machine Learning in Python: a Hands-on Guide with Code”.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, <em>Applied Machine Learning in Python: A Hands-on Guide with Code</em> [e-book]. Zenodo. doi:10.5281/zenodo.15169138 <a class="reference external" href="https://doi.org/10.5281/zenodo.15169138"><img alt="DOI" src="https://zenodo.org/badge/863274676.svg" /></a></p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, <em>MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository</em> (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312. GitHub repository: <a class="github reference external" href="https://github.com/GeostatsGuy/MachineLearningDemos">GeostatsGuy/MachineLearningDemos</a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="https://zenodo.org/badge/862519860.svg" /></a></p>
</div>
<p>By Michael J. Pyrcz <br />
© Copyright 2024.</p>
<p>This chapter is a tutorial for / demonstration of <strong>Bagging and Random Forest</strong>.</p>
<p><strong>YouTube Lecture</strong>: check out my lectures on:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl">Introduction to Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/JUGo1Pu3QT4?si=ebQXv6Yglar0mYWp">Decision Tree</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/m5_wk310fho?si=up-mzVPHvniXsYE6">Random Forest</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/___T8_ixIwc?si=ozHR_eIuMF3SPTxJ">Gradient Boosting</a></p></li>
</ul>
<p>These lectures are all part of my <a class="reference external" href="https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&amp;si=XonjO2wHdXffMpeI">Machine Learning Course</a> on YouTube with linked well-documented Python workflows and interactive dashboards. My goal is to share accessible, actionable, and repeatable educational content. If you want to know about my motivation, check out <a class="reference external" href="https://michaelpyrcz.com/my-story">Michael’s Story</a>.</p>
<section id="motivations-for-bagging-and-random-forest">
<h2>Motivations for Bagging and Random Forest<a class="headerlink" href="#motivations-for-bagging-and-random-forest" title="Permalink to this heading">#</a></h2>
<p>Decision tree are not the most powerful, cutting edge method in machine learning, but,</p>
<ul class="simple">
<li><p>one of the most understandable, interpretable predictive machine learning modeling</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/spruce.png" style="display: block; margin: 0 auto; width: 50%;">
  <figcaption style="text-align: center;"> Solitary black spruce tree in Hinton, Alberta, Canada, image from https://hikebiketravel.com/6-fun-things-to-do-in-hinton-alberta-in-winter.
</figcaption>
</figure>
<ul class="simple">
<li><p>decision trees are enhanced with random forests, bagging and boosting to be one of the best models in many cases</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/forest.png" style="display: block; margin: 0 auto; width: 100%;">
  <figcaption style="text-align: center;"> Black spruce forest near Hinton, Alberta, east of Jasper National Park, Canada, image from https://en.wikivoyage.org/wiki/Hinton.
</figcaption>
</figure>
<p>Now we cover ensemble trees, tree bagging and random forest building on decision trees. First, I provide some prerequisite concepts for decision trees and then for ensemble methods.</p>
<ul class="simple">
<li><p>if you are not familiar with decision trees it may be a good idea to review the <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_decision_tree.html">Decision Tree Chapter</a>.</p></li>
</ul>
</section>
<section id="tree-model-formulation">
<h2>Tree Model Formulation<a class="headerlink" href="#tree-model-formulation" title="Permalink to this heading">#</a></h2>
<p>The prediction feature space is partitioned into <span class="math notranslate nohighlight">\(J\)</span> exhaustive, mutually exclusive regions <span class="math notranslate nohighlight">\(R_1, R_2, \ldots, R_J\)</span>. For a given prediction case <span class="math notranslate nohighlight">\(x_1,\ldots,x_m \in R_j\)</span>, the prediction is:</p>
<p><strong>Regression</strong> - the average of the training data in the region, <span class="math notranslate nohighlight">\(R_j\)</span></p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \frac{1}{|R_j|} \sum_{\mathbf{x}_i \in R_j} y_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}\)</span> is the predicted value for input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, <span class="math notranslate nohighlight">\(R_j\)</span> is the region (leaf node) that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> falls into, <span class="math notranslate nohighlight">\(|R_j|\)</span> is the number of training samples in region <span class="math notranslate nohighlight">\(R_j\)</span>, and <span class="math notranslate nohighlight">\(y_i\)</span> is the actual target values of those training samples in <span class="math notranslate nohighlight">\(R_j\)</span>.</p>
<p><strong>Classification</strong> - category with the plurality of training cases (most common case) in region <span class="math notranslate nohighlight">\(R_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \arg\max_{c \in C} \left( \frac{1}{|R_j|} \sum_{\mathbf{x}_i \in R_j} \mathbb{1}(y_i = c) \right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is the set of all possible categories, <span class="math notranslate nohighlight">\(\mathbb{1}(y_i = c)\)</span> is indicator transform, 1 if <span class="math notranslate nohighlight">\(y_i = c\)</span>, 0 otherwise, <span class="math notranslate nohighlight">\(|R_j|\)</span> is the number of training samples in region <span class="math notranslate nohighlight">\(R_j\)</span>, and <span class="math notranslate nohighlight">\(\hat{y}\)</span> is the predicted class label.</p>
<p>The predictor space, <span class="math notranslate nohighlight">\(𝑋_1,\ldots,𝑋_𝑚\)</span>, is segmented into <span class="math notranslate nohighlight">\(J\)</span> mutually exclusive, exhaustive regions, <span class="math notranslate nohighlight">\(R_j, j = 1,\ldots,J\)</span>, where the regions are,</p>
<ul class="simple">
<li><p><strong>mutually exclusive</strong> – any combination of predictor features, <span class="math notranslate nohighlight">\(x_1,\ldots,x_𝑚\)</span>, only belongs to a single region, <span class="math notranslate nohighlight">\(R_j\)</span></p></li>
<li><p><strong>exhaustive</strong> – all combinations of predictor feature values belong a region, <span class="math notranslate nohighlight">\(R_j\)</span>, i.e., all the regions, <span class="math notranslate nohighlight">\(R_j, j = 1,\ldots,J\)</span>, cover entire predictor  feature space</p></li>
</ul>
<p>All prediction cases, <span class="math notranslate nohighlight">\(x_1,\ldots,x_m\)</span> that fall in the same region, <span class="math notranslate nohighlight">\(R_j\)</span>, are estimated with the same value.</p>
<ul class="simple">
<li><p>the prediction model inherently discontinuous at the region boundaries</p></li>
</ul>
<p>For example, consider this decision tree prediction model for the production response feature, <span class="math notranslate nohighlight">\(\hat{Y}\)</span> ̂from porosity, <span class="math notranslate nohighlight">\(X_1\)</span>, predictor feature,</p>
<figure style="text-align: center;">
  <img src="_static/decision_tree/regions.png" style="display: block; margin: 0 auto; width: 80%;">
  <figcaption style="text-align: center;"> Four region decision tree with data and predictions, \(\hat{Y}(R_j) = \overline{Y}(R_j)\) by region, \(R_j, j=1,…,4\). For example, given a predictor feature value of 13% porosity, the model predicts about 2,000 MCFPD for production.
</figcaption>
</figure>
<p>How do we segment the predictor feature space?</p>
<ul class="simple">
<li><p>the set of regions based on hierarchical, binary segmentation.</p></li>
</ul>
</section>
<section id="tree-loss-functions">
<h2>Tree Loss Functions<a class="headerlink" href="#tree-loss-functions" title="Permalink to this heading">#</a></h2>
<p>For regression trees we minimize the residual sum of squares and for classification trees we minimize the weighted average Gini impurity.</p>
<p>The Residual Sum of Squares (RSS) measures the total squared difference between the actual values and predicted values in a regression tree,</p>
<div class="math notranslate nohighlight">
\[
\text{RSS} = \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(J\)</span> is the total number of regions in the tree, <span class="math notranslate nohighlight">\(R_j\)</span> is the <span class="math notranslate nohighlight">\(j\)</span> region, <span class="math notranslate nohighlight">\(y_i\)</span> is the truth value of the response feature at observation the <span class="math notranslate nohighlight">\(i\)</span> training data, and <span class="math notranslate nohighlight">\(\hat{y}_{R_j}\)</span> is the predicted value for region <span class="math notranslate nohighlight">\(R_j\)</span>, the mean of <span class="math notranslate nohighlight">\(y_i \; \forall \; i \in R_j\)</span>.</p>
<p>When a parent node splits into two child nodes ( t_L ) and ( t_R ), the weighted Gini impurity is:</p>
<div class="math notranslate nohighlight">
\[
\text{Gini}_{\text{total}} = \sum_{j=1}^{J} \frac{N_j}{N} \cdot \text{Gini}(j)
\]</div>
<p>where <span class="math notranslate nohighlight">\(J\)</span> is the total number of regions in the tree, <span class="math notranslate nohighlight">\(N\)</span> is the total number of samples in the dataset,   <span class="math notranslate nohighlight">\(N_j\)</span> is the number of samples in leaf node <span class="math notranslate nohighlight">\(j\)</span>, and <span class="math notranslate nohighlight">\(\text{Gini}(j)\)</span> is the Gini impurity of leaf node <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>The Gini impurity for a single decision tree node is calculated as,</p>
<div class="math notranslate nohighlight">
\[
\text{Gini}(j) = 1 - \sum_{c=1}^{C} p_{j,c}^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(p_{j,c}\)</span> is the proportion of class <span class="math notranslate nohighlight">\(c\)</span> samples in node <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>For classification our loss function does not compare the predictions to the truth values like our regression loss!</p>
<ul class="simple">
<li><p>the Gini impurity penalizes mixtures of training data categories! A region of all one category of training data will have a Gini impurity of 0 to contribute to the over all loss.</p></li>
</ul>
<p>Note that the by-region Gini impurity is,</p>
<ul class="simple">
<li><p><strong>weighted</strong> - by the number of training data in each regions, regions with more training data have greater impact on the overall loss</p></li>
<li><p><strong>averaged</strong> - over all the regions to calculate the total Gini impurity of the decision tree</p></li>
</ul>
<p>These losses are calculated during,</p>
<ul class="simple">
<li><p><strong>tree model training</strong> - with respect to training data to grow the tree</p></li>
<li><p><strong>tree model tuning</strong> - with respect to withheld testing data to select the optimum tree complexity.</p></li>
</ul>
<p>Let’s talk about tree model training first and then tree model tuning.</p>
</section>
<section id="training-the-tree-model">
<h2>Training the Tree Model<a class="headerlink" href="#training-the-tree-model" title="Permalink to this heading">#</a></h2>
<p>How do we calculate these mutually exclusive, exhaustive regions? This is accomplished through hierarchical binary segmentation of the predictor feature space.</p>
<p>Training a decision tree model is both,</p>
<ol class="arabic simple">
<li><p>assigning the mutual exclusive, exhaustive regions</p></li>
<li><p>building a decision tree, each region is a terminal node, also known as a leaf node</p></li>
</ol>
<p>These are the same thing! Let’s list the steps and then walk through a training a tree to demonstrate this.</p>
<ol class="arabic simple" start="0">
<li><p><strong>Assign All Data to a Single Region</strong> - this region covers the entire predictor feature space</p></li>
<li><p><strong>Scan All Possible Splits</strong> - over all regions and over all features</p></li>
<li><p><strong>Select the Best Split</strong> - this is greedy optimization, i.e., the best split minimizes the residual sum of squares of errors over all the training data <span class="math notranslate nohighlight">\(y_i\)</span> over all of the regions <span class="math notranslate nohighlight">\(j = 1,\ldots,J\)</span>.</p></li>
<li><p><strong>Iterate Until Very Overfit</strong> - return to step 1 for the next split until the tree is very overfit.</p></li>
</ol>
<p>For brevity we stop here, and make these observations,</p>
<ul class="simple">
<li><p>hierarchical, binary segmentation is the same as sequentially building a decision tree, each split adds a new decision node and increases the number of leaf nodes by one.</p></li>
<li><p>the simple decision trees are in the complicated decision tree, i.e., if we build an <span class="math notranslate nohighlight">\(8\)</span> leaf node model, we have the <span class="math notranslate nohighlight">\(8, 7, \ldots, 2\)</span> leaf node model by sequentially removing the decision nodes, in the order of last one is the first one to remove.</p></li>
<li><p>the ultimate overfit model is number of leaf nodes equal to the number of training data. In this case, the training error is 0.0 as have one region for each training data a we estimate with the training data response feature values for all the at the training data cases.</p></li>
</ul>
</section>
<section id="tuning-the-tree-model">
<h2>Tuning the Tree Model<a class="headerlink" href="#tuning-the-tree-model" title="Permalink to this heading">#</a></h2>
<p>To tune the decision tree we take the very overfit trained tree model,</p>
<ul class="simple">
<li><p>sequentially cut the last decision node</p></li>
<li><p>i.e., prune the last branch of the decision tree</p></li>
</ul>
<p>Since the simpler trees are inside the complicated tree!</p>
<p>We can calculate test error as we prune and select tree with minimum test error</p>
<p>We overfit the decision tree model, with a large number of leaf nodes and then we reduce the number of leaf nodes while tracking the test error.</p>
<ul class="simple">
<li><p>we select the number of leaf nodes that minimize the testing error.</p></li>
<li><p>since we are sequentially removing the last branch to simplify the tree, we call model tuning <strong>pruning</strong> for decision trees</p></li>
</ul>
<p>Let’s discuss decision tree hyperparameters. I prefer number of leaf nodes as my decision tree hyperparameter because it provides,</p>
<ul class="simple">
<li><p><strong>continuous, uniform increase in complexity</strong> -  equal steps in increased complexity without jumps</p></li>
<li><p><strong>intuitive control on complexity</strong> - we can understand and relate the <span class="math notranslate nohighlight">\(2, 3, \ldots, 100\)</span> leaf node decision trees</p></li>
<li><p><strong>flexible complexity</strong> - the tree is free to grow in any manner to reduce training error, including highly asymmetric decision trees</p></li>
</ul>
<p>There are other common decision tree hyperparameters including,</p>
<ul class="simple">
<li><p><strong>Minimum reduction in RSS</strong> – related to the idea that incremental increase in complexity must be offset by sufficient reduction in training error. This could stop the model early, for example, a split with low reduction in training error could lead to a subsequent split with a much larger reduction in training error</p></li>
<li><p><strong>Minimum number of training data in each region</strong> – related to the concept of accuracy of the by-region estimates, i.e., we need at least <span class="math notranslate nohighlight">\(n\)</span> data for a reliable mean and most common category</p></li>
<li><p><strong>Maximum number of levels</strong> – forces symmetric trees, similar number of splits to get to each leaf node. There is a large change in model complexity with change in the hyperparameter.</p></li>
</ul>
</section>
<section id="ensemble-methods">
<h2>Ensemble Methods<a class="headerlink" href="#ensemble-methods" title="Permalink to this heading">#</a></h2>
<p>What is the Testing Accuracy of Our Predictive Machine Learning Models?</p>
<p>Recall the equation for expected test error has three components.</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}\left[(y_0 - \hat{f}(x_1^0, \ldots, x_m^0))^2\right] =
\left(\mathbb{E}[\hat{f}(x_1^0, \ldots, x_m^0)] - f(x_1^0, \ldots, x_m^0)\right)^2 +
\mathbb{E}\left[\left(\hat{f}(x_1^0, \ldots, x_m^0) - \mathbb{E}[\hat{f}(x_1^0, \ldots, x_m^0)]\right)^2\right] +
\sigma_\varepsilon^2
\]</div>
<p>There can be labeled as:</p>
<div class="math notranslate nohighlight">
\[
\text{Expected Test Error} = \text{Model Bias}^2 + \text{Model Variance} + \text{Irreducible Error}
\]</div>
<p>where,</p>
<ul class="simple">
<li><p><strong>Model Variance</strong> - is the error in the model predictions due to sensitivity to the data, i.e., what if we used different training data?</p></li>
<li><p><strong>Model Bias</strong> - is error in the model predictions due to using an approximate model / model is too simple</p></li>
<li><p><strong>Irreducible Error</strong> - is error in the model predictions due to missing features and limited samples can’t be fixed with modeling / entire feature space is not sampled</p></li>
</ul>
<p>Now we can visualize the model variance and bias tradeoff as,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/tradeoff.png" style="display: block; margin: 0 auto; width: 50%;">
  <figcaption style="text-align: center;"> Model variance and bias trade-off, for simple to complicated predictive machine learning models.
</figcaption>
</figure>
<p>Model variance limits the complexity and text accuracy of our models.</p>
<p><strong>How Can We Reduce Model Variance?</strong> - so that we can use more complicated and more accurate models.</p>
<p>By standard error in the average, we observe the reduction in variance by averaging!</p>
<div class="math notranslate nohighlight">
\[
\sigma_{\bar{x}}^2 = \frac{\sigma^2_s}{n}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2_s\)</span> is the sample variance, <span class="math notranslate nohighlight">\(n\)</span> is the number of samples, and <span class="math notranslate nohighlight">\(\sigma_{\bar{x}}^2\)</span> is the variance of the average under the assumption of independent, identically distributed sampling.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/tradeoffensemble.png" style="display: block; margin: 0 auto; width: 50%;">
  <figcaption style="text-align: center;"> Model variance and bias trade-off, for simple to complicated predictive machine learning models with model variance reduced by averaging.
</figcaption>
</figure>
<p>We can reduce model variance by calculating many estimates and averaging them together. We will need to make <span class="math notranslate nohighlight">\(B\)</span> estimates,</p>
<div class="math notranslate nohighlight">
\[
\hat{y}^{(b)} = \hat{f}^{(b)}(X_1, \ldots, X_m), \quad b = 1, \ldots, B
\]</div>
<p>where,</p>
<p><span class="math notranslate nohighlight">\(\hat{y}^{(b)}\)</span> is the prediction made by the <span class="math notranslate nohighlight">\(b^{th}\)</span> model in the ensemble, <span class="math notranslate nohighlight">\(\hat{f}^{(b)}\)</span> is the <span class="math notranslate nohighlight">\(b^{th}\)</span> estimator, <span class="math notranslate nohighlight">\(X_1, \ldots, X_m\)</span> is the predictor features, and <span class="math notranslate nohighlight">\(B\)</span> is the total number of estimators (the multiple models).</p>
<p>Then our ultimate estimate will be the average (regression) or plurality (classification) of our estimates,</p>
<ul class="simple">
<li><p>regression ensemble estimate,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y} = \frac{1}{B} \sum_{b=1}^{B} \hat{y}^{(b)}
\]</div>
<ul class="simple">
<li><p>classification ensemble estimate,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y} = \arg\max_y \sum_{b=1}^{B} \mathbb{I}(\hat{y}^{(b)} = y)
\]</div>
<p>This requires multiple prediction models, <span class="math notranslate nohighlight">\(f^{b}, b = 1,\ldots, B\)</span> to make <span class="math notranslate nohighlight">\(B\)</span> predictions,</p>
<div class="math notranslate nohighlight">
\[
\hat{y}^{(b)} = \hat{f}^{(b)}(X_1, \ldots, X_m), \quad b = 1, \ldots, B
\]</div>
<figure style="text-align: center;">
  <img src="_static/ensemble/multimodels.png" style="display: block; margin: 0 auto; width: 100%;">
  <figcaption style="text-align: center;"> Multiple models to make multiple predictions to reduce model variance by averaging over the ensemble.
</figcaption>
</figure>
<p>But we only have access to a single dataset, <span class="math notranslate nohighlight">\(Y,X_1,\ldots,X_m\)</span>; therefore, every model will be the same,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/multimodelssame.png" style="display: block; margin: 0 auto; width: 90%;">
  <figcaption style="text-align: center;"> Multiple models to make multiple predictions to reduce model variance by averaging over the ensemble, with the same data result in the same model and the same predictions.
</figcaption>
</figure>
<p>Our models are generally deterministic, train with the same data and hyperparameters and we get the same estimate.</p>
</section>
<section id="bootstrap">
<h2>Bootstrap<a class="headerlink" href="#bootstrap" title="Permalink to this heading">#</a></h2>
<p>One source of uncertainty is the paucity of data.</p>
<ul class="simple">
<li><p>Do these 200 or so wells provide a precise (and accurate estimate) of the mean? standard deviation? skew? P13?</p></li>
<li><p>What is the impact of uncertainty in the mean porosity, for example, <span class="math notranslate nohighlight">\(20\% \pm 2\%\)</span>?</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/spatialdata.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Samples and population, from Bootstrap chapter of Applied Geostatistics in Python e-book.
</figcaption>
</figure>
<p>What if we had <span class="math notranslate nohighlight">\(L\)</span> different datasets? <span class="math notranslate nohighlight">\(L\)</span> parallel universes where we collected <span class="math notranslate nohighlight">\(n\)</span> samples from the inaccessible truth (the population).</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/spatialdatareal.png" style="display: block; margin: 0 auto; width: 90%;">
  <figcaption style="text-align: center;"> Multiple dataset realizations from the truth population, from Bootstrap chapter of Applied Geostatistics in Python e-book.
</figcaption>
</figure>
<ul class="simple">
<li><p>but we only exist in 1 universe <span class="math notranslate nohighlight">\(\rightarrow\)</span> this is not possible.</p></li>
</ul>
<p>Instead we sample <span class="math notranslate nohighlight">\(n\)</span> times from the dataset with replacement,</p>
<ul class="simple">
<li><p>bootstrap realizations of the data</p></li>
</ul>
<p>that vary by due to some samples being left out and others sampled multiple times.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/spatialdatarealdata.png" style="display: block; margin: 0 auto; width: 90%;">
  <figcaption style="text-align: center;"> Multiple dataset bootstrap datasets.
</figcaption>
</figure>
<p>Now, here’s a definition of bootstrap,</p>
<ul class="simple">
<li><p>method to assess the uncertainty in a sample statistic by repeated random sampling with replacement
simulating the sampling process to acquire dataset realizations</p></li>
</ul>
<p>Under the assumptions,</p>
<ul class="simple">
<li><p><strong>sufficient sample</strong> - enough data to infer the population parameters. Bootstrap cannot make up for too few data!</p></li>
<li><p><strong>representative sampling</strong> - bias in the sample will be passed to the bootstrap uncertainty model, for example, if the mean is biased in the sample, then the bootstrap uncertainty model will be centered on the biased mean from the sample. We must first debias our data.</p></li>
</ul>
<p>There are also various limitations for bootstrap, including,</p>
<ul class="simple">
<li><p><strong>stationarity</strong> - the statistics from the sample are assumed to be constant over the model space</p></li>
<li><p><strong>one uncertainty source only</strong> - only accounts for uncertainty due to too few samples, for example, no uncertainty due to changes away from data</p></li>
<li><p><strong>does not account for area of interest</strong> - larger model region or smaller model region, bootstrap uncertainty does not change.</p></li>
<li><p><strong>independence between the samples</strong> - does not account for correlation, relationships between the samples</p></li>
<li><p><strong>no local conditioning</strong> - does not account for other local information sources</p></li>
</ul>
<p>We could summarize all of this limitations as, bootstrap does not account for the spatial (or temporal) context of the data.</p>
<ul class="simple">
<li><p>there is a form of bootstrap, known as spatial bootstrap that does account for spatial context, <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0098300424000414">spatial bootstrap and bagging for ensemble machine learning</a>.</p></li>
</ul>
<p>Let’s visualize bootstrap for the case of calculating uncertainty in sample mean estimated ultimate recovery (EUR) given <span class="math notranslate nohighlight">\(n=10\)</span> observations from 10 wells.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/bootstrap1.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> \(n = 10\) samples with replacement to calculate a single realization of the sample mean.
</figcaption>
</figure>
<p>Now we repeat and calculate a <span class="math notranslate nohighlight">\(2^{nd}\)</span> realization of the sample mean,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/bootstrap2.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Second realization of \(n = 10\) samples with replacement to calculate the second realization of the sample mean.
</figcaption>
</figure>
<p>and a third realization of the data to calculate the <span class="math notranslate nohighlight">\(3^{rd}\)</span> realization of the sample mean,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/bootstrap3.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Third realization of \(n = 10\) samples with replacement to calculate the third realization of the sample mean.
</figcaption>
</figure>
<p>and if we repeat <span class="math notranslate nohighlight">\(L\)</span> times we sample the complete distribution for the uncertainty in the sample mean.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/bootstrap4.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> $L$ realizations of the data for $L$ realizations to completely sample the uncertainty in the mean.
</figcaption>
</figure>
<p>Let’s summarize bootstrap,</p>
<ul class="simple">
<li><p>developed by <a class="reference external" href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611970319.fm">Efron, 1982</a></p></li>
<li><p>statistical resampling procedure to calculate uncertainty in a calculated statistic from the data itself.</p></li>
<li><p>seems impossible, but go ahead and compare it to known cases like standard error and you will see that it works,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma_{\bar{x}}^2 = \frac{\sigma^2_s}{n}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2_s\)</span> is the sample variance, <span class="math notranslate nohighlight">\(n\)</span> is the number of samples, and <span class="math notranslate nohighlight">\(\sigma_{\bar{x}}^2\)</span> is the variance of the average under the assumption of independent, identically distributed sampling.</p>
<ul class="simple">
<li><p>may be applied to calculate the uncertainty in any statistic, for example, <span class="math notranslate nohighlight">\(13^{th}\)</span> percentile, skew, etc.</p></li>
<li><p>advanced forms account for spatial information and strategy (game theory).</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/bootflow.png" style="display: block; margin: 0 auto; width: 35%;">
  <figcaption style="text-align: center;"> The general flow chart for bootstrap.
</figcaption>
</figure>
</section>
<section id="bagging-models">
<h2>Bagging Models<a class="headerlink" href="#bagging-models" title="Permalink to this heading">#</a></h2>
<p>Bagging models in machine learning apply bootstrap to,</p>
<ul class="simple">
<li><p>calculate multiple realizations of the data</p></li>
<li><p>train multiple realizations of the model</p></li>
<li><p>calculate multiple realizations of the estimate</p></li>
<li><p>average the estimates to reduce model variance</p></li>
</ul>
<p>Here’s the flow chart for bagging models,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/baggingflow.png" style="display: block; margin: 0 auto; width: 40%;">
  <figcaption style="text-align: center;"> The bagging machine learning model flow chart.
</figcaption>
</figure>
<ol class="arabic simple">
<li><p>Apply statistical bootstrap to obtain multiple realizations of the data,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
Y^b, X_1^b, \dots, X_m^b,\quad b = 1, \dots, B
\]</div>
<ol class="arabic simple" start="2">
<li><p>Train a prediction model (estimator) for each data realization,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\hat{f}^b(X_1^b, \dots, X_m^b)
\]</div>
<ol class="arabic simple" start="3">
<li><p>Calculate a prediction with each estimator,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\hat{Y}^b = \hat{f}^b(X_1^b, \dots, X_m^b)
\]</div>
<ol class="arabic simple" start="4">
<li><p>Aggregate the ensemble of 𝐵 predictions over the estimators,</p></li>
</ol>
<ul class="simple">
<li><p><strong>Regression</strong> – aggregate the ensemble predictions with the average,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{Y} = \frac{1}{B} \sum_{b=1}^{B} \hat{Y}^b
\]</div>
<ul class="simple">
<li><p><strong>Classification</strong> – aggregate the ensemble predictions with majority-rule, plurality,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{Y} = \arg\max(\hat{Y}^b)
\]</div>
<p>I built out an interactive Python dashboard for <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bootstrap_Bagging.ipynb">bagging linear regression</a>.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/bagginglinear.png" style="display: block; margin: 0 auto; width: 80%;">
  <figcaption style="text-align: center;"> Interactive machine learning bagging with linear regression, 16 data bootstrap, model and prediction realizations aggregated by averaging.
</figcaption>
</figure>
</section>
<section id="training-and-tuning-bagging-models">
<h2>Training and Tuning Bagging Models<a class="headerlink" href="#training-and-tuning-bagging-models" title="Permalink to this heading">#</a></h2>
<p>What is the Bagging Regression Model?</p>
<p>Multiple models each trained on different bootstrap data realizations, all with the same hyperparameter(s).</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/themodel.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> The bagging model, we train individually, but we tune the ensemble.
</figcaption>
</figure>
<p>The bagging prediction, <span class="math notranslate nohighlight">\(\hat{y}\)</span>, the aggregate of the individual estimators, is the output of this model.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/themodelcont.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Bagging regression predictions by averaging multiple prediction models.
</figcaption>
</figure>
<p>or for classification,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/themodelcont.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Bagging classification predictions by plurality of multiple prediction models.
</figcaption>
</figure>
<p>Each model, known as an estimator in the ensemble of models, is trained with their respective bootstrapped data realization,
during training each model minimizes train error of the individual estimator with the bootstrapped data realization, residual sum of squares for regression,</p>
<div class="math notranslate nohighlight">
\[
\text{RSS} = \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\]</div>
<p>and Gini impurity for classification,</p>
<div class="math notranslate nohighlight">
\[
\text{Gini}_{\text{total}} = \sum_{j=1}^{J} \frac{N_j}{N} \cdot \text{Gini}(j)
\]</div>
<p>each estimator is trained separately, but they all share the same hyperparameters.</p>
<p>This provides the flexibility to build the best possible model to fit each bootstrap dataset.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/samehyper.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Estimators in the ensemble of models are trained individually, but share the same hyperparameter(s).
</figcaption>
</figure>
<p>We tune our bagged model with the error of the bagging estimate from aggregating the ensemble of estimates, for the case of regression,</p>
<div class="math notranslate nohighlight">
\[
\text{MSE} = \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
\]</div>
<p>where the predicted value <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_i = \frac{1}{B} \sum_{b=1}^{B} \hat{y}_i^{(b)}
\]</div>
<p>We tune our ensemble jointly over all estimators, we do not consider the error of individual model estimators,  <span class="math notranslate nohighlight">\(\hat{y}_i^𝑏 - y_i\)</span>, within the ensemble for model tuning.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/tuningnodes.png" style="display: block; margin: 0 auto; width: 50%;">
  <figcaption style="text-align: center;"> Estimators in the ensemble of models, sharing the same number of leaf nodes hyperparameter.
</figcaption>
</figure>
<p>The result is a single measure of test error for each hyperparameter setting, for the case above with number of leaf nodes, we get this result,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/ensembletesterror.png" style="display: block; margin: 0 auto; width: 40%;">
  <figcaption style="text-align: center;"> Ensemble test error vs. number of leaf nodes hyperparameter.
</figcaption>
</figure>
<p>to select the ensemble hyperparameter to minimize test error. For clarity, let’s add the tuning to our previous training bagging models workflow,</p>
<ul class="simple">
<li><p>we loop over hyperparameters</p></li>
<li><p>minimize the test error of the ensemble estimates</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/baggingtuneflow.png" style="display: block; margin: 0 auto; width: 40%;">
  <figcaption style="text-align: center;"> The workflow for tuning a bagging model.
</figcaption>
</figure>
</section>
<section id="out-of-bag-cross-validation">
<h2>Out-of-Bag Cross Validation<a class="headerlink" href="#out-of-bag-cross-validation" title="Permalink to this heading">#</a></h2>
<p>In expectation, <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span> of the training data is left out of each bootstrap data realization, <span class="math notranslate nohighlight">\(b^c\)</span>; therefore, cross-validation is built in.</p>
<ol class="arabic simple">
<li><p>Sample with replacement <span class="math notranslate nohighlight">\(\frac{2}{3}\)</span> of the training data (in expectation), <span class="math notranslate nohighlight">\(Y^b, X_1^b, \dots, X_m^b\)</span>.</p></li>
<li><p>Train an estimator with the <span class="math notranslate nohighlight">\(\frac{2}{3}\)</span> of training data (in expectation).</p></li>
<li><p>Predict at the out-of-bag samples, <span class="math notranslate nohighlight">\(X_1^{b^c}, \dots, X_m^{b^c}\)</span>, <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span> of the training data (in expectation).</p></li>
</ol>
<figure style="text-align: center;">
  <img src="_static/ensemble/baggingoobflow.png" style="display: block; margin: 0 auto; width: 35%;">
  <figcaption style="text-align: center;"> Out-of-bag error calculation workflow, to apply add to the hyperparameter tuning loop.
</figcaption>
</figure>
<ol class="arabic simple" start="4">
<li><p>Pool the 𝐵/3 predictions (in expectation) for each sample data from all the 𝐵 models and make an out-of-bag prediction, for regression,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\hat{y}_\alpha^{\text{oob}} = \frac{1}{\left(\frac{B}{3}\right)} \sum_{b=1}^{\frac{B}{3}} \hat{y}_\alpha^{(b^c)}
\]</div>
<p>Calculate the out-of-bag error to assess model performance.</p>
<div class="math notranslate nohighlight">
\[
\text{MSE}_{\text{OOB}} = \frac{1}{n} \sum_{\alpha=1}^{n} \left[\hat{y}_\alpha^{\text{oob}} - y_\alpha \right]^2
\]</div>
</section>
<section id="number-of-estimators">
<h2>Number of Estimators<a class="headerlink" href="#number-of-estimators" title="Permalink to this heading">#</a></h2>
<p>Number of Estimators is an important hyperparameter for bagging models</p>
<ul class="simple">
<li><p><strong>More estimators</strong> – improve generalization up to a point, increasing the number of trees generally improves performance and reduces variance, as predictions are averaged across more models.</p></li>
<li><p><strong>Diminishing returns</strong> - beyond a point, adding more estimators gives little or no improvement and only increases computational cost.</p></li>
<li><p><strong>Improved stability</strong> - more trees reduce the likelihood of overfitting to random noise in the training set.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/numberestimators.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Number of estimators, few (upper) and more (lower), within the ensemble model.
</figcaption>
</figure>
</section>
<section id="estimator-complexity">
<h2>Estimator Complexity<a class="headerlink" href="#estimator-complexity" title="Permalink to this heading">#</a></h2>
<p>The hyperparameter(s) shared by the estimators remain as important hyperparameters. Here’s guidance with a focus on tree bagging,</p>
<ul class="simple">
<li><p><strong>More complicated models</strong> – bagging reduces model variance, so we often train more complicated models for the ensemble.</p></li>
<li><p><strong>Too simple models</strong> – may not see any improvement from bagging, because model variance is not an issue for simple models and does not need to be reduced.</p></li>
<li><p><strong>Feature interactions</strong> – more complicated models capture more of the interactions between features, for example, tree bagging models with tree depth 𝑑 can capture 𝑑-way feature interactions</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/varydepth.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Ensembles with different tree depth hyperparameters, 2 (upper), 3 (middle) and 4 (lower).
</figcaption>
</figure>
</section>
<section id="tree-bagging">
<h2>Tree Bagging<a class="headerlink" href="#tree-bagging" title="Permalink to this heading">#</a></h2>
<p>Now let’s summarize the approach,</p>
<ul class="simple">
<li><p>Build an ensemble of decision trees with multiple, bootstrap realizations of the data.</p></li>
</ul>
<p>and provide some guidance,</p>
<ul class="simple">
<li><p>the ensemble of tree estimators reduces model variance</p></li>
<li><p>hyperparameter tune over the entire ensemble model, i.e., All trees in the ensemble have the same hyperparameters.</p></li>
<li><p>number of estimators is an additional, important hyperparameter in addition to the tree estimators’ complexity</p></li>
<li><p>in expectation, $\frac{1}{3} of the data is not used for each tree, this provides the opportunity to have access to out-of-bag samples for cross validation, so we can build our model and cross validate with all the data at once, no train and test split.</p></li>
<li><p>overgrown trees will often outperform simpler trees due to reduction in model variance with averaging over the estimators.</p></li>
</ul>
<p>Spoiler Alert - we want the trees to be decorrelated, diverse to maximize the reduction in model variance, this leads to random forest. More on this later.</p>
<p>To visualize tree bagging, here’s an example of tree bagging by-hand, 6 estimators from bootstrap realizations of the data, predicting over porosity and brittleness and the average over all the estimates as the bagging model.</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/baggingbyhand.png" style="display: block; margin: 0 auto; width: 100%;">
  <figcaption style="text-align: center;"> 6 bootstrapped, complicated decision trees (left) and the bagging model, average of all 6 models (right).
</figcaption>
</figure>
<p>Observe the impact on the prediction model with the addition of more trees – transition from a discontinuous to continuous prediction model!</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/baggingestimators.png" style="display: block; margin: 0 auto; width: 80%;">
  <figcaption style="text-align: center;"> 6 tree bagging prediction models and all training data with increasing number of estimators, for 1, 3, 5, 10, 30 and 500 trees.
</figcaption>
</figure>
<p>Observe the improved testing accuracy in cross validation with increasing number of trees,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/baggingerrors.png" style="display: block; margin: 0 auto; width: 80%;">
  <figcaption style="text-align: center;"> Cross validation with 6 tree bagging prediction models with increasing number of trees.
</figcaption>
</figure>
<p>Observe the reduction in model variance with increasing number of trees,</p>
<figure style="text-align: center;">
  <img src="_static/ensemble/baggingreducevar.png" style="display: block; margin: 0 auto; width: 80%;">
  <figcaption style="text-align: center;"> 3 models with 1 and 100 trees to demonstrate the reduction in model variance with increased ensemble aggregation.
</figcaption>
</figure>
</section>
<section id="random-forest">
<h2>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this heading">#</a></h2>
<p>A limitation with tree bagging is that the individual trees may be highly correlated.</p>
<ul class="simple">
<li><p>this occurs when there is a dominant predictor feature as it will always be applied to the top split(s), the result is all the trees in the ensemble are very similar (i.e., correlated)</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/correlatedtrees.png" style="display: block; margin: 0 auto; width: 60%;">
  <figcaption style="text-align: center;"> Highly correlated trees in a tree bagging ensemble model, trees with the same initial splits resulting in very similar predictions.
</figcaption>
</figure>
<p>With highly correlated trees, there is significantly less reduction in model variance with the ensemble,</p>
<ul class="simple">
<li><p>consider, standard error in the mean assumes the samples 𝑛 are independent!</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma_{\bar{x}}^2 = \frac{\sigma_s^2}{n}
\]</div>
<p>correlation between samples reduces the 𝑛 to a 𝑛 effective, as correlation increases, <span class="math notranslate nohighlight">\(n\)</span> effectively is reduced.</p>
<p>Random forest is tree bagging, but for each split only a subset <span class="math notranslate nohighlight">\(𝑝\)</span> of the <span class="math notranslate nohighlight">\(𝑚\)</span> available predictors are candidates for splits (selected at random).</p>
<div class="math notranslate nohighlight">
\[
p \ll m
\]</div>
<p>This forces each tree in the ensemble to evolve in dissimilar manner,</p>
<p>Common defaults for 𝑝 for classification,</p>
<div class="math notranslate nohighlight">
\[
p = \sqrt{m} \quad \text{or} \quad \log_2(p)
\]</div>
<p>and for regression,</p>
<div class="math notranslate nohighlight">
\[
p=\frac{m}{3}
\]</div>
<p>Lower <span class="math notranslate nohighlight">\(p\)</span> less correlation, better generalization, higher <span class="math notranslate nohighlight">\(p\)</span> more correlation, may overfit.
note, too low <span class="math notranslate nohighlight">\(p\)</span> will underfit with high model bias</p>
<p>Here’s an example random forest model for the previous prediction problem,</p>
<ul class="simple">
<li><p>300 trees, trained to a maximum depth of <span class="math notranslate nohighlight">\(7\)</span>, <span class="math notranslate nohighlight">\(𝑝=1\)</span>, i.e., 1 predictor feature randomly selected for each split</p></li>
</ul>
<figure style="text-align: center;">
  <img src="_static/ensemble/randomforest.png" style="display: block; margin: 0 auto; width: 80%;">
  <figcaption style="text-align: center;"> Highly correlated trees in a tree bagging ensemble model, trees with the same initial splits resulting in very similar predictions.
</figcaption>
</figure>
<p>Now we are ready to demonstrate tree bagging and random forest.</p>
</section>
<section id="load-the-required-libraries">
<h2>Load the Required Libraries<a class="headerlink" href="#load-the-required-libraries" title="Permalink to this heading">#</a></h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline                                         
<span class="n">suppress_warnings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># to set current working directory </span>
<span class="kn">import</span> <span class="nn">math</span>                                                   <span class="c1"># square root operator</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># arrays and matrix math</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>                                      <span class="c1"># statistical methods</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>                                           <span class="c1"># DataFrames</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span><span class="n">AutoMinorLocator</span><span class="p">,</span><span class="n">FuncFormatter</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>                  <span class="c1"># custom color maps</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>                                         <span class="c1"># for matrix scatter plots</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>                <span class="c1"># decision tree method</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>                 <span class="c1"># bagging tree method</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>            <span class="c1"># random forest method</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">_tree</span>                                <span class="c1"># for accessing tree information</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>                                   <span class="c1"># measures to check our models</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>              <span class="c1"># standardize the features</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>                      <span class="c1"># graphical visualization of trees</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="p">(</span><span class="n">cross_val_score</span><span class="p">,</span><span class="n">train_test_split</span><span class="p">,</span><span class="n">GridSearchCV</span><span class="p">,</span><span class="n">KFold</span><span class="p">)</span> <span class="c1"># model tuning</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Pipeline</span><span class="p">,</span><span class="n">make_pipeline</span><span class="p">)</span>         <span class="c1"># machine learning modeling pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>                                   <span class="c1"># measures to check our models</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>           <span class="c1"># multi-processor K-fold crossvalidation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>          <span class="c1"># train and test split</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>                     <span class="c1"># custom displays</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># default color bar, no bias and friendly for color vision defeciency</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;axes&#39;</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># grid behind plotting elements</span>
<span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>  
    <span class="kn">import</span> <span class="nn">warnings</span>                                           <span class="c1"># suppress any warnings for this demonstration</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span> 
<span class="n">seed</span> <span class="o">=</span> <span class="mi">13</span>                                                     <span class="c1"># random number seed for workflow repeatability</span>
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‘python -m pip install [package-name]’. More assistance is available with the respective package docs.</p>
</section>
<section id="declare-functions">
<h2>Declare Functions<a class="headerlink" href="#declare-functions" title="Permalink to this heading">#</a></h2>
<p>Let’s define a couple of functions to streamline plotting correlation matrices and visualization of a decision, boosting tree and random forest regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">comma_format</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1">&#39;</span>

<span class="k">def</span> <span class="nf">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">,</span><span class="n">nominal</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span> <span class="c1"># feature ranking plot</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">);</span> <span class="n">mask_low</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">-</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">nominal</span><span class="o">-</span><span class="n">mmin</span><span class="p">);</span> <span class="n">mask_high</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">mmax</span><span class="o">-</span><span class="n">nominal</span><span class="p">);</span> <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">1.5</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span><span class="s1">&#39;r--&#39;</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;dodgerblue&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_low</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">mask_low</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_high</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">mask_high</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predictor Features&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">limits</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span>                 <span class="c1"># plots a graphical correlation matrix </span>
    <span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>          
    <span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
    <span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">white_low</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span> <span class="o">-</span> <span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">);</span> <span class="n">white_high</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">)</span>
    <span class="n">newcolors</span><span class="p">[</span><span class="n">white_low</span><span class="p">:</span><span class="n">white_high</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                <span class="c1"># mask all correlations less than abs(0.8)</span>
    <span class="n">newcmp</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">fignum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">limits</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">limits</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">newcmp</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">);</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s1">&#39;bottom&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_bottom</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">&#39;vertical&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
    
<span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;major&#39;</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;minor&#39;</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">&#39;major&#39;</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">&#39;minor&#39;</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks   </span>
    
<span class="k">def</span> <span class="nf">visualize_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">xfeature</span><span class="p">,</span><span class="n">x_min</span><span class="p">,</span><span class="n">x_max</span><span class="p">,</span><span class="n">yfeature</span><span class="p">,</span><span class="n">y_min</span><span class="p">,</span><span class="n">y_max</span><span class="p">,</span><span class="n">response</span><span class="p">,</span><span class="n">z_min</span><span class="p">,</span><span class="n">z_max</span><span class="p">,</span><span class="n">clabel</span><span class="p">,</span><span class="n">xlabel</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">title</span><span class="p">):</span><span class="c1"># plots the data points and model </span>
    <span class="n">xplot_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span><span class="o">/</span><span class="mf">300.0</span><span class="p">;</span> <span class="n">yplot_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_max</span> <span class="o">-</span> <span class="n">y_min</span><span class="p">)</span><span class="o">/</span><span class="mf">300.0</span> <span class="c1"># resolution of the model visualization</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">xplot_step</span><span class="p">),</span> <span class="c1"># set up the mesh</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">yplot_step</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>          <span class="c1"># predict with our trained model over the mesh</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">interpolation</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span><span class="n">aspect</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span><span class="n">extent</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_min</span><span class="p">,</span><span class="n">x_max</span><span class="p">,</span><span class="n">y_min</span><span class="p">,</span><span class="n">y_max</span><span class="p">],</span> <span class="n">vmin</span> <span class="o">=</span> <span class="n">z_min</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">z_max</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">xfeature</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">str</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">yfeature</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xfeature</span><span class="p">,</span><span class="n">yfeature</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">response</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">z_min</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">z_max</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
                    <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>                                          <span class="c1"># add the labels</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span><span class="n">x_max</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">y_min</span><span class="p">,</span><span class="n">y_max</span><span class="p">])</span>
    <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">&#39;vertical&#39;</span><span class="p">)</span>         <span class="c1"># add the color bar</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">clabel</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Z</span>
    
<span class="k">def</span> <span class="nf">visualize_grid</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">xfeature</span><span class="p">,</span><span class="n">x_min</span><span class="p">,</span><span class="n">x_max</span><span class="p">,</span><span class="n">yfeature</span><span class="p">,</span><span class="n">y_min</span><span class="p">,</span><span class="n">y_max</span><span class="p">,</span><span class="n">response</span><span class="p">,</span><span class="n">z_min</span><span class="p">,</span><span class="n">z_max</span><span class="p">,</span><span class="n">clabel</span><span class="p">,</span><span class="n">xlabel</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">title</span><span class="p">,):</span><span class="c1"># plots the data points and the decision tree  </span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">xplot_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span><span class="o">/</span><span class="mf">300.0</span><span class="p">;</span> <span class="n">yplot_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_max</span> <span class="o">-</span> <span class="n">y_min</span><span class="p">)</span><span class="o">/</span><span class="mf">300.0</span> <span class="c1"># resolution of the model visualization</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">xplot_step</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">yplot_step</span><span class="p">))</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="n">z_min</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">z_max</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">z_min</span><span class="p">,</span> <span class="n">z_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>

    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xfeature</span><span class="p">,</span><span class="n">yfeature</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">response</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="n">z_min</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">z_max</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
                     <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
    <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">&#39;vertical&#39;</span><span class="p">)</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">clabel</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    
<span class="k">def</span> <span class="nf">check_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">title</span><span class="p">):</span> <span class="c1"># get OOB MSE and cross plot a decision tree </span>
    <span class="n">oob_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> <span class="n">model</span><span class="o">.</span><span class="n">estimators_samples_</span><span class="p">)</span>
    <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">oob_prediction_</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">]</span>
    <span class="n">oob_y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">]</span>
    <span class="n">MSE_oob</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">oob_y</span><span class="p">,</span><span class="n">oob_y_hat</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">oob_y</span><span class="p">,</span><span class="n">oob_y_hat</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> 
                <span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Truth: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">));</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Estimated: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">],[</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Out of Bag MSE: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">MSE_oob</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s1">,.0f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">),[</span><span class="mi">4500</span><span class="p">,</span><span class="mi">2500</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">check_model_OOB_MSE</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">title</span><span class="p">):</span>      <span class="c1"># OOB MSE and cross plot over multiple bagged trees, checks for unestimated </span>
    <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">oob_prediction_</span>
    <span class="n">oob_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">oob_y_hat</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> 
    <span class="n">MSE_oob</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">oob_y</span><span class="p">,</span><span class="n">oob_y_hat</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">oob_y</span><span class="p">,</span><span class="n">oob_y_hat</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> 
                <span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Truth: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">));</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Estimated: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">],[</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Out of Bag MSE: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">MSE_oob</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s1">,.0f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">),[</span><span class="mi">4500</span><span class="p">,</span><span class="mi">2500</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    
<span class="k">def</span> <span class="nf">check_grid</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">xfeature</span><span class="p">,</span><span class="n">yfeature</span><span class="p">,</span><span class="n">response</span><span class="p">,</span><span class="n">zmin</span><span class="p">,</span><span class="n">zmax</span><span class="p">,</span><span class="n">title</span><span class="p">):</span> <span class="c1"># plots the estimated vs. the actual  </span>
    <span class="k">if</span> <span class="n">grid</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Prediction array must be 2D&quot;</span><span class="p">)</span>
    <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">xstep</span> <span class="o">=</span> <span class="p">(</span><span class="n">xmax</span> <span class="o">-</span> <span class="n">xmin</span><span class="p">)</span><span class="o">/</span><span class="n">nx</span><span class="p">;</span> <span class="n">ystep</span> <span class="o">=</span> <span class="p">(</span><span class="n">ymax</span><span class="o">-</span><span class="n">ymin</span><span class="p">)</span><span class="o">/</span><span class="n">ny</span> 
    <span class="n">predict_train</span> <span class="o">=</span> <span class="n">feature_sample</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">ymin</span><span class="p">,</span> <span class="n">xstep</span><span class="p">,</span> <span class="n">ystep</span><span class="p">,</span> <span class="n">xfeature</span><span class="p">,</span> <span class="n">yfeature</span><span class="p">,</span> <span class="s1">&#39;sample&#39;</span><span class="p">)</span>
    <span class="n">MSE</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">response</span><span class="p">,</span><span class="n">predict_train</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">response</span><span class="p">,</span><span class="n">predict_train</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> 
                <span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Truth: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">));</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Estimated: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">zmin</span><span class="p">,</span><span class="n">zmax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">zmin</span><span class="p">,</span><span class="n">zmax</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">zmin</span><span class="p">,</span><span class="n">zmax</span><span class="p">],[</span><span class="n">zmin</span><span class="p">,</span><span class="n">zmax</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="c1"># plt.annotate(&#39;Out of Bag MSE: &#39; + str(f&#39;{(np.round(MSE_oob,2)):,.0f}&#39;),[4500,2500]) # not technically OOB MSE</span>

<span class="k">def</span> <span class="nf">feature_sample</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">ymin</span><span class="p">,</span> <span class="n">xstep</span><span class="p">,</span> <span class="n">ystep</span><span class="p">,</span> <span class="n">df_x</span><span class="p">,</span> <span class="n">df_y</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span> <span class="c1"># sampling predictions from a feature space grid </span>
    <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Array must be 2D&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_x</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_y</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x and y feature arrays must have equal lengths&quot;</span><span class="p">)</span>   
    <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">v</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">nsamp</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">isamp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamp</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">df_x</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">isamp</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">df_y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">isamp</span><span class="p">]</span>
        <span class="n">iy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ny</span> <span class="o">-</span> <span class="nb">int</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">ymin</span><span class="p">)</span> <span class="o">/</span> <span class="n">ystep</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ny</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">xmin</span><span class="p">)</span> <span class="o">/</span> <span class="n">xstep</span><span class="p">),</span> <span class="n">nx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">v</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">array</span><span class="p">[</span><span class="n">iy</span><span class="p">,</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">df</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">return</span> <span class="n">df</span>    

<span class="k">def</span> <span class="nf">display_sidebyside</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>                                <span class="c1"># display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)</span>
    <span class="n">html_str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="n">html_str</span> <span class="o">+=</span> <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">()</span>  <span class="c1"># Using .head() for the first few rows</span>
    <span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&lt;div style=&quot;display: flex;&quot;&gt;</span><span class="si">{</span><span class="n">html_str</span><span class="si">}</span><span class="s1">&lt;/div&gt;&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="set-the-working-directory">
<h2>Set the working directory<a class="headerlink" href="#set-the-working-directory" title="Permalink to this heading">#</a></h2>
<p>I always like to do this so I don’t lose files and to simplify subsequent read and writes (avoid including the full address each time).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#os.chdir(&quot;c:/PGE383&quot;)                                        # set the working directory</span>
</pre></div>
</div>
</div>
</div>
<p>You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. “~/PGE”).</p>
</section>
<section id="loading-data">
<h2>Loading Data<a class="headerlink" href="#loading-data" title="Permalink to this heading">#</a></h2>
<p>Let’s load the provided multivariate, spatial dataset <a class="reference external" href="https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv">unconv_MV.csv</a> available in my GeoDataSet repo. It is a comma delimited file with:</p>
<ul class="simple">
<li><p>well index (integer)</p></li>
<li><p>porosity (%)</p></li>
<li><p>permeability (<span class="math notranslate nohighlight">\(mD\)</span>)</p></li>
<li><p>acoustic impedance (<span class="math notranslate nohighlight">\(\frac{kg}{m^3} \cdot \frac{m}{s} \cdot 10^6\)</span>).</p></li>
<li><p>brittleness (%)</p></li>
<li><p>total organic carbon (%)</p></li>
<li><p>vitrinite reflectance (%)</p></li>
<li><p>initial gas production (90 day average) (MCFPD)</p></li>
</ul>
<p>We load it with the pandas ‘read_csv’ function into a data frame we called ‘df’ and then preview it to make sure it loaded correctly.</p>
<p><strong>Python Tip: using functions from a package</strong> just type the label for the package that we declared at the beginning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
<p>so we can access the pandas function ‘read_csv’ with the command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">()</span>
</pre></div>
</div>
<p>but read csv has required input parameters. The essential one is the name of the file. For our circumstance all the other default parameters are fine. If you want to see all the possible parameters for this function, just go to the docs <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html">here</a>.</p>
<ul class="simple">
<li><p>The docs are always helpful</p></li>
<li><p>There is often a lot of flexibility for Python functions, possible through using various inputs parameters</p></li>
</ul>
<p>also, the program has an output, a pandas DataFrame loaded from the data. So we have to specify the name / variable representing that new object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;unconv_MV.csv&quot;</span><span class="p">)</span>  
</pre></div>
</div>
<p>Let’s run this command to load the data and then this command to extract a random subset of the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.30</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">73073</span><span class="p">);</span> 
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="feature-engineering">
<h2>Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permalink to this heading">#</a></h2>
<p>Let’s make some changes to the data to improve the workflow:</p>
<ul class="simple">
<li><p><strong>Select the predictor features (x2) and the response feature (x1)</strong>, make sure the metadata is also consistent.</p></li>
<li><p><strong>Metadata</strong> encoding such as the units, labels and display ranges for each feature.</p></li>
<li><p><strong>Reduce the number of data</strong> for ease of visualization (hard to see if too many points on our plots).</p></li>
<li><p><strong>Train and test data split</strong> to demonstrate and visualize simple hyperparameter tuning.</p></li>
<li><p><strong>Add random noise to the data</strong> to demonstrate model overfit. The original data is error free and does not readily demonstrate overfit.</p></li>
</ul>
<p>Given this is properly set, one should be able to use any dataset and features for this demonstration.</p>
<ul class="simple">
<li><p>for brevity we don’t show any feature selection here. Previous chapter, e.g., k-nearest neighbours include some feature selection methods, but see the feature selection chapter for many possible methods with codes for feature selection.</p></li>
</ul>
</section>
<section id="optional-add-random-noise-to-the-response-feature">
<h2>Optional: Add Random Noise to the Response Feature<a class="headerlink" href="#optional-add-random-noise-to-the-response-feature" title="Permalink to this heading">#</a></h2>
<p>We can do this to observe the impact of data noise on overfit and hyperparameter tuning.</p>
<ul class="simple">
<li><p>This is for experiential learning, of course we wouldn’t add random noise to our data</p></li>
<li><p>We set the random number seed for reproducibility</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">add_error</span> <span class="o">=</span> <span class="kc">True</span>                                              <span class="c1"># add random error to the response feature</span>
<span class="n">std_error</span> <span class="o">=</span> <span class="mi">500</span>                                               <span class="c1"># standard deviation of random error, for demonstration only</span>
<span class="n">idata</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv&quot;</span><span class="p">)</span> <span class="c1"># load the data from my github repo</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.30</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">);</span> <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span> <span class="c1"># extract 30% random to reduce the number of data</span>
    
<span class="k">elif</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv&quot;</span><span class="p">)</span> <span class="c1"># load the data </span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.70</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">);</span> <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span> <span class="c1"># extract 30% random to reduce the number of data</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Prod&quot;</span><span class="p">:</span> <span class="s2">&quot;Production&quot;</span><span class="p">})</span>
    
<span class="n">yname</span> <span class="o">=</span> <span class="s1">&#39;Production&#39;</span><span class="p">;</span> <span class="n">Xname</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Por&#39;</span><span class="p">,</span><span class="s1">&#39;Brittle&#39;</span><span class="p">]</span>               <span class="c1"># specify the predictor features (x2) and response feature (x1)</span>
<span class="n">Xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">];</span> <span class="n">Xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">25.0</span><span class="p">,</span><span class="mf">100.0</span><span class="p">]</span>                         <span class="c1"># set minimums and maximums for visualization </span>
<span class="n">ymin</span> <span class="o">=</span> <span class="mf">1500.0</span><span class="p">;</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">7000.0</span>
<span class="n">Xlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Porosity&#39;</span><span class="p">,</span><span class="s1">&#39;Brittleness&#39;</span><span class="p">];</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Production&#39;</span>    <span class="c1"># specify the feature labels for plotting</span>
<span class="n">Xunit</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;%&#39;</span><span class="p">,</span><span class="s1">&#39;%&#39;</span><span class="p">];</span> <span class="n">yunit</span> <span class="o">=</span> <span class="s1">&#39;MCFPD&#39;</span>
<span class="n">Xlabelunit</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; (&#39;</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; (&#39;</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span><span class="p">]</span>
<span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">&#39; (&#39;</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span>

<span class="k">if</span> <span class="n">add_error</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                         <span class="c1"># method to add error</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>                                 <span class="c1"># set random number seed</span>
    <span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">std_error</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df_load</span><span class="p">))</span> <span class="c1"># add noise</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">_get_numeric_data</span><span class="p">();</span> <span class="n">values</span><span class="p">[</span><span class="n">values</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># set negative to 0 in a shallow copy ndarray</span>
    
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span>                              <span class="c1"># extract selected features as X and y DataFrames</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_load</span><span class="p">[</span><span class="n">Xname</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                                  <span class="c1"># make one DataFrame with both X and y (remove all other features)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s make sure that we have selected reasonable features to build a model</p>
<ul class="simple">
<li><p>the 2 predictor features are not collinear, as this would result in an unstable prediction model</p></li>
<li><p>each of the features are related to the response feature, the predictor features inform the response</p></li>
</ul>
</section>
<section id="calculate-the-correlation-matrix-and-correlation-with-response-ranking">
<h2>Calculate the Correlation Matrix and Correlation with Response Ranking<a class="headerlink" href="#calculate-the-correlation-matrix-and-correlation-with-response-ranking" title="Permalink to this heading">#</a></h2>
<p>Let’s start with correlation analysis. We can calculate and view the correlation matrix and correlation to the response features with these previously declared functions.</p>
<ul class="simple">
<li><p>correlation analysis is based on the assumption of linear relationships, but it is a good start</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">correlation</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="s1">&#39;Correlation Matrix&#39;</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>           <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Features&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Features&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">Xname</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">&#39;Feature Ranking, Correlation with &#39;</span> <span class="o">+</span> <span class="n">yname</span><span class="p">,</span><span class="s1">&#39;Correlation&#39;</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/152d837d72f43ba9a48527a444d81b3bbc73a2ba553d2760d27f5c20206ea0b2.png" src="_images/152d837d72f43ba9a48527a444d81b3bbc73a2ba553d2760d27f5c20206ea0b2.png" />
</div>
</div>
<p>Note the 1.0 diagonal resulting from the correlation of each variable with themselves.</p>
<p>This looks good.  There is a mix of correlation magnitudes. Of course, correlation coefficients are limited to degree of linear correlations.</p>
<ul class="simple">
<li><p>Let’s look at the matrix scatter plot to see the pairwise relationship between the features.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pairgrid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="nb">vars</span><span class="o">=</span><span class="n">Xname</span><span class="o">+</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span>                <span class="c1"># matrix scatter plots</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkorange&#39;</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span><span class="c1"># Map a density plot to the lower triangle</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> 
                              <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8d03fa748bcc76aea92c8e57b5fb8071473e84b910d1402f5fd62dd21b102145.png" src="_images/8d03fa748bcc76aea92c8e57b5fb8071473e84b910d1402f5fd62dd21b102145.png" />
</div>
</div>
</section>
<section id="train-and-test-split">
<h2>Train and Test Split<a class="headerlink" href="#train-and-test-split" title="Permalink to this heading">#</a></h2>
<p>Since we are working with ensemble methods the train and test split is built into the model training with out-of-bag samples.</p>
<ul class="simple">
<li><p>we will work with the entire dataset</p></li>
<li><p>note, we could split a testing dataset for the train, validate, test approach. For simplicity I only use train and test in these workflows.</p></li>
</ul>
</section>
<section id="visualize-the-dataframe">
<h2>Visualize the DataFrame<a class="headerlink" href="#visualize-the-dataframe" title="Permalink to this heading">#</a></h2>
<p>Visualizing the train and test DataFrame is useful check before we build our models.</p>
<ul class="simple">
<li><p>many things can go wrong, e.g., we loaded the wrong data, all the features did not load, etc.</p></li>
</ul>
<p>We can preview by utilizing the ‘head’ DataFrame member function (with a nice and clean format, see below).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>                                                  <span class="c1"># check the loaded DataFrame</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.22</td>
      <td>63.09</td>
      <td>2006.074005</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13.01</td>
      <td>50.41</td>
      <td>4244.321703</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10.03</td>
      <td>37.74</td>
      <td>2493.189177</td>
    </tr>
    <tr>
      <th>3</th>
      <td>18.10</td>
      <td>56.09</td>
      <td>6124.075271</td>
    </tr>
    <tr>
      <th>4</th>
      <td>16.95</td>
      <td>61.43</td>
      <td>5951.336259</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="summary-statistics-for-tabular-data">
<h2>Summary Statistics for Tabular Data<a class="headerlink" href="#summary-statistics-for-tabular-data" title="Permalink to this heading">#</a></h2>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames.</p>
<ul class="simple">
<li><p>The describe command provides count, mean, minimum, maximum, percentiles in a nice data table.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">(</span><span class="n">percentiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.9</span><span class="p">])</span>                            <span class="c1"># check DataFrame summary statistics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>140.000000</td>
      <td>140.000000</td>
      <td>140.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>14.897357</td>
      <td>48.345429</td>
      <td>4273.644226</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.181639</td>
      <td>14.157619</td>
      <td>1138.466092</td>
    </tr>
    <tr>
      <th>min</th>
      <td>6.550000</td>
      <td>10.940000</td>
      <td>1517.373571</td>
    </tr>
    <tr>
      <th>10%</th>
      <td>10.866000</td>
      <td>28.853000</td>
      <td>2957.573690</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>14.855000</td>
      <td>50.735000</td>
      <td>4315.186629</td>
    </tr>
    <tr>
      <th>90%</th>
      <td>18.723000</td>
      <td>65.813000</td>
      <td>5815.526968</td>
    </tr>
    <tr>
      <th>max</th>
      <td>23.550000</td>
      <td>84.330000</td>
      <td>6907.632261</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>It is good that we checked the summary statistics.</p>
<ul class="simple">
<li><p>there are no obvious issues</p></li>
<li><p>check out the range of values for each feature to set up and adjust plotting limits. See above.</p></li>
</ul>
</section>
<section id="visualize-the-distributions">
<h2>Visualize the Distributions<a class="headerlink" href="#visualize-the-distributions" title="Permalink to this heading">#</a></h2>
<p>Let’s check the histograms and scatter plots of the predictor features.</p>
<ul class="simple">
<li><p>check to make sure the data cover the range of possible predictor feature combinations</p></li>
<li><p>check that the predictor features are not highly correlated, collinear, as this increases model variance</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 histogram</span>
<span class="n">freq</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train&#39;</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span><span class="o">*</span><span class="mf">1.10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train&#39;</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span><span class="o">*</span><span class="mf">1.10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Porosity&#39;</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>                                              <span class="c1"># predictor features #1 and #2 scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; vs &#39;</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train&#39;</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span><span class="o">*</span><span class="mf">1.10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">ylabelunit</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Porosity&#39;</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.6</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.6</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="c1">#plt.savefig(&#39;Test.pdf&#39;, dpi=600, bbox_inches = &#39;tight&#39;,format=&#39;pdf&#39;)   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c9e941b61d19ec872e32d0b10c48a28622e57437ff6fa45205763cc62773906a.png" src="_images/c9e941b61d19ec872e32d0b10c48a28622e57437ff6fa45205763cc62773906a.png" />
</div>
</div>
<p>Once again, the distributions are well behaved,</p>
<ul class="simple">
<li><p>we cannot observe obvious gaps nor truncations.</p></li>
<li><p>the predictor features are not highly correlated</p></li>
</ul>
<p>Let’s look at a scatter plot of Porosity vs. Brittleness with points colored by Production.</p>
<ul class="simple">
<li><p>to visualize the prediction problem, i.e., the shape of the system</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># visualize the train and test data in predictor feature space</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
    <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Train&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training &#39;</span> <span class="o">+</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">&#39; vs. &#39;</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; and &#39;</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; (&#39;</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; (&#39;</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;upper right&#39;</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">&#39;vertical&#39;</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">ylabel</span> <span class="o">+</span> <span class="s1">&#39; (&#39;</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6980039b0d3453603a0540f6ba29b6b821be9104ac38a6c3b26a9b0a9cd5a007.png" src="_images/6980039b0d3453603a0540f6ba29b6b821be9104ac38a6c3b26a9b0a9cd5a007.png" />
</div>
</div>
</section>
<section id="ensemble-tree-method-tree-bagging-regression">
<h2>Ensemble Tree Method - Tree Bagging Regression<a class="headerlink" href="#ensemble-tree-method-tree-bagging-regression" title="Permalink to this heading">#</a></h2>
<p>We are ready to build a tree bagging model. To perform tree bagging we:</p>
<ol class="arabic simple">
<li><p>set the hyperparameters for the individual trees</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">73073</span>
<span class="n">max_depth</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">2</span>  
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>instantiate an individual regression tree</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>set the bagging hyperparameters</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_trees</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">73073</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>instantiate the bagging regressor with the previously instantiated regression tree (wrapping the decision tree)</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bagging_model</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">regressor</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">num_trees</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>train the bagging regression (wrapping the decision tree)</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bagging_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>visualize the model result over the feature space (easy to do as we have only 2 predictor features)</p></li>
</ol>
</section>
<section id="demonstration-of-bagging-by-hand">
<h2>Demonstration of Bagging by-Hand<a class="headerlink" href="#demonstration-of-bagging-by-hand" title="Permalink to this heading">#</a></h2>
<p>For demonstration of by-hand tree bagging let’s set the number of trees to 1 and run tree bagging regression 6 times.</p>
<ul class="simple">
<li><p>the result for each is a single complicated decision tree</p></li>
<li><p>note, the random_state parameter is the random number seed for the bootstrap in the bagging method</p></li>
<li><p>the trees vary for each random number seed since the bootstrapped dataset will be different for each</p></li>
</ul>
<p>We will loop over the models and store each of them in an list of models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">2</span>                         <span class="c1"># set for a complicated tree</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span><span class="p">)</span> <span class="c1"># instantiate a decision tree</span>

<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">1</span>                                                  <span class="c1"># use only a single tree for this demonstration</span>
<span class="n">seeds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">73073</span><span class="p">,</span> <span class="mi">73074</span><span class="p">,</span> <span class="mi">73075</span><span class="p">,</span> <span class="mi">73076</span><span class="p">,</span> <span class="mi">73077</span><span class="p">,</span> <span class="mi">73078</span><span class="p">]</span>
<span class="n">bagging_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">oob_MSE</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span><span class="p">:</span>                                            <span class="c1"># visualize models over random number seeds</span>
    <span class="n">bagging_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">regressor</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">oob_score</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                           <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">bagging_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">oob_MSE</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bagging_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">oob_score_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">bag_X1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">bagging_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">estimators_samples_</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">bag_X2</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">bagging_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">estimators_samples_</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">bag_y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">bagging_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">estimators_samples_</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">bagging_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">bag_X1</span><span class="p">,</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">bag_X2</span><span class="p">,</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">bag_y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span>
                <span class="n">ylabelunit</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Bootstrap Data and Decision Tree #&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.4</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eea5b5097df4ef4a221f3d871149c4a47c31d4898b463afa9acc7b2d628ea79f.png" src="_images/eea5b5097df4ef4a221f3d871149c4a47c31d4898b463afa9acc7b2d628ea79f.png" />
</div>
</div>
<p>Notice the data changes for each model,</p>
<ul class="simple">
<li><p>we have bootstrapped the dataset so some of the data are missing and others are used 2 or more times</p></li>
<li><p>recall, in expectation, only 2/3 of the data are used for each tree, and 1/3 is out-of-bag</p></li>
</ul>
<p>Let’s check the cross validation results with the out-of-bag data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">seeds</span><span class="p">)):</span>                             <span class="c1"># check models over random number seeds</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">check_model</span><span class="p">(</span><span class="n">bagging_models</span><span class="p">[</span><span class="n">index</span><span class="p">],</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">&#39;Out-of-Bag Predictions Decision Tree #&#39;</span> <span class="o">+</span>  <span class="nb">str</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.6</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/783d535af6b2da8c5c69b4db7d4c9baff9168424e9c07006449e47e66dc7c771.png" src="_images/783d535af6b2da8c5c69b4db7d4c9baff9168424e9c07006449e47e66dc7c771.png" />
</div>
</div>
<p>Now let’s demonstrate the averaging of the predictions over the 6 decision trees, we are performing bagging tree prediction by-hand to clearly demonstrate the method.</p>
<ul class="simple">
<li><p>we average the predicted response feature (production) over the discretized predictor feature space</p></li>
<li><p>we can take advantage of broadcast methods for operations on entire arrays</p></li>
<li><p>we will apply the same model check, but we will use a modified function to will read in the response feature 2D array, instead of a model</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span><span class="p">:</span>                                            <span class="c1"># loop over random number seeds</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">+</span> <span class="n">pred</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>                                 <span class="c1"># calculate the average response over 3 trees</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">seeds</span><span class="p">)</span>                                            <span class="c1"># grid pixel-wise average of the 6 bootstrapped decision trees</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># plot predictions over predictor feature space</span>
<span class="n">visualize_grid</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span>
               <span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;All Data and Average of 6 Bootstrapped Trees&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>                                              <span class="c1"># check model predictions vs. testing dataset</span>
<span class="n">check_grid</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">&#39;Model Check - Average of 6 Bootstrapped Trees&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f20a801dbb20a98a7b2ad7c35c59f481b086f7e243c708954eaafbad1eeac330.png" src="_images/f20a801dbb20a98a7b2ad7c35c59f481b086f7e243c708954eaafbad1eeac330.png" />
</div>
</div>
<p>We made 6 complicated trees, each trained with bootstrap resamples of the original data and then averaged the predictions from each.</p>
<ul class="simple">
<li><p>the result is more smooth - lower model variance</p></li>
<li><p>the result more closely matches the training data</p></li>
</ul>
</section>
<section id="demonstration-of-bagging-with-increasing-number-of-trees">
<h2>Demonstration of Bagging with Increasing Number of Trees<a class="headerlink" href="#demonstration-of-bagging-with-increasing-number-of-trees" title="Permalink to this heading">#</a></h2>
<p>For demonstration, let’s build 6 bagging tree regression models with increasing number of overly complicated (and likely overfit) trees averaged.</p>
<ul class="simple">
<li><p>with the bagging regressor from scikit learn this is automated with the ‘num_tree’ hyperparameter</p></li>
</ul>
<p>We will loop over the models and store each of them in an list of models again!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">5</span>                           <span class="c1"># set for a complicated tree</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span><span class="p">)</span> <span class="c1"># instantiate a decision tree</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">73073</span><span class="p">;</span>
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">500</span><span class="p">]</span>                                 <span class="c1"># number of trees averaged for each estimator</span>

<span class="n">bagging_models_ntrees</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">oob_MSE</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                    <span class="c1"># visualize the models over number of trees</span>
    <span class="n">bagging_models_ntrees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">regressor</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">oob_score</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">bagging_models_ntrees</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">bagging_models_ntrees</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span>
                <span class="n">ylabelunit</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Bagging with &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_tree</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; Trees&#39;</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.4</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e9e7848dcfd2d82704e1e1d9b3907be8d850b9056a7478092436d1d3dd0376bb.png" src="_images/e9e7848dcfd2d82704e1e1d9b3907be8d850b9056a7478092436d1d3dd0376bb.png" />
</div>
</div>
<p>Observe the impact of averaging an increasing number of trees.</p>
<ul class="simple">
<li><p>we transition from a discontinuous response prediction model to a smooth prediction model (the jumps are smoothed out)</p></li>
</ul>
<p>Let’s repeat the modeling cross validation step with the withheld testing data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                    <span class="c1"># check models over number of trees</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">check_model_OOB_MSE</span><span class="p">(</span><span class="n">bagging_models_ntrees</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">&#39;Out-of-Bag Predictions with &#39;</span> <span class="o">+</span> 
                        <span class="nb">str</span><span class="p">(</span><span class="n">num_trees</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="s1">&#39; Decision Trees&#39;</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.6</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/897537eef68fdc77ecefa72c0cf2f943ddf6dd1f8cb70e99a54d498f9c901116.png" src="_images/897537eef68fdc77ecefa72c0cf2f943ddf6dd1f8cb70e99a54d498f9c901116.png" />
</div>
</div>
<p>See the improvement with testing accuracy with increasing level of ensemble model averaging?</p>
<p>Let’s run many cases and check the accuracy vs. number of trees.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">5</span>                           <span class="c1"># set for a complicated tree</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span><span class="p">)</span> <span class="c1"># instantiate a decision tree</span>
<span class="n">ntree_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE_oob_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">350</span><span class="p">,</span><span class="mi">50</span><span class="p">):</span>                          <span class="c1"># check OOB MSE over number of trees</span>
    <span class="n">bagg_tree</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">regressor</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">oob_score</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">bagg_tree</span><span class="o">.</span><span class="n">oob_prediction_</span>
    <span class="n">oob_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">oob_y_hat</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="c1"># remove if not estimated</span>
    <span class="n">MSE_oob_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">oob_y</span><span class="p">,</span><span class="n">oob_y_hat</span><span class="p">));</span> <span class="n">ntree_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_tree</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ntree_list</span><span class="p">,</span><span class="n">MSE_oob_list</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ntree_list</span><span class="p">,</span><span class="n">MSE_oob_list</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Bagged Trees&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Square Error&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Out-of-Bag Mean Square Error vs Number of Bagged Trees&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">));</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/20cacfb6fdbc25ddeb7189cfe3d06ce924ab316613a2ab54a7cf162211ee6ca9.png" src="_images/20cacfb6fdbc25ddeb7189cfe3d06ce924ab316613a2ab54a7cf162211ee6ca9.png" />
</div>
</div>
<p>The number of trees improves model accuracy through reduction in model variance. Let’s actually observe this reduction in model variance with an experiment.</p>
</section>
<section id="model-variance-vs-ensemble-model-averaging">
<h2>Model Variance vs. Ensemble Model Averaging<a class="headerlink" href="#model-variance-vs-ensemble-model-averaging" title="Permalink to this heading">#</a></h2>
<p>Let’s see the change in model variance through model averaging, we will compare multiple models with different numbers of trees averaged.</p>
<ul class="simple">
<li><p>we accomplish this by visual comparison, let’s look at different bagging modeling through changing the random number seed</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">5</span>                           <span class="c1"># set for a complicated tree</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span><span class="p">)</span> <span class="c1"># instantiate a decision tree</span>

<span class="n">seeds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">73083</span><span class="p">,</span> <span class="mi">73084</span><span class="p">,</span> <span class="mi">73085</span><span class="p">]</span>                                 <span class="c1"># number of random number seeds</span>
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">100</span><span class="p">]</span>                                      <span class="c1"># number of trees averaged for each estimator</span>
<span class="n">bagging_models_ntrees_seeds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">MSE_oob_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">ntree_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                    <span class="c1"># loop over number of trees</span>
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span><span class="p">:</span>                                        <span class="c1"># loop over number of random number seeds</span>
        <span class="n">bagg_tree</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">regressor</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span> 
                                     <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">oob_score</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">bagg_tree</span><span class="o">.</span><span class="n">oob_prediction_</span>
        <span class="n">oob_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">oob_y_hat</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="c1"># remove if not estimated</span>
        <span class="n">bagging_models_ntrees_seeds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bagg_tree</span><span class="p">)</span>
        <span class="n">MSE_oob_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">oob_y</span><span class="p">,</span><span class="n">oob_y_hat</span><span class="p">));</span> <span class="n">ntree_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_tree</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
        <span class="n">visualize_model</span><span class="p">(</span><span class="n">bagging_models_ntrees_seeds</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span>
                <span class="n">ylabelunit</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Training Data and Tree Model - &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_tree</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; Tree(s)&#39;</span><span class="p">)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b98738c95478440ea08fddbeba7798bc726e6077288dbed59915ba84d147fdd9.png" src="_images/b98738c95478440ea08fddbeba7798bc726e6077288dbed59915ba84d147fdd9.png" />
</div>
</div>
<p>As we increase the number of decision trees averaged for the bagged tree regression models:</p>
<ul class="simple">
<li><p>once again, the response predictions over the predictor feature space gets more smooth</p></li>
<li><p>the multiple realizations of the model start to converge, this is lower model variance</p></li>
</ul>
</section>
<section id="id1">
<h2>Random Forest<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>With random forest we limit the number of features considered for each split. Note, in scikit learn the default is <span class="math notranslate nohighlight">\(\frac{m}{3}\)</span>. Use this hyperparameter to set to square root of the number of predictor features. Another common alternative in practice <span class="math notranslate nohighlight">\(\sqrt{m}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_features</span> <span class="o">=</span> <span class="s1">&#39;sqrt&#39;</span>
</pre></div>
</div>
<p>This forces tree diversity / decorrelates the trees.</p>
<ul class="simple">
<li><p>recall the model variance reduced by averaging over multiple decision trees <span class="math notranslate nohighlight">\(Y = \frac{1}{B} \sum_{b=1}^{B} Y^b(X_1^b,...,X_m^b)\)</span></p></li>
<li><p>recall from the <a class="reference external" href="https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Spatial_Bootstrap.ipynb">spatial bootstrap workflow</a> that correlation of samples being averaged attenuates the variance reduction</p></li>
</ul>
<p>Let’s experiment with random forest to demonstrate this.</p>
<ol class="arabic simple">
<li><p>Set the hyperparameters.</p></li>
</ol>
<p>Even if I am just running one model, I set the random number seed to ensure I have a deterministic model, a model that can be rerun to get the same result every time. If the random number seed is not set, then it is likely set based on the system time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">73073</span>
</pre></div>
</div>
<p>We will overfit the trees, let them grow overly complicated. Once again, the ensemble approach will mitigate model variance and overfit.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
<p>We will use a large number of trees to mitigate model variance and to benefit from random forest tree diversity.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_tree</span> <span class="o">=</span> <span class="mi">300</span>
</pre></div>
</div>
<p>We are using a simple 2 predictor feature example for ease of visualization.  The default for scikit learn’s random forest is to select <span class="math notranslate nohighlight">\(\frac{m}{3}\)</span>  features at random for consideration for each split.</p>
<p>This doesn’t make much sense when <span class="math notranslate nohighlight">\(m = 2\)</span>, as with our case, so we set the maximum number of features considered for each split to 1.</p>
<ul class="simple">
<li><p>We are forcing random selection of porosity or brittleness for consideration with each split, hierarchical binary segmentation.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Instantiate the random forest regressor with our hyperparameters</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">my_first_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Train the random forest regression</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">my_first_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>Visualize the model result over the feature space (easy to do as we have only 2 predictor features)</p></li>
</ol>
<p>Let’s build, visualize and cross validate our first random forest regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">73093</span>                                                  <span class="c1"># set the random forest hyperparameters</span>
<span class="n">max_depth</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">my_first_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
                                       <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">my_first_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>                             <span class="c1"># train the model with training data </span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># predict with the model over the predictor feature space and visualize</span>
<span class="n">visualize_model</span><span class="p">(</span><span class="n">my_first_forest</span><span class="p">,</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span>
                <span class="n">ylabelunit</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Training Data and Random Forest Model&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>                                              <span class="c1"># perform cross validation with withheld testing data</span>
<span class="n">check_model_OOB_MSE</span><span class="p">(</span><span class="n">my_first_forest</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">&#39;Out-of-Bag Predictions with Random Forest&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/95b5bfda652444fd962f82e48a68222434830f5645e78fb1598f64cd5a3e888e.png" src="_images/95b5bfda652444fd962f82e48a68222434830f5645e78fb1598f64cd5a3e888e.png" />
</div>
</div>
<p>The power of tree diversity!  We just built our best model so far.</p>
<ul class="simple">
<li><p>the conditional bias has decreased (our plot has a slope closer to 1:1)</p></li>
<li><p>we have the lower out-of-bag mean score error</p></li>
</ul>
<p>Let’s run some tests to make sure we understand random forest regression model.</p>
<p>First let’s confirm that only one feature (at random) is considered for each split</p>
<ul class="simple">
<li><p>limit ourselves to maximum depth = 1, only one split</p></li>
<li><p>limit ourselves to a single tree in each forest!</p></li>
</ul>
<p>This way we can see the diversity in the first splits over multiple models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span>                                                 <span class="c1"># set the random forest hyperparameters</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">simple_forest</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">seeds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">73103</span><span class="p">,</span><span class="mi">73104</span><span class="p">,</span><span class="mi">73105</span><span class="p">,</span><span class="mi">73106</span><span class="p">,</span><span class="mi">73107</span><span class="p">,</span><span class="mi">73108</span><span class="p">]</span>                 <span class="c1"># set the random number seeds</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span><span class="p">:</span>                                            <span class="c1"># loop over random number seeds</span>
    <span class="n">simple_forest</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">))</span>
    <span class="n">simple_forest</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>                                    <span class="c1"># predict with the model over the predictor feature space and visualize</span>
    <span class="n">visualize_model</span><span class="p">(</span><span class="n">simple_forest</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span>
                <span class="n">ylabelunit</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Training Data and Random Forest Model&#39;</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c161b87c59434bdd65f9831ab841ba71436cfc60c9e04fa4d5980bbfa7bd80c5.png" src="_images/c161b87c59434bdd65f9831ab841ba71436cfc60c9e04fa4d5980bbfa7bd80c5.png" />
</div>
</div>
<p>Notice that the first splits are 50/50 porosity and brittleness.</p>
<ul class="simple">
<li><p>aside, for all decision trees that I have fit to this dataset, porosity is always the feature selected for the first 2-3 levels of the tree.</p></li>
<li><p>the random forest has resulted in model diversity by limiting the predictor features under consideration for the first split!</p></li>
</ul>
<p>Just incase you don’t trust this, let’s rerun the above code with both predictors allowed for all splits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span>                                                 <span class="c1"># set the random forest hyperparameters</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">simple_forest</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">seeds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">73103</span><span class="p">,</span><span class="mi">73104</span><span class="p">,</span><span class="mi">73105</span><span class="p">,</span><span class="mi">73106</span><span class="p">,</span><span class="mi">73107</span><span class="p">,</span><span class="mi">73108</span><span class="p">]</span>                 <span class="c1"># random number seeds </span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span><span class="p">:</span>                                            <span class="c1"># loop over random number seeds</span>
    <span class="n">simple_forest</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">))</span>
    <span class="n">simple_forest</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>                                    <span class="c1"># predict with the model over the predictor feature space and visualize</span>
    <span class="n">visualize_model</span><span class="p">(</span><span class="n">simple_forest</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span>
                <span class="n">ylabelunit</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Training Data and Random Forest Model&#39;</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1b44bed65bdc0d762afa4706304f8c37a7f517a522d534c448aabf24ae1a9e9a.png" src="_images/1b44bed65bdc0d762afa4706304f8c37a7f517a522d534c448aabf24ae1a9e9a.png" />
</div>
</div>
<p>Now we have a set of first splits that vary (due to the bootstrap of the training data), but are all over porosity.</p>
</section>
<section id="model-performance-by-out-of-bag-and-feature-importance">
<h2>Model Performance by Out-of-Bag and Feature Importance<a class="headerlink" href="#model-performance-by-out-of-bag-and-feature-importance" title="Permalink to this heading">#</a></h2>
<p>Since we are now building a more robust model with a large ensemble of trees, let’s get more serious about model checking.</p>
<ul class="simple">
<li><p>we will look at out-of-bag mean square error</p></li>
<li><p>we will look at feature importance</p></li>
</ul>
<p>Let’s start with a pretty big forest, this may take a while to run!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">73093</span>                                                  <span class="c1"># set the random forest hyperparameters</span>
<span class="n">max_depth</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">big_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
                                   <span class="n">oob_score</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span><span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">big_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                              <span class="c1"># predict with the model over the predictor feature space and visualize</span>
<span class="n">visualize_model</span><span class="p">(</span><span class="n">big_forest</span><span class="p">,</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span>
                <span class="n">ylabelunit</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Training Data and Random Forest Model&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>                                              <span class="c1"># perform cross validation with withheld testing data</span>
<span class="n">check_model_OOB_MSE</span><span class="p">(</span><span class="n">big_forest</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">&#39;Model Check Random Forest Model&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/735c2983e57fae5ab1825ca0a3a59ee1c45ab833f9d685009c96b01ecdc3728b.png" src="_images/735c2983e57fae5ab1825ca0a3a59ee1c45ab833f9d685009c96b01ecdc3728b.png" />
</div>
</div>
<p>To get the feature importance we just have to access the model member ‘feature_importance_’.</p>
<ul class="simple">
<li><p>we had to set feature_importance to true in the model instantiation for this to be available</p></li>
<li><p>this measure is standardized to sum to 1.0</p></li>
<li><p>same order as the predictor features in the 2D array, porosity and then brittleness</p></li>
<li><p>feature importance is the proportion of total MSE reduction through splits for each feature</p></li>
<li><p>we can access the importance for each feature for each tree in the forest or the global average for each over the entire forest</p></li>
</ul>
<p>We get the global average of feature importance with this member of the random forest regressor model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">importances</span> <span class="o">=</span> <span class="n">big_forest</span><span class="o">.</span><span class="n">feature_importances_</span> 
</pre></div>
</div>
<p>Let’s plot the feature importance with significance calculated from the ensemble.</p>
<ul class="simple">
<li><p>when we report model-based feature importance, it is always a good idea to show that the model is a good model. I like to show a model check beside the feature importance result, in this case the out-of-bag cross validation plot and mean square error.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importances</span> <span class="o">=</span> <span class="n">big_forest</span><span class="o">.</span><span class="n">feature_importances_</span>                 <span class="c1"># expected (global) importance over the forest fore each predictor feature</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">big_forest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># retrieve importance by tree</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>                       <span class="c1"># sort in descending feature importance</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Porosity&#39;</span><span class="p">,</span><span class="s1">&#39;Brittleness&#39;</span><span class="p">]</span>                         <span class="c1"># names or predictor features</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Forest Feature Importances&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">importances</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;darkorange&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predictor Features&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature Importance&#39;</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>                                              <span class="c1"># perform cross validation with withheld testing data</span>
<span class="n">check_model_OOB_MSE</span><span class="p">(</span><span class="n">big_forest</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">&#39;Model Check Random Forest Model for Feature Importance&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.6</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bfd529c0f34dd25a412354a45b90571b09239a3df8498d1354f217fdd4261c67.png" src="_images/bfd529c0f34dd25a412354a45b90571b09239a3df8498d1354f217fdd4261c67.png" />
</div>
</div>
<p>Let’s try some hyperparameter training with the out-of-bag mean square error measure from our forest.</p>
<p>Let’s start with the number of trees in our forest.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">5</span>                                                 <span class="c1"># set the random forest hyperparameters</span>
<span class="n">num_trees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">trained_forests</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">MSE_oob_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">ntree_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                     <span class="c1"># loop over number of trees in our random forest</span>
    <span class="n">trained_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span>
            <span class="n">oob_score</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span><span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">trained_forests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trained_forest</span><span class="p">)</span>
    <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">trained_forest</span><span class="o">.</span><span class="n">oob_prediction_</span>
    <span class="n">oob_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">oob_y_hat</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="c1"># remove if not estimated</span>
    <span class="n">MSE_oob_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">oob_y</span><span class="p">,</span><span class="n">oob_y_hat</span><span class="p">));</span> <span class="n">ntree_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_tree</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ntree_list</span><span class="p">,</span><span class="n">MSE_oob_list</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ntree_list</span><span class="p">,</span><span class="n">MSE_oob_list</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Random Forest Trees&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Square Error&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Out-of-Bag Mean Square Error vs Number of Random Forest Trees&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">));</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">ntree_list</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">ntree_list</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/db70240c4201575375d6d14e35f366a22e45978000e8acabb77231b8d2d2b89f.png" src="_images/db70240c4201575375d6d14e35f366a22e45978000e8acabb77231b8d2d2b89f.png" />
</div>
</div>
<p>Now let’s try the depth of the trees, given enough trees (we’ll use 60 trees) as determined above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>                             <span class="c1"># set the tree maximum tree depths to consider</span>

<span class="n">num_tree</span> <span class="o">=</span> <span class="mi">60</span>                                                 <span class="c1"># set the random forest hyperparameters</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">trained_forests</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">MSE_oob_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">max_depth_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">max_depth</span> <span class="ow">in</span> <span class="n">max_depths</span><span class="p">:</span>                                  <span class="c1"># loop over tree depths    </span>
    <span class="n">trained_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">max_depth</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_tree</span><span class="p">,</span>
            <span class="n">oob_score</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span><span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">trained_forests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trained_forest</span><span class="p">)</span>
    <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">trained_forest</span><span class="o">.</span><span class="n">oob_prediction_</span>
    <span class="n">oob_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="n">oob_y_hat</span> <span class="o">=</span> <span class="n">oob_y_hat</span><span class="p">[</span><span class="n">oob_y_hat</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">];</span> <span class="c1"># remove if not estimated</span>
    <span class="n">MSE_oob_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">oob_y</span><span class="p">,</span><span class="n">oob_y_hat</span><span class="p">));</span> <span class="n">max_depth_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">max_depth</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>                                                <span class="c1"># plot OOB MSE vs. maximum tree depth</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">max_depth_list</span><span class="p">,</span><span class="n">MSE_oob_list</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">max_depth_list</span><span class="p">,</span><span class="n">MSE_oob_list</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Tree Maximum Depth&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Square Error&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Out-of-Bag Mean Square Error vs Tree Maximum Depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">));</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">max_depth_list</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">max_depth_list</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/50da58b3e08eb8830bbeb48b559c70b77602e790fec0e4d361e547e0c74ffb4b.png" src="_images/50da58b3e08eb8830bbeb48b559c70b77602e790fec0e4d361e547e0c74ffb4b.png" />
</div>
</div>
<p>It looks like we need a maximum tree depth of at least 10 for best performance of our model with respect to out-of-bag mean square error.</p>
<ul class="simple">
<li><p>note that our model is robust and resistant to overfit, the out-of-bag performance evaluation is close to monotonically increasing.</p></li>
</ul>
</section>
<section id="machine-learning-pipelines-for-clean-compact-machine-learning-code">
<h2>Machine Learning Pipelines for Clean, Compact Machine Learning Code<a class="headerlink" href="#machine-learning-pipelines-for-clean-compact-machine-learning-code" title="Permalink to this heading">#</a></h2>
<p>Pipelines are a scikit-learn class that allows for the encapsulation of a sequence of data preparation and modeling steps</p>
<ul class="simple">
<li><p>then we can treat the pipeline as an object in our much condensed workflow</p></li>
</ul>
<p>The pipeline class allows us to:</p>
<ul class="simple">
<li><p>improve code readability and to keep everything straight</p></li>
<li><p>build complete workflows with very few lines of readable code</p></li>
<li><p>avoid common workflow problems like data leakage, testing data informing model parameter training</p></li>
<li><p>abstract common machine learning modeling and focus on building the best model possible</p></li>
</ul>
<p>The fundamental philosophy is to treat machine learning as a combinatorial search to find the best model (AutoML)</p>
<p>For more information see my recorded lecture on <a class="reference external" href="https://www.youtube.com/watch?v=tYrPs8s1l9U&amp;list=PLG19vXLQHvSAufDFgZEFAYQEwMJXklnQV&amp;index=5">Machine Learning Pipelines</a> and a well-documented demonstration <a class="reference external" href="http://localhost:8892/notebooks/OneDrive%20-%20The%20University%20of%20Texas%20at%20Austin/Courses/Workflows/PythonDataBasics_Pipelines.ipynb">Machine Learning Pipeline Workflow</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">;</span> <span class="n">x2</span> <span class="o">=</span> <span class="mf">0.3</span>                                           <span class="c1"># predictor values for the prediction</span>

<span class="n">pipe_forest</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>                                      <span class="c1"># the machine learning workflow as a pipeline object</span>
    <span class="p">(</span><span class="s1">&#39;forest&#39;</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>                                                    <span class="c1"># the machine learning workflow method&#39;s parameters to search</span>
    <span class="s1">&#39;forest__max_leaf_nodes&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">dtype</span> <span class="o">=</span> <span class="nb">int</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">tuned_forest</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe_forest</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="c1"># hyperparameter tuning w. grid search k-fold cross validation </span>
                             <span class="n">refit</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">tuned_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>                                         <span class="c1"># fit model with tuned hyperparameters to all the data</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tuned hyperparameter: max_leaf_nodes = &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tuned_forest</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>

<span class="n">estimate</span> <span class="o">=</span> <span class="n">tuned_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>                 <span class="c1"># make a prediction (no tuning shown)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Estimated &#39;</span> <span class="o">+</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">&#39; for &#39;</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; = &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; and &#39;</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; = &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>  <span class="o">+</span> <span class="s1">&#39; is &#39;</span> <span class="o">+</span> 
      <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">estimate</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">yunit</span><span class="p">)</span> <span class="c1"># print results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tuned hyperparameter: max_leaf_nodes = {&#39;forest__max_leaf_nodes&#39;: 64}
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 2001.2 MCFPD
</pre></div>
</div>
</div>
</div>
</section>
<section id="comments">
<h2>Comments<a class="headerlink" href="#comments" title="Permalink to this heading">#</a></h2>
<p>I hope you found this chapter helpful. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a>,</p>
<p><em>Michael</em></p>
</section>
<section id="the-author">
<h2>The Author:<a class="headerlink" href="#the-author" title="Permalink to this heading">#</a></h2>
<p>Michael Pyrcz, Professor, The University of Texas at Austin
<em>Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions</em></p>
<p>With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers’ and geoscientists’ impact in subsurface resource development.</p>
<p>For more about Michael check out these links:</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a> | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?<a class="headerlink" href="#want-to-work-together" title="Permalink to this heading">#</a></h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I’d be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz&#37;&#52;&#48;austin&#46;utexas&#46;edu">mpyrcz<span>&#64;</span>austin<span>&#46;</span>utexas<span>&#46;</span>edu</a>.</p></li>
</ul>
<p>I’m always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
</section>
<section id="more-resources-available-at-twitter-github-website-googlescholar-book-youtube-applied-geostats-in-python-e-book-linkedin">
<h2>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a><a class="headerlink" href="#more-resources-available-at-twitter-github-website-googlescholar-book-youtube-applied-geostats-in-python-e-book-linkedin" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="MachineLearning_decision_tree.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Decision Tree</p>
      </div>
    </a>
    <a class="right-next"
       href="MachineLearning_gradient_boosting.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gradient Boosting Trees</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivations-for-bagging-and-random-forest">Motivations for Bagging and Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-model-formulation">Tree Model Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-loss-functions">Tree Loss Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-tree-model">Training the Tree Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-the-tree-model">Tuning the Tree Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-methods">Ensemble Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrap">Bootstrap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging-models">Bagging Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-tuning-bagging-models">Training and Tuning Bagging Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#out-of-bag-cross-validation">Out-of-Bag Cross Validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#number-of-estimators">Number of Estimators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimator-complexity">Estimator Complexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-bagging">Tree Bagging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest">Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-required-libraries">Load the Required Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#declare-functions">Declare Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-the-working-directory">Set the working directory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-data">Loading Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering">Feature Engineering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-add-random-noise-to-the-response-feature">Optional: Add Random Noise to the Response Feature</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-the-correlation-matrix-and-correlation-with-response-ranking">Calculate the Correlation Matrix and Correlation with Response Ranking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-and-test-split">Train and Test Split</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-the-dataframe">Visualize the DataFrame</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-statistics-for-tabular-data">Summary Statistics for Tabular Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-the-distributions">Visualize the Distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-tree-method-tree-bagging-regression">Ensemble Tree Method - Tree Bagging Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration-of-bagging-by-hand">Demonstration of Bagging by-Hand</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration-of-bagging-with-increasing-number-of-trees">Demonstration of Bagging with Increasing Number of Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-variance-vs-ensemble-model-averaging">Model Variance vs. Ensemble Model Averaging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-performance-by-out-of-bag-and-feature-importance">Model Performance by Out-of-Bag and Feature Importance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-pipelines-for-clean-compact-machine-learning-code">Machine Learning Pipelines for Clean, Compact Machine Learning Code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">Comments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-author">The Author:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-work-together">Want to Work Together?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-resources-available-at-twitter-github-website-googlescholar-book-youtube-applied-geostats-in-python-e-book-linkedin">More Resources Available at: Twitter | GitHub | Website | GoogleScholar | Book | YouTube  | Applied Geostats in Python e-book | LinkedIn</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael J. Pyrcz
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024 CC-BY-SA 4.0.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>