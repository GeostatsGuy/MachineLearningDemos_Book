

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Machine Learning Glossary &#8212; Applied Machine Learning in Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'MachineLearning_glossary';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="References" href="references.html" />
    <link rel="prev" title="Conclusions" href="conclusions.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/AppliedMachineLearning.jpg" class="logo__image only-light" alt="Applied Machine Learning in Python - Home"/>
    <script>document.write(`<img src="_static/AppliedMachineLearning.jpg" class="logo__image only-dark" alt="Applied Machine Learning in Python - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_concepts.html">Machine Learning Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_training_tuning.html">Training and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_workflow_construction.html">Workflow Construction and Coding</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_probability.html">Probability Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_plotting_data_models.html">Loading and Plotting Data and Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_univariate_analysis.html">Univariate Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_multivariate_analysis.html">Multivariate Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_feature_transformations.html">Feature Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_feature_ranking.html">Feature Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_feature_imputation.html">Feature Imputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_clustering.html">Cluster Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_density-based_clustering.html">Density-based Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_spectral_clustering.html">Spectral Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_PCA.html">Principal Components Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_multidimensional_scaling.html">Multidimensional Scaling</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_random_projection.html">Random Projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_ridge_regression.html">Ridge Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_LASSO_regression.html">LASSO Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html">Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_naive_Bayes.html">Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_polynomial_regression.html">Polynomial Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_knearest_neighbours.html">k-Nearest Neighbours</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_decision_tree.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_ensemble_trees.html">Bagging Tree and Random Forest</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_gradient_boosting.html">Gradient Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_support_vector_machines.html">Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning_time_series.html">Time Series Analysis and Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusions.html">Conclusions</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/GeostatsPyDemos_Book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/GeostatsPyDemos_Book/issues/new?title=Issue%20on%20page%20%2FMachineLearning_glossary.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/MachineLearning_glossary.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine Learning Glossary</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-machine-learning-concepts">Motivation for Machine Learning Concepts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjacency-matrix-spectral-clustering"><strong>Adjacency Matrix</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#addition-rule-probability"><strong>Addition Rule</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#affine-correction"><strong>Affine Correction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#affinity-matrix-spectral-clustering"><strong>Affinity Matrix</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging-models"><strong>Bagging Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-expansion"><strong>Basis Expansion</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-function"><strong>Basis Function</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-probability"><strong>Bayes’ Theorem</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-probability"><strong>Bayesian Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-updating-for-classification"><strong>Bayesian Updating for Classification</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression"><strong>Bayesian Linear Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#big-data"><strong>Big Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#big-data-analytics"><strong>Big Data Analytics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-transform-also-indicator-transform"><strong>Binary Transform</strong> (also Indicator Transform)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boosting-models"><strong>Boosting Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrap"><strong>Bootstrap</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-feature"><strong>Categorical Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-nominal-feature"><strong>Categorical Nominal Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-ordinal-feature"><strong>Categorical Ordinal Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#causation"><strong>Causation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cell-based-declustering"><strong>Cell-based Declustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cognitive-biases"><strong>Cognitive Biases</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complimentary-events-probability"><strong>Complimentary Events</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity"><strong>Computational Complexity</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability"><strong>Conditional Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-interval"><strong>Confidence Interval</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix"><strong>Confusion Matrix</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-feature"><strong>Continuous Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-interval-feature"><strong>Continuous, Interval Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-ratio-feature"><strong>Continuous, Ratio Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuously-differentiable"><strong>Continuously Differentiable</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution"><strong>Convolution</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-data"><strong>Core Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation"><strong>Correlation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance"><strong>Covariance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation"><strong>Cross Validation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-function-cdf"><strong>Cumulative Distribution Function</strong> (CDF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curse-of-dimensionality"><strong>Curse of Dimensionality</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-data-aspects"><strong>Data</strong> (data aspects)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-convexity"><strong>Data Convexity</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataframe"><strong>DataFrame</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-analytics"><strong>Data Analytics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation"><strong>Data Preparation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#degree-matrix-spectral-clustering"><strong>Degree Matrix</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dbscan-for-density-based-clustering"><strong>DBSCAN for Density-based Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-criteria"><strong>Decision Criteria</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree"><strong>Decision Tree</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#declustering"><strong>Declustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#declustering-statistics"><strong>Declustering</strong> (statistics)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#density-connected-dbscan"><strong>Density-Connected</strong> (DBSCAN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#density-based-cluster-dbscan"><strong>Density-based Cluster</strong> (DBSCAN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#density-reachable-dbscan"><strong>Density-Reachable</strong> (DBSCAN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-model"><strong>Deterministic Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction"><strong>Dimensionality Reduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#directly-density-reachable-dbscan"><strong>Directly Density Reachable</strong> (DBSCAN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-feature"><strong>Discrete Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution-transformations"><strong>Distribution Transformations</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eager-learning"><strong>Eager Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation"><strong>Estimation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#f1-score-classification-accuracy-metric"><strong>f1-score</strong> (classification accuracy metric)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-also-variable"><strong>Feature</strong> (also variable)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering"><strong>Feature Engineering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance"><strong>Feature Importance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-imputation"><strong>Feature Imputation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-projection"><strong>Feature Projection</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-space"><strong>Feature Space</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-ranking"><strong>Feature Ranking</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-transformations"><strong>Feature Transformations</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourth-paradigm"><strong>Fourth Paradigm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-probability"><strong>Frequentist Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-anamorphosis"><strong>Gaussian Anamorphosis</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gibbs-sampler-mcmc"><strong>Gibbs Sampler</strong> (MCMC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-boosting-models"><strong>Gradient Boosting Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-laplacian-spectral-clustering"><strong>Graph Laplacian</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geostatistics"><strong>Geostatistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-based-optimization"><strong>Gradient-based Optimization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-spectral-clustering"><strong>Graph</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gridded-data"><strong>Gridded Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hard-data"><strong>Hard Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hermite-polynomials"><strong>Hermite Polynomials</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#heuristic-algorithm"><strong>Heuristic Algorithm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-clustering"><strong>Hierarchical Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#histogram"><strong>Histogram</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hybrid-model"><strong>Hybrid Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-probability"><strong>Independence</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indicator-transform-also-binary-transform"><strong>Indicator Transform</strong> (also Binary Transform)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indicator-variogram"><strong>Indicator Variogram</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-inferential-statistics"><strong>Inference, Inferential Statistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inlier"><strong>Inlier</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instance-based-learning"><strong>Instance-based Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intersection-of-events-probability"><strong>Intersection of Events</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#irreducible-error"><strong>Irreducible Error</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inertia-clustering"><strong>Inertia</strong> (clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-probability"><strong>Joint Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-bins-discretization"><strong>K Bins Discretization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation"><strong>K-fold Cross Validation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering"><strong>k-Means Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbours"><strong>k-Nearest Neighbours</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-trick-support-vector-machines"><strong>Kernel Trick</strong> (support vector machines)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kriging"><strong>Kriging</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kriging-based-declustering"><strong>Kriging-based Declustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kolmogorovs-3-probability-axioms"><strong>Kolmogorov’s 3 Probability Axioms</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-1-norm"><strong><span class="math notranslate nohighlight">\(L^1\)</span> Norm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-2-norm"><strong><span class="math notranslate nohighlight">\(L^2\)</span> Norm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-1-vs-l-2-norm"><strong><span class="math notranslate nohighlight">\(L^1\)</span> vs. <span class="math notranslate nohighlight">\(L^2\)</span> Norm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-1-or-l-2-normalizer"><strong><span class="math notranslate nohighlight">\(L^1\)</span> or <span class="math notranslate nohighlight">\(L^2\)</span> Normalizer</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression"><strong>LASSO Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lazy-learning"><strong>Lazy Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-gradient-boosting"><strong>Learning Rate</strong> (gradient boosting)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likewise-deletion-mrmr"><strong>Likewise Deletion</strong> (MRMR)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression"><strong>Linear Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#location-map"><strong>Location Map</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function"><strong>Loss Function</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-workflow-design"><strong>Machine Learning Workflow Design</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#margin-support-vector-machines"><strong>Margin</strong> (support vector machines)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-probability"><strong>Marginal Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-scatter-plots"><strong>Matrix Scatter Plots</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-relevance-minimum-redundancy-mrmr"><strong>Maximum Relevance Minimum Redundancy</strong> (MRMR)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metropolis-hastings-mcmc-sampler"><strong>Metropolis-Hastings MCMC Sampler</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minkowski-distance"><strong>Minkowski Distance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-feature-values"><strong>Missing Feature Values</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-at-random-mar"><strong>Missing at Random</strong> (MAR)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-bias"><strong>Model Bias</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-bias-variance-trade-off"><strong>Model-Bias Variance Trade-off</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-checking"><strong>Model Checking</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-complexity-or-flexibility"><strong>Model Complexity or Flexibility</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-generalization"><strong>Model Generalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-hyperparameters"><strong>Model Hyperparameters</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parameters"><strong>Model Parameters</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-regularization">Model Regularization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-variance"><strong>Model Variance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum-optimization"><strong>Momentum</strong> (optimization)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo-mcmc"><strong>Markov Chain Monte Carlo</strong> (MCMC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metropolis-hastings-sampling-mcmc"><strong>Metropolis-Hastings Sampling</strong> (MCMC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-simulation-mcs"><strong>Monte Carlo Simulation (MCS)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-simulation-workflow"><strong>Monte Carlo Simulation Workflow</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiplication-rule-probability"><strong>Multiplication Rule</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-information"><strong>Mutual Information</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutually-exclusive-events-probability"><strong>Mutually Exclusive Events</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multidimensional-scaling"><strong>Multidimensional Scaling</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes"><strong>Naive Bayes</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ndarray"><strong>ndarray</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nonparametric-model"><strong>Nonparametric Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#norm"><strong>Norm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization"><strong>Normalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-histogram"><strong>Normalized Histogram</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding"><strong>One Hot Encoding</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#out-of-bag-sample"><strong>Out-of-Bag Sample</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfit-model"><strong>Overfit Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-statistics"><strong>Parameters</strong> (statistics)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-machine-learning"><strong>Parameters</strong> (machine learning)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-model"><strong>Parametric Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-correlation-coefficient"><strong>Partial Correlation Coefficient</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partitional-clustering"><strong>Partitional Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#polygonal-declustering"><strong>Polygonal Declustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-regression"><strong>Polynomial Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#population"><strong>Population</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#power-law-average"><strong>Power Law Average</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-classification-accuracy-metric"><strong>Precision</strong> (classification accuracy metric)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-interval"><strong>Prediction Interval</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-predictive-statistics"><strong>Prediction, Predictive Statistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-feature"><strong>Predictor Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-feature-space"><strong>Predictor Feature Space</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#primary-data"><strong>Primary Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis"><strong>Principal Component Analysis</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-function-pdf"><strong>Probability Density Function</strong> (PDF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-non-negativity-normalization"><strong>Probability Non-negativity, Normalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-of-acceptance-mcmc"><strong>Probability of Acceptance</strong> (MCMC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-operators"><strong>Probability Operators</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-perspectives"><strong>Probability Perspectives</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prototype-clustering"><strong>Prototype</strong> (clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qualitative-features"><strong>Qualitative Features</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantitative-features"><strong>Quantitative Features</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-2-also-coefficient-of-determination"><strong><span class="math notranslate nohighlight">\(r^2\)</span></strong> (also coefficient of determination)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest"><strong>Random Forest</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#realization"><strong>Realization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#realizations-uncertainty"><strong>Realizations</strong> (uncertainty)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reasons-to-learn-some-coding"><strong>Reasons to Learn Some Coding</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-classification-accuracy-metric"><strong>Recall</strong> (classification accuracy metric)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-feature-elimination"><strong>Recursive Feature Elimination</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reservoir-modeling-workflow"><strong>Reservoir Modeling Workflow</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#response-feature"><strong>Response Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-tikhonov-regularization"><strong>Ridge Regression</strong> (Tikhonov Regularization)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample"><strong>Sample</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenarios-uncertainty"><strong>Scenarios</strong> (uncertainty)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#secondary-data"><strong>Secondary Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seismic-data"><strong>Seismic Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shapley-value"><strong>Shapley Value</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simpson-s-paradox"><strong>Simpson’s Paradox</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#soft-data"><strong>Soft Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-sampling-biased"><strong>Spatial Sampling</strong> (biased)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-sampling-clustered"><strong>Spatial Sampling</strong> (clustered)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-sampling-common-practice"><strong>Spatial Sampling</strong> (common practice)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-sampling-representative"><strong>Spatial Sampling</strong> (representative)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-clustering"><strong>Spectral Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization"><strong>Standardization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-based-optimization"><strong>Stochastic Gradient-based Optimization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-model"><strong>Stochastic Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-practice"><strong>Statistics</strong> (practice)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-measurement"><strong>Statistics</strong> (measurement)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-distribution"><strong>Statistical Distribution</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-support-vector-machines"><strong>Support Vector</strong> (support vector machines)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machines"><strong>Support Vector Machines</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-data"><strong>Tabular Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-testing-splits"><strong>Training and Testing Splits</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-function-reservoir-modeling-workflow"><strong>Transfer Function</strong> (reservoir modeling workflow)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-modeling"><strong>Uncertainty Modeling</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underfit-model"><strong>Underfit Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#union-of-events-probability"><strong>Union of Events</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-parameters"><strong>Univariate Parameters</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-statistics"><strong>Univariate Statistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning"><strong>Unsupervised Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-also-feature"><strong>Variable</strong> (also feature)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-inflation-factor-vif"><strong>Variance Inflation Factor</strong> (VIF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#volume-variance-relations"><strong>Volume-Variance Relations</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#venn-diagrams"><strong>Venn Diagrams</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#well-log-data"><strong>Well Log Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weak-learner"><strong>Weak Learner</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#well-log-data-image-logs"><strong>Well Log Data, Image Logs</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">Comments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-author">The Author:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-work-together">Want to Work Together?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <figure style="text-align: center;">
  <img src="_static/intro/title_page.png" style="display: block; margin: 0 auto; width: 100%;">
</figure>
<section id="machine-learning-glossary">
<h1>Machine Learning Glossary<a class="headerlink" href="#machine-learning-glossary" title="Permalink to this heading">#</a></h1>
<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book “Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy”.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book">https://geostatsguy.github.io/GeostatsPyDemos_Book</a>.</p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the GeostatsPyDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, GeostatsPyDemos: GeostatsPy Python Package for Spatial Data Analytics and Geostatistics Demonstration Workflows Repository (0.0.1). Zenodo. <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.12667035">https://zenodo.org/doi/10.5281/zenodo.12667035</a></p>
<p><a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.12667035"><img alt="DOI" src="https://zenodo.org/badge/777871341.svg" /></a></p>
</div>
<p>By Michael J. Pyrcz <br />
© Copyright 2024.</p>
<p>This chapter is a summary of essential <strong>Machine Learning Terminology</strong>.</p>
<section id="motivation-for-machine-learning-concepts">
<h2>Motivation for Machine Learning Concepts<a class="headerlink" href="#motivation-for-machine-learning-concepts" title="Permalink to this heading">#</a></h2>
<p>Firstly, why do this? I have received the request for a course glossary from the students in my <strong>Subsurface Machine Learning</strong> combined undergraduate and graduate course. While I usually dedicate a definition slide in the lecture slide decks for salient terms, some of my students have requested course glossary, list of terminology for their course review. The e-book provides a great vehicle and motivation to finally complete this.</p>
<p>Let me begin with a confession. There is a <a class="reference external" href="https://developers.google.com/machine-learning/glossary">Machine Learning Glossary</a> written by Google developers. For those seeking the in depth, comprehensive list of geostatistical terms please use this book!</p>
<ul class="simple">
<li><p>By writing my own glossary I can limit the scope and descriptions to course content. I fear that many students would be overwhelmed by the size and mathematical notation of a standard machine learning glossary.</p></li>
<li><p>Also, by including a glossary in the e-book I can link from glossary entries to the chapters in the e-book for convenience. I will eventual populate all the chapters with hyperlinks to the glossary to enable moving back and forth between the chapters and the glossary.</p></li>
</ul>
<p>Finally, like the rest of the book, I want the glossary to be a evergreen living document.</p>
</section>
<section id="adjacency-matrix-spectral-clustering">
<h2><strong>Adjacency Matrix</strong> (spectral clustering)<a class="headerlink" href="#adjacency-matrix-spectral-clustering" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_spectral_clustering.html"><span class="doc std std-doc">Spectral Clustering</span></a>: a matrix representing a graph with the pairwise connections between all pairwise combinations of graph nodes, samples.</p>
<ul class="simple">
<li><p>the values are indicators, 0 if not connected, 1 if connected</p></li>
</ul>
<p>Note, node self connections are set to 0 in the adjacency matrix</p>
</section>
<section id="addition-rule-probability">
<h2><strong>Addition Rule</strong> (probability)<a class="headerlink" href="#addition-rule-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: when we add probabilities (the union of outcomes), the probability of <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> is calculated with the probability addition rule,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = P(A) + P(B) - P(A,B)
\]</div>
<p>given mutually exclusive events we can generalize the addition rule as,</p>
<div class="math notranslate nohighlight">
\[
P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i)
\]</div>
</section>
<section id="affine-correction">
<h2><strong>Affine Correction</strong><a class="headerlink" href="#affine-correction" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: a distribution rescaling that can be thought of as shifting, and stretching or squeezing of a univariate distribution (e.g., <em>histogram</em>). For the case of affine correction of <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span>,</p>
<div class="math notranslate nohighlight">
\[
y_i = \frac{\sigma_y}{\sigma_x}(x_i - \overline{x}) + \overline{y}, \quad \forall \quad i, \ldots, n
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{x}\)</span> and <span class="math notranslate nohighlight">\(\sigma_x\)</span> are the original mean and variance, and <span class="math notranslate nohighlight">\(\overline{y}\)</span> and <span class="math notranslate nohighlight">\(\sigma_y\)</span> are the new mean and variance.</p>
<p>We can see above that the affine correlation method first centers the distribution (by subtracting the original mean), then rescales the dispersion (distribution spread) by the ratio of the new standard deviation to the original standard deviation and then shifts the distribution to centered on the new mean.</p>
<ul class="simple">
<li><p>there is no shape change for affine correction. For shape change consider <em>Distribution Transformation</em> like <em>Gaussian Anamorphosis</em>.</p></li>
</ul>
</section>
<section id="affinity-matrix-spectral-clustering">
<h2><strong>Affinity Matrix</strong> (spectral clustering)<a class="headerlink" href="#affinity-matrix-spectral-clustering" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_spectral_clustering.html"><span class="doc std std-doc">Spectral Clustering</span></a>: a matrix representing a graph with the degree of pairwise connections between all pairwise combinations of graph nodes, samples.</p>
<ul class="simple">
<li><p>values indicate the strength of the connection, unlike adjacency matrix with indicators, 0 if not connected, 1 if connected</p></li>
</ul>
<p>Note, node self connections are set to 0 in the adjacency matrix</p>
</section>
<section id="bagging-models">
<h2><strong>Bagging Models</strong><a class="headerlink" href="#bagging-models" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_ensemble_trees.html"><span class="doc std std-doc">Bagging Tree and Random Forest</span></a>: the application of bootstrap to obtain data realizations,</p>
<div class="math notranslate nohighlight">
\[
Y^b, X_1^b, \dots, X_m^b, \quad b = 1, \dots, B
\]</div>
<p>to train predictive model realizations,</p>
<p><span class="math notranslate nohighlight">\(\hat{Y}^b = \hat{f}^b (X_1^b, \dots, X_m^b)\)</span></p>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((X_1^b, \dots, X_m^b)\)</span> - the bootstrap predictor features in the <span class="math notranslate nohighlight">\(b^{th}\)</span> bootstrapped dataset</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{f}^b\)</span> - the <span class="math notranslate nohighlight">\(b^{th}\)</span> bootstrapped model</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{Y}^b\)</span> - predicted value for the model in the <span class="math notranslate nohighlight">\(b^{th}\)</span> bootstrapped model</p></li>
</ul>
<p>to calculate prediction realizations. The ensemble of prediction realizations are aggregated to reduce model variance. The aggregation includes,</p>
<ul class="simple">
<li><p><em>regression</em> - the average of the predictions
$<span class="math notranslate nohighlight">\(
\hat{Y} = \frac{1}{B} \sum_{b=1}^{B} \hat{Y}^b
\)</span>$</p></li>
<li><p><em>classification</em> - the mode of the predictions</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{Y} = \text{argmax}(\hat{Y}^b)
\]</div>
<p>We can perform bagging with any prediction model, in fact the BaggingClassifier and BaggingRegressor functions in scikit-learn are wrappers that take the prediction model as an input.</p>
</section>
<section id="basis-expansion">
<h2><strong>Basis Expansion</strong><a class="headerlink" href="#basis-expansion" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_polynomial_regression.html"><span class="doc std std-doc">Polynomial Regression</span></a>: to add flexibility to our model, for example, to capture non-linearity in our model for regression, classification, we expand the features with a set of basis functions</p>
<ul class="simple">
<li><p>in mathematics basis expansion is the approach of representing a more complicated function with a linear combination of simpler basis functions that make the problem easier to solve</p></li>
<li><p>with basis expansion we expand the dimensionality of the problem with basis functions of the original features, but still use linear methods on the transformed features.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
ℎ(𝑥_𝑖 )=\left( ℎ_1(𝑥_𝑖 ),ℎ_2(𝑥_𝑖 ),\ldots,ℎ_𝑘(𝑥_𝑖 ) \right)
\]</div>
<p>Here an example of basis expansion, the set of basis functions for polynomial basis expansion:</p>
<div class="math notranslate nohighlight">
\[ 
h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i^2, \quad h_{i,3}(x_i) = x_i^3, \quad h_{i,4}(x_i) = x_i^4, \dots, \quad h_{i,k}(x_i) = x_i^k 
\]</div>
</section>
<section id="basis-function">
<h2><strong>Basis Function</strong><a class="headerlink" href="#basis-function" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_polynomial_regression.html"><span class="doc std std-doc">Polynomial Regression</span></a>: to add flexibility to our model, for example, to capture non-linearity in our model for regression, classification, we expand the features with a set of basis functions</p>
<ul class="simple">
<li><p>in mathematics basis expansion is the approach of representing a more complicated function with a linear combination of simpler basis functions that make the problem easier to solve</p></li>
<li><p>with basis expansion we expand the dimensionality of the problem with basis functions of the original features, but still use linear methods on the transformed features.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
ℎ(𝑥_𝑖 )=\left( ℎ_1(𝑥_𝑖 ),ℎ_2(𝑥_𝑖 ),\ldots,ℎ_𝑘(𝑥_𝑖 ) \right)
\]</div>
<p>were each of <span class="math notranslate nohighlight">\(h_1\)</span>, \ldots, <span class="math notranslate nohighlight">\(h_k\)</span> are basis functions. For example, here are the basis functions for polynomial basis expansion:</p>
<div class="math notranslate nohighlight">
\[ 
h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i^2, \quad h_{i,3}(x_i) = x_i^3, \quad h_{i,4}(x_i) = x_i^4, \dots, \quad h_{i,k}(x_i) = x_i^k 
\]</div>
</section>
<section id="bayes-theorem-probability">
<h2><strong>Bayes’ Theorem</strong> (probability)<a class="headerlink" href="#bayes-theorem-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: the mathematical model central to Bayesian probability for the Bayesian updating from prior probability, with likelihood probability from new information to posterior probability.</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(P(A)\)</span> is the prior, <span class="math notranslate nohighlight">\(P(B|A)\)</span> is the likelihood, <span class="math notranslate nohighlight">\(P(B)\)</span> is the evidence term and <span class="math notranslate nohighlight">\(P(A|B)\)</span> is the posterior. If is convenient to substitute more descriptive labels for <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> to better conceptualize this approach,</p>
<div class="math notranslate nohighlight">
\[
P(\text{Model} | \text{New Data}) = \frac{P(\text{New Data} | \text{Model}) \cdot P(\text{Model})}{P(\text{New Data})}
\]</div>
<p>demonstrating that we are updating our model with new data</p>
</section>
<section id="bayesian-probability">
<h2><strong>Bayesian Probability</strong><a class="headerlink" href="#bayesian-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: probabilities based on a degree of belief (expert judgement and experience) in the likelihood of an event. The general approach,</p>
<ul class="simple">
<li><p>start with prior probability, prior to the collection of new information</p></li>
<li><p>formulate a likelihood probability, based on new information alone</p></li>
<li><p>update prior with likelihood to calculate the updated posterior probability</p></li>
<li><p>continue to update as new information is available</p></li>
<li><p>solve probability problems that we cannot use simple frequencies, i.e., <em>frequentist probability</em> approach</p></li>
<li><p>Bayesian updating is modeled with <em>Bayes’ Theorem</em></p></li>
</ul>
</section>
<section id="bayesian-updating-for-classification">
<h2><strong>Bayesian Updating for Classification</strong><a class="headerlink" href="#bayesian-updating-for-classification" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_naive_Bayes.html"><span class="doc std std-doc">Naive Bayes</span></a>: this is how we pose the classification prediction problem from the perspective of Bayesian updating, based on the conditional probability of a category, <span class="math notranslate nohighlight">\(k\)</span>, given <span class="math notranslate nohighlight">\(n\)</span> features, <span class="math notranslate nohighlight">\(x_1, \dots , x_n\)</span>.</p>
<div class="math notranslate nohighlight">
\[
P(C_k | x_1, \dots , x_n)
\]</div>
<p>we can solve for this posterior with Bayesian updating,</p>
<div class="math notranslate nohighlight">
\[
P(C_k | x_1, \dots , x_n) = \frac{P(x_1, \dots , x_n | C_k) P(C_k)}{P(x_1, \dots , x_n)}
\]</div>
<p>let’s combine the likelihood and prior for the moment,</p>
<div class="math notranslate nohighlight">
\[
P(x_1, \dots , x_n | C_k) P(C_k) = P(x_1, \dots , x_n, C_k)
\]</div>
<p>we can expand the full joint distribution recursively as follows,</p>
<div class="math notranslate nohighlight">
\[
P(x_1, \dots , x_n, C_k)
\]</div>
<p>expansion of the joint with the conditional and prior,</p>
<div class="math notranslate nohighlight">
\[
P(x_1 | x_2, \dots , x_n, C_k) P(x_2, \dots , x_n, C_k)
\]</div>
<p>continue recursively expanding,</p>
<div class="math notranslate nohighlight">
\[
P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3, \dots , x_n, C_k)
\]</div>
<p>we can generalize as,</p>
<div class="math notranslate nohighlight">
\[
P(C_k | x_1, \dots , x_n) = P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3 | x_4, \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k)  (x_{n} | C_k) P(C_k)
\]</div>
</section>
<section id="bayesian-linear-regression">
<h2><strong>Bayesian Linear Regression</strong><a class="headerlink" href="#bayesian-linear-regression" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html"><span class="doc std std-doc">Bayesian Linear Regression</span></a>: the frequentist formulation of the linear regression model is,</p>
<div class="math notranslate nohighlight">
\[
y = b_1 \times x + b_0 + \sigma
\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the predictor feature, <span class="math notranslate nohighlight">\(b_1\)</span> is the slope parameter, <span class="math notranslate nohighlight">\(b_0\)</span> is the intercept parameter and <span class="math notranslate nohighlight">\(\sigma\)</span> is the error or noise. There is an analytical form for the ordinary least squares solution to fit the available data while minimizing the <span class="math notranslate nohighlight">\(L^2\)</span> norm of the data error vector.</p>
<p>For the Bayesian formulation of linear regression is we pose the model as a prediction of the distribution of the response, <span class="math notranslate nohighlight">\(Y\)</span>, now a random variable:</p>
<div class="math notranslate nohighlight">
\[
Y \sim N(\beta^{T}X, \sigma^{2} I)
\]</div>
<p>We estimate the model parameter distributions through Bayesian updating for inferring the model parameters from a prior and likelihood from training data.</p>
<div class="math notranslate nohighlight">
\[
P(\beta | y, X) = \frac{P(y,X| \beta) P(\beta)}{P(y,X)}
\]</div>
<p>In general for continuous features we are not able to directly calculate the posterior and we must use a sampling method, such as Markov chain Monte Carlo (McMC) to sample the posterior.</p>
</section>
<section id="big-data">
<h2><strong>Big Data</strong><a class="headerlink" href="#big-data" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: you have big data if your data has a combination of these criteria:</p>
<ol class="arabic simple">
<li><p><em>Data Volume</em> - many data samples and features, difficult to store, transmit and visualize</p></li>
<li><p><em>Data Velocity</em> - high-rate collection, continuous data collection relative to decision making cycles, challenges keeping up with the new data while updating the models</p></li>
<li><p><em>Data Variety</em> - data form various sources, with various types of data, types of information, and scales</p></li>
<li><p><em>Data Variability</em> - data acquisition changes during the project, even for a single feature there may be multiple vintages of data with different scales, distributions, and veracity</p></li>
<li><p><em>Data Veracity</em> - data has various levels of accuracy, the data is not certain</p></li>
</ol>
<p>For common subsurface applications most, if not all, of these criteria are met. Subsurface engineering and geoscience are often working with big data!</p>
</section>
<section id="big-data-analytics">
<h2><strong>Big Data Analytics</strong><a class="headerlink" href="#big-data-analytics" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the process of examining large and varied data (<em>big data</em>) sets to discover patterns and make decisions, the application of statistics to big data.</p>
</section>
<section id="binary-transform-also-indicator-transform">
<h2><strong>Binary Transform</strong> (also Indicator Transform)<a class="headerlink" href="#binary-transform-also-indicator-transform" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: indicator coding a random variable to a probability relative to a category or a threshold.</p>
<p>If <span class="math notranslate nohighlight">\(i(\bf{u}:z_k)\)</span> is an indicator for a categorical variable,</p>
<ul class="simple">
<li><p>what is the probability of a realization equal to a category?</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
i(\bf{u}; z_k) =
\begin{cases} 
    1, &amp; \text{if } Z(\bf{u}) = z_k \\
    0, &amp; \text{if } Z(\bf{u}) \ne z_k 
\end{cases}
\end{split}\]</div>
<p>for example,</p>
<ul class="simple">
<li><p>given threshold, <span class="math notranslate nohighlight">\(z_2 = 2\)</span>, and data at <span class="math notranslate nohighlight">\(\bf{u}_1\)</span>, <span class="math notranslate nohighlight">\(z(\bf{u}_1) = 2\)</span>, then <span class="math notranslate nohighlight">\(i(bf{u}_1; z_2) = 1\)</span></p></li>
<li><p>given threshold, <span class="math notranslate nohighlight">\(z_1 = 1\)</span>, and a RV away from data, <span class="math notranslate nohighlight">\(Z(\bf{u}_2)\)</span> then is calculated as <span class="math notranslate nohighlight">\(F^{-1}_{\bf{u}_2}(z_1)\)</span> of the RV as <span class="math notranslate nohighlight">\(i(\bf{u}_2; z_1) = 0.23\)</span></p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(I\{\bf{u}:z_k\}\)</span> is an indicator for a continuous variable,</p>
<ul class="simple">
<li><p>what is the probability of a realization less than or equal to a threshold?</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
i(\bf{u}; z_k) =
\begin{cases} 
    1, &amp; \text{if } Z(\bf{u}) \le z_k \\
    0, &amp; \text{if } Z(\bf{u}) &gt; z_k 
\end{cases}
\end{split}\]</div>
<p>for example,</p>
<ul class="simple">
<li><p>given threshold, <span class="math notranslate nohighlight">\(z_1 = 6\%\)</span>, and data at <span class="math notranslate nohighlight">\(\bf{u}_1\)</span>, <span class="math notranslate nohighlight">\(z(\bf{u}_1) = 8\%\)</span>, then <span class="math notranslate nohighlight">\(i(\bf{u}_1; z_1) = 0\)</span></p></li>
<li><p>given threshold, <span class="math notranslate nohighlight">\(z_4 = 18\%\)</span>, and a RV away from data, <span class="math notranslate nohighlight">\(Z(\bf{u}_2) = N\left[\mu = 16\%,\sigma = 3\%\right]\)</span> then <span class="math notranslate nohighlight">\(i(\bf{u}_2; z_4) = 0.75\)</span></p></li>
</ul>
<p>The indicator coding may be applied over an entire random function by indicator transform of all the random variables at each location.</p>
</section>
<section id="boosting-models">
<h2><strong>Boosting Models</strong><a class="headerlink" href="#boosting-models" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_gradient_boosting.html"><span class="doc std std-doc">Gradient Boosting</span></a>: addition of multiple week learners to build a stronger learner.</p>
<ul class="simple">
<li><p>a weak learner is one that offers predictions just marginally better than random selection</p></li>
</ul>
<p>This is the method in words, and then with equations,</p>
<ul class="simple">
<li><p>build a simple model with a high error rate, the model can be quite inaccurate, but moves in the correct direction</p></li>
<li><p>calculate the error from the model</p></li>
<li><p>fit another model to the error</p></li>
<li><p>calculate the error from this addition of the first and second model</p></li>
<li><p>repeat until the desired accuracy is obtained or some other stopping criteria</p></li>
</ul>
<p>Now with equations, the general workflow for predicting <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> is,</p>
<ul class="simple">
<li><p>build a week learner to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, <span class="math notranslate nohighlight">\(\hat{F}_k(X)\)</span> from the training data <span class="math notranslate nohighlight">\(x_{i,j}\)</span>.</p></li>
<li><p>loop over number of desired estimators, <span class="math notranslate nohighlight">\(k = 1,\ldots,K\)</span></p></li>
</ul>
<ol class="arabic simple">
<li><p>calculate the residuals at the training data, <span class="math notranslate nohighlight">\(h_k(x_{i}) = y_i - \hat{F}_k(x_{i})\)</span></p></li>
<li><p>fit another week learner to predict <span class="math notranslate nohighlight">\(h_k\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, <span class="math notranslate nohighlight">\(\hat{F}_k(X)\)</span> from the training data <span class="math notranslate nohighlight">\(x_{i,j}\)</span>.</p></li>
</ol>
<ul class="simple">
<li><p>each model builds on the previous to improve the accuracy</p></li>
</ul>
<p>The regression estimator is the summation over the <span class="math notranslate nohighlight">\(K\)</span> simple models,</p>
<div class="math notranslate nohighlight">
\[
\hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m)
\]</div>
</section>
<section id="bootstrap">
<h2><strong>Bootstrap</strong><a class="headerlink" href="#bootstrap" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_ensemble_trees.html"><span class="doc std std-doc">Bagging Tree and Random Forest</span></a>: a statistical resampling procedure to calculate uncertainty in a calculated statistic from the sample data itself. Some general comments,</p>
<ul class="simple">
<li><p><em>sampling with replacement</em> - <span class="math notranslate nohighlight">\(n\)</span> (number of data samples) <em>Monte Carlo simulation</em>s from the dataset <em>cumulative distribution function</em>, this results in a new realization of the data</p></li>
<li><p><em>simulates the data collection process</em> - the fundamental idea is to simulate the original data collection process. Instead of actually collecting new sample sets, we randomly select from the data to get data realizations</p></li>
<li><p><em>bootstrap any statistic</em> - this approach is very flexible as we can calculate realizations of any statistics from the data realizations</p></li>
<li><p><em>computationally cheap</em> - repeat this approach to get realizations of the statistic to build a complete distribution of uncertainty. Use a large number of realizations, <span class="math notranslate nohighlight">\(L\)</span>, for a reliable uncertainty model.</p></li>
<li><p><em>calculates the entire distribution of uncertainty</em> - for any statistic, you calculate any summary statistic for the uncertainty model, e.g., mean, P10 and P90 of the uncertainty in the mean</p></li>
<li><p><em>bagging for machine learning</em> - is the application of bootstrap to obtain data realizations to train predictive model realizations to aggregate predictions over ensembles of prediction models to reduce model variance</p></li>
</ul>
<p>What are the limitations of bootstrap?</p>
<ul class="simple">
<li><p>biased sample data will likely result in a biased bootstrapped uncertainty model, you must first debias the samples, e.g., <em>declustering</em></p></li>
<li><p>you must have a sufficient sample size</p></li>
<li><p>integrates uncertainty due to sparse samples in space only</p></li>
<li><p>does not account for the spatial context of the data, i.e., sample data locations, volume of interest nor the spatial continuity. There is a variant of bootstrap called <a class="reference external" href="https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Spatial_Bootstrap.ipynb">spatial bootstrap</a>.</p></li>
</ul>
</section>
<section id="categorical-feature">
<h2><strong>Categorical Feature</strong><a class="headerlink" href="#categorical-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a feature that can only take one of a limited, and usually fixed, number of possible values</p>
</section>
<section id="categorical-nominal-feature">
<h2><strong>Categorical Nominal Feature</strong><a class="headerlink" href="#categorical-nominal-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a <em>categorical</em> feature without any natural ordering, for example,</p>
<ul class="simple">
<li><p>facies = {boundstone, wackystone, packstone, brecia}</p></li>
<li><p>minerals = {quartz, feldspar, calcite}</p></li>
</ul>
</section>
<section id="categorical-ordinal-feature">
<h2><strong>Categorical Ordinal Feature</strong><a class="headerlink" href="#categorical-ordinal-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a <em>categorical</em> feature with a natural ordering, for example,</p>
<ul class="simple">
<li><p>geologic age = {Miocene, Pliocene, Pleistocene} - ordered from older to younger rock</p></li>
<li><p>Mohs hardness = <span class="math notranslate nohighlight">\(\{1, 2, \ldots, 10\}\)</span> - ordered from softer to harder rock</p></li>
</ul>
</section>
<section id="causation">
<h2><strong>Causation</strong><a class="headerlink" href="#causation" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_multivariate_analysis.html"><span class="doc std std-doc">Multivariate Analysis</span></a>: a relationship where a change in one or more feature(s) directly leads to a change in one or more other feature(s).</p>
<p>Some important aspects of causal relationships,</p>
<ol class="arabic simple">
<li><p><em>Asymmetry and temporal precedence</em> - <span class="math notranslate nohighlight">\(A\)</span> is caused by <span class="math notranslate nohighlight">\(B\)</span> does not indicate that <span class="math notranslate nohighlight">\(B\)</span> is caused by <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p><em>Non-spurious</em> - not due to random effect or confounding features</p></li>
<li><p><em>Mechanism and explanation</em> - a plausible mechanism or process is available to explain the relationship</p></li>
<li><p><em>Consistency</em> - the relationship is observable over a range of conditions, times, locations, populations, etc.</p></li>
<li><p><em>Strength</em> - stronger relationships increase the likelihood of causation given all the previous 1-5 hold</p></li>
</ol>
<p>Establishing causation is very difficult,</p>
<ul class="simple">
<li><p>in this course we typically avoid causation and causal analysis, and emphasize this with statements such as correlation does not imply causation</p></li>
</ul>
</section>
<section id="cell-based-declustering">
<h2><strong>Cell-based Declustering</strong><a class="headerlink" href="#cell-based-declustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: a declustering method to assign weights to spatial samples based on local sampling density, such that the weighted statistics are likely more representative of the population. Data weights are assigned such that,</p>
<ul class="simple">
<li><p>samples in densely sampled areas receive less weight</p></li>
<li><p>samples in sparsely sampled areas receive more weight</p></li>
</ul>
<p>The goal of declustering is for the sample statistics to be independent of sample locations, e.g., infill drilling or blast hole samples should not change the statistics for the area of interest due to increased local sample density.</p>
<p>Cell-based declustering proceeds as follows:</p>
<ol class="arabic simple">
<li><p>a cell mesh is placed over the spatial data and weights are set as proportional to the inverse of the number of samples in the cell</p></li>
<li><p>the cell mesh size is varied, and the cell size that minimizes the declustered mean (in the sample mean is biased high) or maximizes the declustered mean (if the sample mean is biased low) is selected</p></li>
<li><p>to remove the impact of cell mesh position, the cell mesh is randomly moved several times and the resulting declustering weights are averaged for each datum</p></li>
</ol>
<p>The weights are calculated as:</p>
<div class="math notranslate nohighlight">
\[
w(\bf{u}_j) = \frac{1}{n_l} \cdot \frac{n}{L_o}
\]</div>
<p>where <span class="math notranslate nohighlight">\(n_l\)</span> is the number of data in the current cell, <span class="math notranslate nohighlight">\(L_o\)</span> is the number of cells with data, and <span class="math notranslate nohighlight">\(n\)</span> is the total number of data.</p>
<p>Here are some highlights for cell-based declustering,</p>
<ul class="simple">
<li><p>expert judgement to assign cell size based on the nominal sample spacing (e.g., data spacing before infill drilling) will improve the performance over the automated method for cell size selection based on minimum or maximum declustered mean (mentioned above)</p></li>
<li><p>cell-based declustering is not aware of the boundaries of the area of interest; therefore, data near the boundary of the area of interest may appear to be more sparsely sampled and receive more weight</p></li>
<li><p>cell-based was developed by Professor Andre Journel in 1983, <span id="id1">[]</span></p></li>
</ul>
</section>
<section id="cognitive-biases">
<h2><strong>Cognitive Biases</strong><a class="headerlink" href="#cognitive-biases" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: an automated (subconscious) thought process used by human brain to simplify information processing from large amount of personal experience and learned preferences. While these have been critical for our evolution and  survival on this planet, they can lead to the following issues in data science:</p>
<ol class="arabic simple">
<li><p><em>Anchoring Bias</em>, too much emphasis on the first piece of information. Studies have shown that the first piece of information could be irrelevant as we are beginning to learn about a topic, and often the earliest data in a project has the largest uncertainty. Address anchoring bias by curating all data, integrating uncertainty, fostering open discussion and debate on your project team.</p></li>
<li><p><em>Availability Heuristic</em>, overestimate importance of easily available information, for example, grandfather smoked 3 packs a day and lived to 100 years old, i.e., relying on anecdotes. Address availability heuristic by ensuring the project team documents all available information and applies quantitative analysis to move beyond anecdotes.</p></li>
<li><p><em>Bandwagon Effect</em>, assessed probability increases with the number of people holding the same belief. Watch out for everyone jumping on board or the loudest voice influencing all others on your project teams. Encouraging all members of the project team to contribute and even separate meetings may be helpful to address bandwagon effect.</p></li>
<li><p><em>Blind-spot Effect</em>, fail to see your own cognitive biases. This is the hardest cognitive bias of all. One possible solution is to invite arms length review of your project team’s methods, results and decisions.</p></li>
<li><p><em>Choice-supportive Bias</em>, probability increases after a commitment, i.e., a decision is made. For example, it was good that I bought that car supported by focusing on positive information about the car. This is a specific case of confirmation bias.</p></li>
<li><p><em>Clustering Illusion</em>, seeing patterns in random events. Yes, this heuristic helped us stay alive when large predictors hunted us, i.e., false positives are much better than false negatives! The solution is to model uncertainty confidence intervals and test all data and results against random effect.</p></li>
<li><p><em>Confirmation Bias</em>, only consider new information that supports current model. Choice-supportive bias is a specific case of confirmation bias. The solution to confirmation bias is to seek out people that you will likely disagree with and build skilled project teams that hold diverse technical opinions and have different expert experience. My approach is to get nervous if everyone in the room agrees with me!</p></li>
<li><p><em>Conservatism Bias</em>, favor old data to newly collected data. Data curation and quantitative analysis are helpful.</p></li>
<li><p><em>Recency Bias</em>, favor the most recently collected data. Ensure your team documents previous data and choices to enhance team memory. Just like conservative bias, data curation and quantitative analysis are our first line of defense.</p></li>
<li><p><em>Survivorship Bias</em>, focus on success cases only. Check for any possible pre-selection or filters on the data available to your team.</p></li>
</ol>
<p>Robust use of statistics / data analytics protects use from bias.</p>
</section>
<section id="complimentary-events-probability">
<h2><strong>Complimentary Events</strong> (probability)<a class="headerlink" href="#complimentary-events-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: the NOT operator for probability, if we define A then A compliment, <span class="math notranslate nohighlight">\(A^c\)</span>, is not A and we have this resulting closure relationship,</p>
<div class="math notranslate nohighlight">
\[
P(A) + P(A^c) = 1.0
\]</div>
<p>complimentary events may be considered for beyond univariate problems, for example consider this bivariate closure,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) + P(A^c|B) = 1.0
\]</div>
<p>Note, the given term must be the same.</p>
</section>
<section id="computational-complexity">
<h2><strong>Computational Complexity</strong><a class="headerlink" href="#computational-complexity" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_linear_regression.html"><span class="doc std std-doc">Linear Regression</span></a>: represents the computer resources for a method, we use it in machine learning to understand how our machine learning methods scale as we change the dimensionality, number of features, and the number of training data, represented by,</p>
<div class="math notranslate nohighlight">
\[
𝑂(𝑓(𝑛))
\]</div>
<p>where <span class="math notranslate nohighlight">\(𝑛\)</span> represents size of the problem. There are 2 components of computational complexity,</p>
<ul class="simple">
<li><p><em>time complexity</em> - refers to computational time and the scaling of this time to the size of the problem for a given algorithm</p></li>
<li><p><em>space complexity</em> - refers to computer memory required and the scaling of storage to the size of the problem for a given algorithm</p></li>
</ul>
<p>For example, if time complexity is <span class="math notranslate nohighlight">\(O(n^3)\)</span>, where is <span class="math notranslate nohighlight">\(n\)</span> is number of training data, then if we double the number of data the run time increases eight times.</p>
<p>Additional salient points about computational complexity,</p>
<ul class="simple">
<li><p><em>default to worst-case complexity</em> - the worst case for complexity given a specific problem size, provides an upper bound</p></li>
<li><p><em>asymptotic complexity</em> - where <span class="math notranslate nohighlight">\(𝑛\)</span> is large. Some algorithms have speed-up for small datasets, this is not used</p></li>
<li><p>assumes all steps are required, e.g., data is not presorted, etc.</p></li>
</ul>
<p>Time complexity examples,</p>
<ul class="simple">
<li><p><em>quadratic time</em>, <span class="math notranslate nohighlight">\(𝑶(𝒏^𝟐)\)</span> - for example, integer multiplication, bubble sort</p></li>
<li><p><em>linear time</em>, <span class="math notranslate nohighlight">\(𝑶(𝒏)\)</span> - for example, finding the min or max in an unsorted array</p></li>
<li><p><em>fractional power</em>, <span class="math notranslate nohighlight">\(𝑶(𝒏^𝒄 )\)</span> - where <span class="math notranslate nohighlight">\([0 &lt; c &lt; 1]\)</span>, for example, searching in a kd-tree, <span class="math notranslate nohighlight">\(𝑂(𝑛^(\frac{1}{2}))\)</span></p></li>
<li><p><em>exponential Time</em>, <span class="math notranslate nohighlight">\(𝑶(𝟐^𝒏)\)</span> - for example, traveling salesman problem with dynamic programing</p></li>
</ul>
</section>
<section id="conditional-probability">
<h2><strong>Conditional Probability</strong><a class="headerlink" href="#conditional-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: the probability of an event, given another event has occurred,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(A,B)}{P(A)}
\]</div>
<p>we read this as the probability of A given B has occurred as the joint divided by the marginal. We can extend conditional probabilities to any multivariate case by adding joints to either component. For example,</p>
<div class="math notranslate nohighlight">
\[
P(C|B,A) = \frac{P(A,B,C)}{P(B,C)}
\]</div>
</section>
<section id="confidence-interval">
<h2><strong>Confidence Interval</strong><a class="headerlink" href="#confidence-interval" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_linear_regression.html"><span class="doc std std-doc">Linear Regression</span></a>: the uncertainty in a summary statistic or model parameter represented as a range, lower and upper bound, based on a specified probability interval known as the confidence level.</p>
<p>We communicate confidence intervals like this,</p>
<ul class="simple">
<li><p>there is a 95% probability (or 19 times out of 20) that model slope is between 0.5 and 0.7.</p></li>
</ul>
<p>Other salient points about confidence intervals,</p>
<ul class="simple">
<li><p>calculated by analytical methods, when available, or with more general and flexible bootstrap</p></li>
<li><p>for Bayesian methods we refer credibility intervals</p></li>
</ul>
</section>
<section id="confusion-matrix">
<h2><strong>Confusion Matrix</strong><a class="headerlink" href="#confusion-matrix" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_naive_Bayes.html"><span class="doc std std-doc">Naive Bayes</span></a>: a matrix with frequencies of predicted (x axis) vs. actual (y axis) categories to visualize the performance of a classification model.</p>
<ul class="simple">
<li><p>visualize and diagnose all the combinations of correct and misclassification with the classification model, for example, category 1 is often misclassified as category 3.</p></li>
<li><p>perfect accuracy is number of each class on the diagonal, category 1 is always predicted as category 1, etc.</p></li>
<li><p>the classification matrix is applied to calculate a single summary of categorical accuracy, for example, precision, recall, etc.</p></li>
</ul>
</section>
<section id="continuous-feature">
<h2><strong>Continuous Feature</strong><a class="headerlink" href="#continuous-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a feature that can take any value between a lower and upper bound. For example,</p>
<ul class="simple">
<li><p>porosity = <span class="math notranslate nohighlight">\(\{13.01\%, 5.23\%, 24.62\%\}\)</span></p></li>
<li><p>gold grade = <span class="math notranslate nohighlight">\(\{4.56 \text{ g/t}, 8.72 \text{ g/t}, 12.45 \text{ g/t} \}\)</span></p></li>
</ul>
</section>
<section id="continuous-interval-feature">
<h2><strong>Continuous, Interval Feature</strong><a class="headerlink" href="#continuous-interval-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a <em>continuous feature</em> where the intervals between numbers are equal, for example, the difference between 1.50 and 2.50 is the same as the difference between 2.50 and 3.50, but the actual values do not have an objective, physical reality (exist on an arbitrary scale), i.e., do not have a true zero point, for example,</p>
<ul class="simple">
<li><p>Celsius scale of temperature (an arbitrary scale based on water freezing at 0 and boiling at 100)</p></li>
<li><p>calendar year (there is no true zero year)</p></li>
</ul>
<p>We can use addition and subtraction operations to compare continuous, interval features.</p>
</section>
<section id="continuous-ratio-feature">
<h2><strong>Continuous, Ratio Feature</strong><a class="headerlink" href="#continuous-ratio-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a <em>continuous feature</em> where the intervals between numbers are equal, for example, the difference between 1.50 and 2.50 is the same as the difference between 2.50 and 3.50, but the values do have an objective reality (measure an actual physical phenomenon), i.e., do have true zero point, for example,</p>
<ul class="simple">
<li><p>Kelvin scale of temperature</p></li>
<li><p>porosity</p></li>
<li><p>permeability</p></li>
<li><p>saturation</p></li>
</ul>
<p>Since there is a true zero, continuous, ratio features can be compared with multiplication and division mathematical operations (in addition to addition and subtraction), e.g., twice as much porosity.</p>
</section>
<section id="continuously-differentiable">
<h2><strong>Continuously Differentiable</strong><a class="headerlink" href="#continuously-differentiable" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_training_tuning.html"><span class="doc std std-doc">Machine Learning Training and Tuning</span></a>: a function is continuously differentiable if it satisfies two key conditions:</p>
<ol class="arabic simple">
<li><p>The function is differentiable, the derivative of the function exists at every point in its domain, i.e., the function has a well-defined slope at every possible point.</p></li>
<li><p>The derivative is continuous, the derivative of the function does not have any jumps, discontinuities, or abrupt changes, i.e, the derivative function itself is continuous at every point in its domain.</p></li>
</ol>
<p>For a machine learning example,</p>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(L^2\)</span> norm is continuously differentiable and as a result for linear and ridge regression we can apply partial derivatives to the loss function to calculate a closed form of training the model parameters</p></li>
<li><p>the <span class="math notranslate nohighlight">\(L^1\)</span> norm is not continuously differentiable and as a result for LASSO regression we cannot apply partial derivatives to the loss function to calculate a closed form of training the model parameters. We must use iterative optimization to train the model parameters.</p></li>
</ul>
</section>
<section id="convolution">
<h2><strong>Convolution</strong><a class="headerlink" href="#convolution" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_knearest_neighbours.html"><span class="doc std std-doc">k-Nearest Neighbours</span></a>: Integral product of two functions, after one is reversed and shifted by <span class="math notranslate nohighlight">\(\Delta\)</span>.</p>
<ul class="simple">
<li><p>one interpretation is smoothing a function with weighting function, <span class="math notranslate nohighlight">\(𝑓(\Delta)\)</span>, is applied to calculate the weighted average of function, <span class="math notranslate nohighlight">\(𝑔(x)\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
(f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta 
\]</div>
<p>this easily extends into multidimensional</p>
<div class="math notranslate nohighlight">
\[
(f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \, d\Delta_x \, d\Delta_y \, d\Delta_z
\]</div>
<p>The choice of which function is shifted before integration does not change the result, the convolution operator has commutativity.</p>
<div class="math notranslate nohighlight">
\[ 
(f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta 
\]</div>
<div class="math notranslate nohighlight">
\[
(f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta 
\]</div>
<ul class="simple">
<li><p>if either function is reflected then convolution is equivalent to cross-correlation, measure of similarity between 2 signals as a function of displacement.</p></li>
</ul>
</section>
<section id="core-data">
<h2><strong>Core Data</strong><a class="headerlink" href="#core-data" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the primary sampling method for direct measure for subsurface resources (recovered drill cuttings are also direct measures with greater uncertainty and smaller, irregular scale). Comments on core data,</p>
<ul class="simple">
<li><p>expensive / time consuming to collect for oil and gas, interrupt drilling operations, sparse and selective (very biased) coverage</p></li>
<li><p>very common in mining (diamond drill holes) for grade control with regular patterns and tight spacing</p></li>
<li><p>gravity, piston, etc. coring are used to sample sediments in lakes and oceans</p></li>
</ul>
<p>What do we learn from core data?</p>
<ul class="simple">
<li><p>petrological features (sedimentary structures, mineral grades), petrophysical features (porosity, permeability), and mechanical features (elastic modulas, Poisson’s ratio)</p></li>
<li><p>stratigraphy and ore body geometry through interpolation between wells and drill holes</p></li>
</ul>
<p>Core data are critical to support subsurface resource interpretations. They anchor the entire reservoir concept and  framework for prediction,</p>
<ul class="simple">
<li><p>for example, core data collocated with well log data are used to calibrate (ground truth) facies, porosity from well logs</p></li>
</ul>
</section>
<section id="correlation">
<h2><strong>Correlation</strong><a class="headerlink" href="#correlation" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_multivariate_analysis.html"><span class="doc std std-doc">Multivariate Analysis</span></a>: the Pearson’s product-moment correlation coefficient is a measure of the degree of linear relationship,</p>
<div class="math notranslate nohighlight">
\[
\rho_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x \sigma_y}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{x}\)</span> and <span class="math notranslate nohighlight">\(\overline{y}\)</span> are the means of features <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The measure is bound <span class="math notranslate nohighlight">\(\[-1,1\]\)</span>.</p>
<ul class="simple">
<li><p>correlation coefficient is a standardized covariance</p></li>
</ul>
<p>The Person’s correlation coefficient is quite sensitive to outliers and departure from linear behavior (in the bivariate sense).  We have an alternative known as the Spearman’s rank correlations coefficient,</p>
<div class="math notranslate nohighlight">
\[
\rho_{R_x R_y}  = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0
\]</div>
<p>The rank correlation applies the rank transform to the data prior to calculating the correlation coefficient. To calculate the rank transform simply replace the data values with the rank <span class="math notranslate nohighlight">\(R_x = 1,\dots,n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the maximum value and <span class="math notranslate nohighlight">\(1\)</span> is the minimum value.</p>
<div class="math notranslate nohighlight">
\[
x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j 
\]</div>
<div class="math notranslate nohighlight">
\[
R_{x_i} = i
\]</div>
</section>
<section id="covariance">
<h2><strong>Covariance</strong><a class="headerlink" href="#covariance" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_multivariate_analysis.html"><span class="doc std std-doc">Multivariate Analysis</span></a>: a measure of how two features vary together,</p>
<div class="math notranslate nohighlight">
\[
C_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{x}\)</span> and <span class="math notranslate nohighlight">\(\overline{y}\)</span> are the means of features <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The measure is bound <span class="math notranslate nohighlight">\(\[-\sigma_x \cdot \sigm_y,\sigma_x \cdot \sigm_y\]\)</span>.</p>
<ul class="simple">
<li><p>correlation coefficient is a standardized covariance</p></li>
</ul>
<p>The Person’s correlation coefficient is quite sensitive to outliers and departure from linear behavior (in the bivariate sense).  We have an alternative known as the Spearman’s rank correlations coefficient,</p>
<div class="math notranslate nohighlight">
\[
\rho_{R_x R_y}  = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i} - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le 1.0
\]</div>
<p>The rank correlation applies the rank transform to the data prior to calculating the correlation coefficient.  To calculate the rank transform simply replace the data values with the rank <span class="math notranslate nohighlight">\(R_x = 1,\dots,n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the maximum value and <span class="math notranslate nohighlight">\(1\)</span> is the minimum value.</p>
<div class="math notranslate nohighlight">
\[
x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall \, i \gt j 
\]</div>
<div class="math notranslate nohighlight">
\[
R_{x_i} = i
\]</div>
</section>
<section id="cross-validation">
<h2><strong>Cross Validation</strong><a class="headerlink" href="#cross-validation" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: withholding a portion of the data from the model parameter training to test the ability of the model to predict for cases not used to train the model</p>
<ul class="simple">
<li><p>this is typically conducted by a train and test data split, with 15% - 30% of data assigned to testing</p></li>
<li><p>a dress rehearsal for real-world model use, the train-test split must be fair, resulting in similar prediction difficulty to the planned use of the model</p></li>
<li><p>there are more complicated designs such as k-fold cross validation that allows testing over all data via k-folds each with trained model</p></li>
<li><p>cross validation may be applied to check model performance for estimation accuracy (most common) and uncertainty model goodness (<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0920410521006343">Maldonado-Cruz and Pyrcz, 2021</a>)</p></li>
</ul>
</section>
<section id="cumulative-distribution-function-cdf">
<h2><strong>Cumulative Distribution Function</strong> (CDF)<a class="headerlink" href="#cumulative-distribution-function-cdf" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_univariate_analysis.html"><span class="doc std std-doc">Univariate Analysis</span></a>: the sum of a discrete PDF or the integral of a continuous PDF. Here are the important concepts,</p>
<ul class="simple">
<li><p>the CDF is stated as <span class="math notranslate nohighlight">\(F_x(x)\)</span>, note the PDF is stated as <span class="math notranslate nohighlight">\(f_x(x)\)</span></p></li>
<li><p>is the probability that a random sample, <span class="math notranslate nohighlight">\(X\)</span>, is less than or equal to a specific value <span class="math notranslate nohighlight">\(x\)</span>; therefore, the y axis is cumulative probability</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
F_x(x) = P(X \le x) = \int_{-infty}^x f(u) du 
\]</div>
<ul class="simple">
<li><p>for CDFs there is no bin assumption; therefore, bins are at the resolution of the data.</p></li>
<li><p>monotonically non-decreasing function, because a negative slope would indicate negative probability over an interval.</p></li>
</ul>
<p>The requirements for a valid CDF include,</p>
<ol class="arabic simple">
<li><p>non-negativity constraint:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
F_x(x) = P(X \le x) \ge 0.0, \quad \forall x
\]</div>
<ol class="arabic simple" start="2">
<li><p>valid probability:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
0.0 \le F_x(x) \le 1.0, \quad \forall x
\]</div>
<ol class="arabic simple" start="3">
<li><p>cannot have negative slope:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{dF_x(x)}{dx} \ge 0.0, \quad \forall x
\]</div>
<ol class="arabic simple" start="4">
<li><p>minimum and maximum (ensuring probability closure) values:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\text{min}(F_x(x)) = 0.0 \quad \text{max}(F_x(x)) = 1.0
\]</div>
</section>
<section id="curse-of-dimensionality">
<h2><strong>Curse of Dimensionality</strong><a class="headerlink" href="#curse-of-dimensionality" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: the suite of challenges associated with working with many features, i.e., high dimensional space, including,</p>
<ul class="simple">
<li><p>impossible to visualize data and model in high dimensionality space</p></li>
<li><p>usually insufficient sampling for statistical inference in vast high dimensional space</p></li>
<li><p>low coverage of high dimensional predictor feature space</p></li>
<li><p>distorted feature space, including warped space dominated by corners and distances lose sensitivity</p></li>
<li><p>multicollinearity between features is more likely as the dimensionality increases</p></li>
</ul>
</section>
<section id="data-data-aspects">
<h2><strong>Data</strong> (data aspects)<a class="headerlink" href="#data-data-aspects" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: when describing spatial dataset these are the fundamental aspects,</p>
<p><em>Data coverage</em> - what proportion of the population has been sampled for this?</p>
<p>In general, hard data has high resolution (small scale, volume support), but with poor data coverage (measure only an extremely small proportion of the population, for example,</p>
<ul class="simple">
<li><p><em>Core coverage deepwater oil and gas</em> - well core only sample one five hundred millionth to one five billionth of a deepwater reservoir, assuming 3 inch diameter cores with 10% core coverage in vertical wells with 500 m to 1,500 m spacing</p></li>
<li><p><em>Core coverage mining grade control</em> - diamond drill hole cores sample one eight thousandth to one thirty thousandth of ore body, assuming HQ 63.5 mm diameter cores with 100% core coverage in vertical drill holes with 5 m to 10 m spacing</p></li>
</ul>
<p>Soft data tend to have excellent (often complete) coverage, but with low resolution,</p>
<ul class="simple">
<li><p><em>Seismic reflection surveys and gradiometric surveys</em> - data is generally available over the entire volume of interest, but resolution is low and generally decreasing with depth</p></li>
</ul>
<p><em>Data Scale</em> (support size) - What is the scale or volume sampled by the individual samples? For example,</p>
<ul class="simple">
<li><p>core tomography images of core samples at the pore scale, 1 - 50 <span class="math notranslate nohighlight">\(\mu m\)</span></p></li>
<li><p>gamma ray well log sampled at 0.3 m intervals with 1 m penetration away from the bore hole</p></li>
<li><p>ground-based gravity gradiometry map with 20 m x 20 m x 100 m resolution</p></li>
</ul>
<p><em>Data Information Type</em> - What does the data tell us about the subsurface? For example,</p>
<ul class="simple">
<li><p>grain size distribution that may be applied to calibrate permeability and saturations</p></li>
<li><p>fluid type to assess the location of the oil water contact</p></li>
<li><p>dip and continuity of important reservoir layers to access connectivity</p></li>
<li><p>mineral grade to map high, mid and low grade ore shells for mine planning</p></li>
</ul>
</section>
<section id="data-convexity">
<h2><strong>Data Convexity</strong><a class="headerlink" href="#data-convexity" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_density-based_clustering.html"><span class="doc std std-doc">Density-based Clustering</span></a>: a subset, <span class="math notranslate nohighlight">\(A\)</span>, of Euclidean feature space is convex if, for any two points <span class="math notranslate nohighlight">\(𝑥_1\)</span> and <span class="math notranslate nohighlight">\(𝑥_2\)</span> within <span class="math notranslate nohighlight">\(𝐴\)</span>, the entire line segment connecting these points is within <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\left[𝑥_1,𝑥_2\right] \in A\)</span>.</p>
</section>
<section id="dataframe">
<h2><strong>DataFrame</strong><a class="headerlink" href="#dataframe" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Machine Learning Workflow Construction and Coding</span>: a convenient Pandas class for working with data tables with rows for each sample and columns for each feature, due to,</p>
<ul class="simple">
<li><p>convenient data structure to store, access, manipulate tabular data</p></li>
<li><p>built-in methods to load data from a variety of file types, Python classes and even directly from Excel</p></li>
<li><p>built-in methods to calculate summary statistics and visualize data</p></li>
<li><p>built-in methods for data queries, sort, data filters</p></li>
<li><p>built-in methods  for data manipulation, cleaning, reformatting</p></li>
<li><p>built-in attributes to store information about the data, e.g. size, number nulls and null value</p></li>
</ul>
</section>
<section id="data-analytics">
<h2><strong>Data Analytics</strong><a class="headerlink" href="#data-analytics" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the use of statistics with visualization to support decision making.</p>
<ul class="simple">
<li><p>Dr. Pyrcz says that data analytics is the same as statistics.</p></li>
</ul>
</section>
<section id="data-preparation">
<h2><strong>Data Preparation</strong><a class="headerlink" href="#data-preparation" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: any workflow steps to enhance, improve raw data to be model ready.</p>
<ul class="simple">
<li><p>data-driven science needs data, data preparation remains essential</p></li>
<li><p><span class="math notranslate nohighlight">\(\gt &gt;80\%\)</span> of any subsurface study is data preparation and interpretation</p></li>
</ul>
<p>We continue to face a challenge with data:</p>
<ul class="simple">
<li><p>data curation - format standards, version control, storage, transmission, security and documentation</p></li>
<li><p>large volume to manage - visualization, availability and data mining and exploration</p></li>
<li><p>large volumes of metadata - lack of platforms, standards and formats</p></li>
<li><p>engineering integration, variety of data, scale, interpretation and uncertainty</p></li>
</ul>
<p>Clean databases are prerequisite to all data analytics and machine learning</p>
<ul class="simple">
<li><p>must start with this foundation</p></li>
<li><p>garbage in, garbage out</p></li>
</ul>
</section>
<section id="degree-matrix-spectral-clustering">
<h2><strong>Degree Matrix</strong> (spectral clustering)<a class="headerlink" href="#degree-matrix-spectral-clustering" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_spectral_clustering.html"><span class="doc std std-doc">Spectral Clustering</span></a>: a matrix representing a graph with the number of connections for each graph nodes, samples.</p>
<ul class="simple">
<li><p>diagonal matrix with integer for number of connections</p></li>
</ul>
</section>
<section id="dbscan-for-density-based-clustering">
<h2><strong>DBSCAN for Density-based Clustering</strong><a class="headerlink" href="#dbscan-for-density-based-clustering" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_density-based_clustering.html"><span class="doc std std-doc">Density-based Clustering</span></a>: an density-based clustering algorithm, groups are seeded or grown in feature space at locations with sufficient point density determined by hyperparameters,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> – the radius of the local neighbourhood in the metric of normalized features. The is the scale / resolution of the clusters. If this values is set too small, too many samples are left as outliers and if set too large, all the clusters merge to one single cluster.</p></li>
<li><p><span class="math notranslate nohighlight">\(min_{Pts}\)</span> – the minimum number of points to assign a core point, where core points are applied to initialize or grow a cluster group.</p></li>
</ul>
<p>Density is quantified by number of samples over a volume, where the volume is based on a radius over all dimensions of feature space.</p>
<p>Automated or guided <span class="math notranslate nohighlight">\(\epsilon\)</span> parameter estimation is available by k-distance graph (in this case is k nearest neighbor).</p>
<ol class="arabic simple">
<li><p>Calculate the nearest neighbor distance in normalized feature space for all the sample data (1,700 in this case).</p></li>
<li><p>Sort in ascending order and plot.</p></li>
<li><p>Select the distance that maximizes the positive curvature (the elbow).</p></li>
</ol>
<p>Here is a summary of salient aspects for DBSCAN clustering,</p>
<ul class="simple">
<li><p><em>DBSCAN</em> - stands for Density-Based Spatial Clustering of Applications with Noise (Ester et al.,1996).</p></li>
<li><p><em>Advantages</em> - include minimum domain knowledge to estimate hyperparameters, the ability to represent any arbitrary shape of cluster groups and efficient to apply for large data sets</p></li>
<li><p><em>Hierarchical Bottom-up / Agglomerative Clustering</em> – all data samples start as their own group, called ‘unvisited’ but practically as outliers until assigned to a group, and then the cluster group grow iteratively.</p></li>
<li><p><em>Mutually Exclusive</em> – like k-means clustering, all samples may only belong to a single cluster group.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(C_i \cap C_j | i \ne j) = 0.0
\]</div>
<ul class="simple">
<li><p><em>Non-exhaustive</em> – some samples may be left as unassigned and assumed as outliers for the cluster group assignment</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(C_1 \cup C_2 \cup \dots C_k) \le 1.0 
\]</div>
</section>
<section id="decision-criteria">
<h2><strong>Decision Criteria</strong><a class="headerlink" href="#decision-criteria" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a feature that is calculated by applying the transfer function to the subsurface model(s) to support decision making. The decision criteria represents value, health, environment and safety. For example:</p>
<ul class="simple">
<li><p>contaminant recovery rate to support design of a pump and treat soil remediation project</p></li>
<li><p>oil-in-place resources to determine if a reservoir should be developed</p></li>
<li><p>Lorenz coefficient heterogeneity measure to classify a reservoir and determine mature analogs</p></li>
<li><p>recovery factor or production rate to schedule production and determine optimum facilities</p></li>
<li><p>recovered mineral grade and tonnage to determine economic ultimate pit shell</p></li>
</ul>
</section>
<section id="decision-tree">
<h2><strong>Decision Tree</strong><a class="headerlink" href="#decision-tree" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_decision_tree.html"><span class="doc std std-doc">Decision Tree</span></a>: a intuitive, regression and classification predictive machine learning model that devides the predictor space, <span class="math notranslate nohighlight">\(𝑋_1,…,𝑋_𝑚\)</span>, into <span class="math notranslate nohighlight">\(𝐽\)</span> mutually exclusive, exhaustive regions, <span class="math notranslate nohighlight">\(𝑅_𝑗\)</span>.</p>
<ul class="simple">
<li><p><em>mutually exclusive</em> – any combination of predictors only belongs to a single region, <span class="math notranslate nohighlight">\(𝑅_𝑗\)</span></p></li>
<li><p><em>exhaustive</em> – all combinations of predictors belong a region, <span class="math notranslate nohighlight">\(𝑅_𝑗\)</span>, regions cover entire feature space, range of the variables being considered</p></li>
</ul>
<p>The same prediction in each region, mean of training data in region, <span class="math notranslate nohighlight">\(\hat{Y}(𝑅_𝑗) = \overline{Y}(𝑅_𝑗)\)</span></p>
<ul class="simple">
<li><p>for classification the most common, mode-based or argmax operator</p></li>
</ul>
<p>Other salient points about decision tree,</p>
<ul class="simple">
<li><p><em>supervised Learning</em> - the response feature label, <span class="math notranslate nohighlight">\(Y\)</span>, is available over the training and testing data</p></li>
<li><p><em>hierarchical, binary segmentation</em> - of the predictor feature space, start with 1 region and sequentially divide, creating new regions</p></li>
<li><p><em>compact, interpretable model</em> - since the classification is based on a hierarchy of binary segmentations of the feature space (one feature at a time) the model can be specified in a intuitive manner as a tree with binary branches**, hence the name decision tree. The code for the model is nested if statements, for example,</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">porosity</span> <span class="o">&gt;</span> <span class="mf">0.15</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">brittleness</span> <span class="o">&lt;</span> <span class="mi">20</span><span class="p">:</span>
        <span class="n">initial_production</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">initial_production</span> <span class="o">=</span> <span class="mi">7000</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">brittleness</span> <span class="o">&lt;</span> <span class="mi">40</span><span class="p">:</span>
        <span class="n">initial_production</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">initial_production</span> <span class="o">=</span> <span class="mi">3000</span>
</pre></div>
</div>
<p>The decision tree is constructed from the top down. We begin with a single region that covers the entire feature space and then proceed with a sequence of splits,</p>
<ul class="simple">
<li><p><em>scan all possible splits</em> - over all regions and over all features.</p></li>
<li><p><em>greedy optimization</em> - proceeds by finding the best split in any feature that minimizes the residual sum of squares of errors over all the training data <span class="math notranslate nohighlight">\(y_i\)</span> over all of the regions <span class="math notranslate nohighlight">\(j = 1,\ldots,J\)</span>. There is no other information shared between subsequent splits.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\]</div>
<p>Hyperparameters include,</p>
<ul class="simple">
<li><p><em>number of regions</em> – very easy to understand, you know what the model will be</p></li>
<li><p><em>minimum reduction in RSS</em> – could stop early, e.g., a low reduction in RSS split could lead to a subsequent split with a larger reduction in RSS</p></li>
<li><p><em>minimum number of training data in each region</em> – related to the concept of accuracy of the region mean prediction, i.e., we need at least 𝑛 data for a reliable mean</p></li>
<li><p><em>maximum number of levels</em> – forces symmetric trees, similar number of splits to get to each region</p></li>
</ul>
</section>
<section id="declustering">
<h2><strong>Declustering</strong><a class="headerlink" href="#declustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: various methods that assign weights to spatial samples based on local sampling density, such that the weighted statistics are likely more representative of the population. Data weights are assigned so that,</p>
<ul class="simple">
<li><p>samples in densely sampled areas receive less weight</p></li>
<li><p>samples in sparsely sampled areas receive more weight</p></li>
</ul>
<p>There are various declustering methods:</p>
<ul class="simple">
<li><p><em>cell-based declustering</em></p></li>
<li><p><em>polygonal declustering</em></p></li>
<li><p><em>kriging-based declustering</em></p></li>
</ul>
<p>It is important to note that no declustering method can prove that for every data set the resulting weighted statistics will improve the prediction of the population parameters, but in expectation these methods tend to reduce the bias.</p>
</section>
<section id="declustering-statistics">
<h2><strong>Declustering</strong> (statistics)<a class="headerlink" href="#declustering-statistics" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: once declustering weights are calculated for a spatial dataset, then declustered statistics are applied as input for only subsequent analysis or modeling. For example,</p>
<ul class="simple">
<li><p>the declustered mean is assigned as the stationary, global mean for simple kriging</p></li>
<li><p>the weighted CDF from all the data with weights are applied to sequential Gaussian simulation to ensure the back-transformed realizations approach the declustered distribution</p></li>
</ul>
<p>Any statistic can be weighted, including the entire CDF! Here are some examples of weighted statistics, given declustering weights, <span class="math notranslate nohighlight">\(w(\bf{u}_j)\)</span>, for all data <span class="math notranslate nohighlight">\(j=1,\ldots,n\)</span>.</p>
<ul class="simple">
<li><p>weighted sample mean,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\overline{x}_{wt} = \frac{\sum_{i=1}^n w(\bf{u}_j) \cdot z(\bf{u}_j)}{\sum_{i=1}^n w(\bf{u}_j)} 
\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the number of data.</p>
<ul class="simple">
<li><p>weighted sample variance,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
s^2_{x_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) - 1} \cdot \sum_{i=1}^n w(\bf{u}_j) \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right)^2 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{x}_{wt}\)</span> is the declustered mean.</p>
<ul class="simple">
<li><p>weighted covariance,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
C_{x,y_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) } \cdot \sum_{i=1}^n w(\bf{u}_j) \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right) \cdot \left( y(\bf{u}_j) - \overline{y}_{wt} \right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{x}_{wt}\)</span> and <span class="math notranslate nohighlight">\(\overline{y}_{wt}\)</span> are the declustered means for features <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<ul class="simple">
<li><p>the entire CDF,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
F_z(z) \approx \sum_{j=1}^{n(Z&lt;z)} w(\bf{u}_j)
\]</div>
<p>where <span class="math notranslate nohighlight">\(n(Z&lt;z)\)</span> is the number of sorted ascending data less than threshold <span class="math notranslate nohighlight">\(z\)</span>. We show this as approximative as this is simplified and at data resolution and without an interpolation model.</p>
<p>It is important to note that no declustering method can prove that for every data set the resulting weighted statistics will improve the prediction of the population parameters, but in expectation these methods tend to reduce the bias.</p>
</section>
<section id="density-connected-dbscan">
<h2><strong>Density-Connected</strong> (DBSCAN)<a class="headerlink" href="#density-connected-dbscan" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_density-based_clustering.html"><span class="doc std std-doc">Density-based Clustering</span></a>: points <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are density-connected if there is a point <span class="math notranslate nohighlight">\(Z\)</span> that is density-reachable from both points <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
</section>
<section id="density-based-cluster-dbscan">
<h2><strong>Density-based Cluster</strong> (DBSCAN)<a class="headerlink" href="#density-based-cluster-dbscan" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_density-based_clustering.html"><span class="doc std std-doc">Density-based Clustering</span></a>: a nonempty set where all points are density-connected to each other.</p>
</section>
<section id="density-reachable-dbscan">
<h2><strong>Density-Reachable</strong> (DBSCAN)<a class="headerlink" href="#density-reachable-dbscan" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_density-based_clustering.html"><span class="doc std std-doc">Density-based Clustering</span></a>: point <span class="math notranslate nohighlight">\(Y\)</span> is density reachable from <span class="math notranslate nohighlight">\(A\)</span> if <span class="math notranslate nohighlight">\(Y\)</span> belongs to a neighborhood of a core point that can reached from <span class="math notranslate nohighlight">\(A\)</span>. This would require a chain of core points each belonging the previous core points and the last core point including point Y.</p>
</section>
<section id="deterministic-model">
<h2><strong>Deterministic Model</strong><a class="headerlink" href="#deterministic-model" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a model that assumes the system or process that is completely predictable</p>
<ul class="simple">
<li><p>often-based on engineering and geoscience physics and expert judgement</p></li>
<li><p>for example, numerical flow simulation or stratigraphic bounding surfaces interpreted from seismic</p></li>
<li><p>for this course we also state that data-driven estimation models like</p></li>
</ul>
<p>Advantages:</p>
<ul class="simple">
<li><p>integration of physics and expert knowledge</p></li>
<li><p>integration of various information sources</p></li>
</ul>
<p>Disadvantages:</p>
<ul class="simple">
<li><p>often quite time consuming</p></li>
<li><p>often no assessment of uncertainty, focus on building one model</p></li>
</ul>
</section>
<section id="dimensionality-reduction">
<h2><strong>Dimensionality Reduction</strong><a class="headerlink" href="#dimensionality-reduction" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_PCA.html"><span class="doc std std-doc">Principal Component Analysis</span></a>: methods to reduce the number of features within a data science workflow. There are 2 primary methods,</p>
<ul class="simple">
<li><p><em>features Selection</em> – find the subset of original features that are most important for the problem</p></li>
<li><p><em>feature projection</em> – transform the data from a higher to lower dimensional space</p></li>
</ul>
<p>Known as dimension reduction or dimensionality reduction</p>
<ul class="simple">
<li><p>motivated by the curse of dimensionality and multicollinearity</p></li>
<li><p>applied in statistics, machine learning and information theory</p></li>
</ul>
</section>
<section id="directly-density-reachable-dbscan">
<h2><strong>Directly Density Reachable</strong> (DBSCAN)<a class="headerlink" href="#directly-density-reachable-dbscan" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_density-based_clustering.html"><span class="doc std std-doc">Density-based Clustering</span></a>: point <span class="math notranslate nohighlight">\(X\)</span> is directly density reachable from <span class="math notranslate nohighlight">\(A\)</span>, if <span class="math notranslate nohighlight">\(A\)</span> is a core point and <span class="math notranslate nohighlight">\(X\)</span> belongs to the neighborhood, distance <span class="math notranslate nohighlight">\(le \epsilon\)</span> from <span class="math notranslate nohighlight">\(A\)</span>.</p>
</section>
<section id="discrete-feature">
<h2><strong>Discrete Feature</strong><a class="headerlink" href="#discrete-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a <em>categorical feature</em> or a <em>continuous feature</em> that is binned or grouped, for example,</p>
<ul class="simple">
<li><p>porosity between 0 and 20% assigned to 10 bins = {0 - 2%, 2% - 4%, \ldots ,20%}</p></li>
<li><p>Mohs hardness = <span class="math notranslate nohighlight">\(\{1, 2, \ldots, 10\}\)</span> (same at <em>categorical feature</em>)</p></li>
</ul>
</section>
<section id="distribution-transformations">
<h2><strong>Distribution Transformations</strong><a class="headerlink" href="#distribution-transformations" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: a mapping from one distribution to another distribution through percentile values, resulting in a new histogram, PDF, and CDF. We perform distribution transformations in geostatistical methods and workflows because,</p>
<ul class="simple">
<li><p><em>inference</em> - to correct a feature distribution to an expected shape, for example, correcting for too few or biased data</p></li>
<li><p><em>theory</em> - a specific distribution assumption is required for a workflow step, for example, Gaussian distribution with mean of 0.0 and variance of 1.0 is required for sequential Gaussian simulation</p></li>
<li><p><em>data preparation or cleaning</em> - to correct for outliers, the transformation will map the outlier into the target distribution no longer as an outlier</p></li>
</ul>
<p>How do we perform distribution transformations?</p>
<p>We transform the values from the cumulative distribution function (CDF), <span class="math notranslate nohighlight">\(F_{X}\)</span>, to a new CDF , <span class="math notranslate nohighlight">\(G_{Y}\)</span>. This can be generalized with the quantile - quantile transformation applied to all the sample data:</p>
<ul class="simple">
<li><p>The forward transform:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Y = G_{Y}^{-1}(F_{X}(X))
\]</div>
<ul class="simple">
<li><p>The reverse transform:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
X = F_{X}^{-1}(G_{Y}(Y))
\]</div>
<p>This may be applied to any data, including parametric or nonparametric distributions. We just need to be able to map from one distribution to another through percentiles, so it is a:</p>
<ul class="simple">
<li><p>rank preserving transform, for example, P25 remains P25 after distribution transformation</p></li>
</ul>
</section>
<section id="eager-learning">
<h2><strong>Eager Learning</strong><a class="headerlink" href="#eager-learning" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_knearest_neighbours.html"><span class="doc std std-doc">k-Nearest Neighbours</span></a>: Model is a generalization of the training data constructed prior to queries</p>
<ul class="simple">
<li><p>the model is input-independent after parameter training and hyperparameter tuning, i.e., the training data does not need to be available to make new predictions</p></li>
</ul>
<p>The opposite is lazy learning.</p>
</section>
<section id="estimation">
<h2><strong>Estimation</strong><a class="headerlink" href="#estimation" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: is process of obtaining the single best value to represent a feature at an unsampled location, or time. Some additional concepts,</p>
<ul class="simple">
<li><p>local accuracy takes precedence over global spatial variability</p></li>
<li><p>too smooth, not appropriate for any transform function that is sensitive to heterogeneity</p></li>
<li><p>for example, inverse distance and kriging</p></li>
<li><p>many predictive machine learning models focus on estimation (e.g., k-nearest neighbours, decision tree, random forest, etc.)</p></li>
</ul>
</section>
<section id="f1-score-classification-accuracy-metric">
<h2><strong>f1-score</strong> (classification accuracy metric)<a class="headerlink" href="#f1-score-classification-accuracy-metric" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_naive_Bayes.html"><span class="doc std std-doc">Naive Bayes</span></a>: a categorical classification prediction model measure of accuracy, a single summary metric for each <span class="math notranslate nohighlight">\(k\)</span> category from the confusion matrix.</p>
<ul class="simple">
<li><p>the harmonic mean of recall and precision</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f1-score_k = \frac{2} { \frac{1}{Precision_k} + \frac{1}{Recall_k} }
\]</div>
<p>As a reminder,</p>
<ul class="simple">
<li><p><em>recall</em> - the ratio of true positives divided by all cases of the category in the testing dataset</p></li>
<li><p><em>precision</em> - the ratio of true positives divided by all positives, true positives + false positives</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Recall_k = \frac{ n_{k, \text{true  positives}} }{n_k}
\]</div>
</section>
<section id="feature-also-variable">
<h2><strong>Feature</strong> (also variable)<a class="headerlink" href="#feature-also-variable" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: any property measured or observed in a study</p>
<ul class="simple">
<li><p>for example, porosity, permeability, mineral concentrations, saturations, contaminant concentration, etc.</p></li>
<li><p>in data mining / machine learning this is known as a feature, statisticians call these variables</p></li>
<li><p>measure often requires significant analysis, interpretation, etc.</p></li>
<li><p>when features are modified and combined to improve our models we call this feature engineering</p></li>
</ul>
</section>
<section id="feature-engineering">
<h2><strong>Feature Engineering</strong><a class="headerlink" href="#feature-engineering" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: using domain expertise to extract improved predictor or response features from raw data,</p>
<ul class="simple">
<li><p>improve the performance, accuracy and convergency, of inferential or predictive machine learning</p></li>
<li><p>improve model interpretability (or may worsen interpretability if our engineered features are in unfamiliar units)</p></li>
<li><p>mitigate outliers &amp; bias, consistency with assumptions such as Gaussianity, linearization, dimensional expansion</p></li>
</ul>
<p>Feature transformation and feature selection are two forms of feature engineering.</p>
</section>
<section id="feature-importance">
<h2><strong>Feature Importance</strong><a class="headerlink" href="#feature-importance" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: a variety of machine learning methods to provide measures for feature ranking, for example decision trees summarize the reduction in mean square error through inclusion of each feature and is summarized as,</p>
<div class="math notranslate nohighlight">
\[
FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t}
\]</div>
<p>where <span class="math notranslate nohighlight">\(T_f\)</span> are all nodes with feature <span class="math notranslate nohighlight">\(x\)</span> as the split, <span class="math notranslate nohighlight">\(N_t\)</span> is the number of training samples reaching node <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(N\)</span> is the total number of samples in the dataset and <span class="math notranslate nohighlight">\(\Delta_{MSE_t}\)</span> is the reduction in MSE with the <span class="math notranslate nohighlight">\(t\)</span> split.</p>
<p>Note, feature importance can be calculated in a similar manner to MSE above for the case of classification trees with <em>Gini Impurity</em>.</p>
<p>Feature importance is part of model-based feature ranking,</p>
<ul class="simple">
<li><p>the accuracy of the feature importance depends on the accuracy of the model, i.e., an inaccurate model will likely provide incorrect feature importance</p></li>
</ul>
</section>
<section id="feature-imputation">
<h2><strong>Feature Imputation</strong><a class="headerlink" href="#feature-imputation" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_imputation.html"><span class="doc std std-doc">Feature Imputation</span></a>: replacing null values in the data table, samples that do not have values for all features with plausible values for 2 reasons,</p>
<ul class="simple">
<li><p>enable statistical calculations and models that require complete data tables, i.e., cannot work with missing feature values</p></li>
<li><p>maximize model accuracy, increasing the number of reliable samples available for training and testing the model</p></li>
<li><p>mitigate model bias that may occur with likewise deletion in feature values are not missing at random</p></li>
</ul>
<p>Feature imputation methods include,</p>
<ul class="simple">
<li><p><em>constant value imputation</em> - replace null values with feature mean or mode</p></li>
<li><p><em>model-based imputation</em> - replace null values with a prediction of the missing feature with available feature values for the same sample</p></li>
</ul>
<p>There are also an iterative methods that depend on convergence,</p>
<ul class="simple">
<li><p><em>Multiple Imputation by Chained Equations (MICE)</em> - assign random values and then iterate over the missing values predicting new values</p></li>
</ul>
<p>The goal of this method is to obtain reasonable imputed values that account for the relationships between all the features and all the available and missing values</p>
</section>
<section id="feature-projection">
<h2><strong>Feature Projection</strong><a class="headerlink" href="#feature-projection" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_PCA.html"><span class="doc std std-doc">Principal Component Analysis</span></a>: a transforms original <span class="math notranslate nohighlight">\(m\)</span> features to <span class="math notranslate nohighlight">\(p\)</span> features, where <span class="math notranslate nohighlight">\(p &lt;&lt; m\)</span> for dimensionality reduction</p>
<ul class="simple">
<li><p>given features, <span class="math notranslate nohighlight">\(𝑋_1,\ldots,𝑋_𝑚\)</span>  we would require <span class="math notranslate nohighlight">\(\binom{m}{2} = \frac{𝑚(𝑚−1)}{2}\)</span> scatter plots to visualize just the two-dimensional scatter plots</p></li>
<li><p>these representations would not capture <span class="math notranslate nohighlight">\(&gt; 2\)</span> dimensional structures</p></li>
<li><p>once we have 4 or more variables understanding our data gets very difficult. Recall the curse of dimensionality.</p></li>
<li><p>principal component analysis, multidimensional scaling and random projection are examples</p></li>
<li><p>feature selection is an alternative method for dimensionality reduction</p></li>
</ul>
</section>
<section id="feature-space">
<h2><strong>Feature Space</strong><a class="headerlink" href="#feature-space" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: commonly feature space only refers to the predictor features and does not include the response feature(s), i.e.,</p>
<ul class="simple">
<li><p>all possible combinations of predictor features for which we need to make predictions</p></li>
<li><p>may be referred to as predictor feature space.</p></li>
</ul>
<p>Typically, we train and test our machines’ predictions over the predictor feature space.</p>
<ul class="simple">
<li><p>the space is typically a hypercuboid with each axis representing a predictor feature and extending from the minimum to maximum, over the range of each predictor feature</p></li>
<li><p>more complicated shapes of predictor feature space are possible, e.g., we could mask or remove subsets with poor data coverage.</p></li>
</ul>
</section>
<section id="feature-ranking">
<h2><strong>Feature Ranking</strong><a class="headerlink" href="#feature-ranking" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: part of feature engineering, feature ranking is a set of methods that assign relative importance or value to each feature with respect to information contained for inference and importance in predicting a response feature.</p>
<p>There are a wide variety of possible methods to accomplish this. My recommendation is a wide-array approach with multiple metric, while understanding the assumptions and limitations of each method.</p>
<p>Here’s the general types of metrics that we will consider for feature ranking:</p>
<ul class="simple">
<li><p><em>Visual Inspection</em> - including data distributions, scatter plots and violin plots</p></li>
<li><p><em>Statistical Summaries</em> - correlation analysis, mutual information</p></li>
<li><p><em>Model-based</em> - including model parameters, feature importance scores and global Shapley values</p></li>
<li><p><em>Recursive feature elimination</em> - and other methods that perform trail and error to find optimum parameters sets through withheld testing data cross validation</p></li>
</ul>
<p>Feature ranking is primarily motivated by the curse of dimensionality, i.e., work with the fewest, most informative predictor features.</p>
</section>
<section id="feature-transformations">
<h2><strong>Feature Transformations</strong><a class="headerlink" href="#feature-transformations" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: a type of feature engineering involving mathematical operation applied to a feature to improve the value of the feature in a workflow. For example,</p>
<ul class="simple">
<li><p>feature truncation</p></li>
<li><p>feature normalization or standardization</p></li>
<li><p>feature distribution transformation</p></li>
</ul>
<p>There are many reasons that we may want to perform feature transformations.</p>
<ul class="simple">
<li><p>the make the features consistent for visualization and comparison</p></li>
<li><p>to avoid bias or impose feature weighting for methods (e.g. k nearest neighbours regression) that rely on distances calculated in predictor feature space</p></li>
<li><p>the method requires the variables to have a specific range or distribution:</p>
<ul>
<li><p>artificial neural networks may require all features to range from [-1,1]</p></li>
<li><p>partial correlation coefficients require a Gaussian distribution.</p></li>
<li><p>statistical tests may require a specific distribution</p></li>
<li><p>geostatistical sequential simulation requires an indicator or Gaussian transform</p></li>
</ul>
</li>
</ul>
<p>Feature transformations is a common basic building blocks in many machine learning workflows.</p>
</section>
<section id="fourth-paradigm">
<h2><strong>Fourth Paradigm</strong><a class="headerlink" href="#fourth-paradigm" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the data-driven paradigm for scientific discovery building from the,</p>
<ul class="simple">
<li><p>First Paradigm - empirical science - experiments and observations</p></li>
<li><p>Second Paradigm - theoretical science - analytical expressions</p></li>
<li><p>Third Paradigm - computation science - numeric simulation</p></li>
</ul>
<p>We augment with new scientific paradigms, we don’t replace older paradigms. Each of the previous paradigm are supported by the previous paradigms, for example,</p>
<ul class="simple">
<li><p>theoretical science is build on empirical science</p></li>
<li><p>numerical simulations integrate analytical expressions and calibrated equations from experiment</p></li>
</ul>
</section>
<section id="frequentist-probability">
<h2><strong>Frequentist Probability</strong><a class="headerlink" href="#frequentist-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: measure of the likelihood that an event will occur based on frequencies observed from an experiment. For random experiments and well-defined settings (such as coin tosses),</p>
<div class="math notranslate nohighlight">
\[
\text{Prob}(A) = P(A) = \lim_{n \to \infty} \frac{n(A)}{n}
\]</div>
<p>where:</p>
<p><span class="math notranslate nohighlight">\(n(A)\)</span> = number of times event <span class="math notranslate nohighlight">\(A\)</span> occurred
<span class="math notranslate nohighlight">\(n\)</span> = number of trails</p>
<p>For example, possibility of drilling a dry hole for the next well, encountering sandstone at a location (<span class="math notranslate nohighlight">\(\bf{u}_{\alpha}\)</span>), exceeding a rock porosity of <span class="math notranslate nohighlight">\(15 \%\)</span> at a location (<span class="math notranslate nohighlight">\(\bf{u}_{\alpha}\)</span>).</p>
</section>
<section id="gaussian-anamorphosis">
<h2><strong>Gaussian Anamorphosis</strong><a class="headerlink" href="#gaussian-anamorphosis" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: a quantile transformation to a Gaussian distribution.</p>
<p>Mapping feature values through their cumulative probabilities.</p>
<div class="math notranslate nohighlight">
\[
y = G_y^{-1}\left( F_x(x)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(𝐹_𝑥\)</span> is the original feature cumulative distribution function (CDF) and <span class="math notranslate nohighlight">\(𝐺_𝑦\)</span> is the Gaussian CDF probability density function</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} exp \left[-1 \frac{1}{2} \left(\frac{x-\mu}{\sigma} \right)^2 \right] 
\]</div>
<p>shorthand for a normal distribution is</p>
<div class="math notranslate nohighlight">
\[
N[\mu,\sigma^2]
\]</div>
<p>for example <span class="math notranslate nohighlight">\(N[0,1]\)</span> is standard normal</p>
<ul class="simple">
<li><p>much of natural variation or measurement error is Gaussian</p></li>
<li><p>parameterized fully by mean, variance and correlation coefficient (if multivariate)</p></li>
<li><p>distribution is unbounded, no min nor max, extremes are very unlikely, some type of truncation is often applied</p></li>
</ul>
<p>Warning, many workflows apply univariate Gaussian anamorphosis and then assume bivariate or multivariate Gaussian, this is not correct, but it is generally too difficult to transform our data to multivariate Gaussian.</p>
<p>Methods that require a Gaussian distribution,</p>
<ul class="simple">
<li><p>Pearson product-moment correlation coefficients completely characterize multivariate relationships when data are multivariate Gaussian</p></li>
<li><p>partial correlations require bivariate Gaussian</p></li>
<li><p>sequential simulation (geostatistics) assumes Gaussian to reproduce the global distribution</p></li>
<li><p>Student’s t test for difference in means</p></li>
<li><p>Chi-square distributions is derived from sum of squares of Gaussian distributed random variables</p></li>
<li><p>Gaussian naive Bayes classification assumes Gaussian conditionals</p></li>
</ul>
</section>
<section id="gibbs-sampler-mcmc">
<h2><strong>Gibbs Sampler</strong> (MCMC)<a class="headerlink" href="#gibbs-sampler-mcmc" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html"><span class="doc std std-doc">Bayesian Linear Regression</span></a>: a set of algorithms to sample from a probability distribution such that the samples match the distribution statistics, based on,</p>
<ul class="simple">
<li><p>sequentially sampling from conditional distributions</p></li>
</ul>
<p>Since only the conditional probability density functions are required, the system is simplified as the full joint probability density function is not needed</p>
<p>Here’s the basic steps of the Gibbs MCMC Sampler for a bivariate case,</p>
<ol class="arabic simple">
<li><p>Assign random values for <span class="math notranslate nohighlight">\(𝑋(0)\)</span>, <span class="math notranslate nohighlight">\(𝑌(0)\)</span></p></li>
<li><p>Sample from <span class="math notranslate nohighlight">\(𝑓(𝑋|𝑌(0))\)</span> to get <span class="math notranslate nohighlight">\(𝑋(1)\)</span></p></li>
<li><p>Sample from <span class="math notranslate nohighlight">\(𝑓(𝑌|𝑋(1))\)</span> to get <span class="math notranslate nohighlight">\(𝑌(1)\)</span></p></li>
<li><p>Repeat for the next steps for samples, <span class="math notranslate nohighlight">\(\ell = 1,\ldots,𝐿\)</span></p></li>
</ol>
<p>The resulting samples will have the correct joint distribution,</p>
<div class="math notranslate nohighlight">
\[
𝑓(𝑋,𝑌)
\]</div>
</section>
<section id="gradient-boosting-models">
<h2><strong>Gradient Boosting Models</strong><a class="headerlink" href="#gradient-boosting-models" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_gradient_boosting.html"><span class="doc std std-doc">Gradient Boosting</span></a>: a prediction model that results from posing a boosting model as gradient descent problem</p>
<p>At each step, <span class="math notranslate nohighlight">\(k\)</span>, a model is being fit, then the error is calculated, <span class="math notranslate nohighlight">\(h_k(X_1,\ldots,X_m)\)</span>.</p>
<p>We can assign a loss function,</p>
<div class="math notranslate nohighlight">
\[
L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)^2}{2}
\]</div>
<p>So we want to minimize the <span class="math notranslate nohighlight">\(\ell2\)</span> loss function:</p>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right)
\]</div>
<p>by adjusting our model result over our training data <span class="math notranslate nohighlight">\(F(x_1), F(x_2),\ldots,F(x_n)\)</span>.</p>
<p>We can take the partial derivative of the error vs. our model,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i
\]</div>
<p>We can interpret the residuals as negative gradients.</p>
<div class="math notranslate nohighlight">
\[
y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} 
\]</div>
<p>So now we have a gradient descent problem:</p>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) + h(X_i)
\]</div>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i)
\]</div>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)}
\]</div>
<p>Of the general form:</p>
<div class="math notranslate nohighlight">
\[
\phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k}
\]</div>
<p>where <span class="math notranslate nohighlight">\(phi_k\)</span> is the current state, <span class="math notranslate nohighlight">\(\rho\)</span> is the learning rate, <span class="math notranslate nohighlight">\(J\)</span> is the loss function, and <span class="math notranslate nohighlight">\(\phi_{k+1}\)</span> is the next state of our estimator.</p>
<p>The error residual at training data is the gradient, then we are performing gradient descent,</p>
<ul class="simple">
<li><p>fitting a series of models to negative gradients</p></li>
</ul>
<p>By approaching the problem as a gradient decent problem we are able to apply a variety of loss functions,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\ell2\)</span> is our <span class="math notranslate nohighlight">\(\frac{\left(y - F(X)\right)^2}{2}\)</span> is practical, but is not robust with outliers</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
- 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\ell1\)</span> is our <span class="math notranslate nohighlight">\(|y - F(X)|\)</span> is more robust with outliers</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
- 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i))
\]</div>
<ul class="simple">
<li><p>there are others like Huber Loss</p></li>
</ul>
</section>
<section id="graph-laplacian-spectral-clustering">
<h2><strong>Graph Laplacian</strong> (spectral clustering)<a class="headerlink" href="#graph-laplacian-spectral-clustering" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_spectral_clustering.html"><span class="doc std std-doc">Spectral Clustering</span></a>: a matrix representing a graph by integrating connections between graph nodes, samples, number of connections for each graph nodes, samples. Calculated as degree matrix minus adjacency matrix. Where,</p>
<ul class="simple">
<li><p><em>degree matrix</em>, <span class="math notranslate nohighlight">\(𝐷\)</span> - degree of connection for each node</p></li>
<li><p>adjacency matrix, <span class="math notranslate nohighlight">\(𝐴\)</span> - specific connections between nodes</p></li>
</ul>
</section>
<section id="geostatistics">
<h2><strong>Geostatistics</strong><a class="headerlink" href="#geostatistics" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a branch of applied statistics that integrates:</p>
<ol class="arabic simple">
<li><p>the spatial (geological) context</p></li>
<li><p>the spatial relationship</p></li>
<li><p>volumetric support / scale</p></li>
<li><p>uncertainty</p></li>
</ol>
<p>I include all spatial statistics with geostatistics, some disagree with me on this. From my experience, any useful statistical method for modeling spatial phenomenon is adopted and added to the geostatistics toolkit! Geostatistics is an expanding and evolving field of study.</p>
</section>
<section id="gradient-based-optimization">
<h2><strong>Gradient-based Optimization</strong><a class="headerlink" href="#gradient-based-optimization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_LASSO_regression.html"><span class="doc std std-doc">LASSO Regression</span></a>: a method to solve for model parameters by iteratively minimizing the loss function. The steps include,</p>
<ol class="arabic simple">
<li><p>start with random model parameters</p></li>
<li><p>calculate the loss function for the model parameters</p></li>
<li><p>calculate the loss function gradient, generally don’t have an equation for the loss function, sampling with numerical calculation of the local loss function derivative,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ 
\nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha}, b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} 
\]</div>
<ol class="arabic simple" start="4">
<li><p>update the parameter estimate by stepping down slope / gradient,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ 
\hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1))
\]</div>
<p>where <span class="math notranslate nohighlight">\(r\)</span> is the learning rate/step size, <span class="math notranslate nohighlight">\(\hat{b}(1,𝑡)\)</span>, is the current model parameter estimate and <span class="math notranslate nohighlight">\(\hat{b}(1,𝑡+1)\)</span> is the updated parameter estimate.</p>
<p>Some important comments about gradient-based optimization,</p>
<ul class="simple">
<li><p><em>gradient search convergence</em> - the method will find a local or global minimum</p></li>
<li><p><em>gradient search step size</em> - impact of step size, <span class="math notranslate nohighlight">\(r\)</span> too small, takes too long to converge to a solution and <span class="math notranslate nohighlight">\(r\)</span> too large, the solution may skip over/miss a global minimum or diverge</p></li>
<li><p><em>multiple model parameters</em> - calculate and decompose the gradient over multiple model parameters, with a vector representation.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\nabla L(y_{\alpha}, F(X_{\alpha}, b_1, b_2)) = \left[ \begin{matrix} \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) &amp; \nabla L(y_{\alpha}, F(X_{\alpha}, b_2)) \end{matrix} \right]
\]</div>
<ul class="simple">
<li><p><em>exploration of parameter space</em> - optimization for training machine learning model parameters is exploration of a high dimensional model parameter space</p></li>
</ul>
</section>
<section id="graph-spectral-clustering">
<h2><strong>Graph</strong> (spectral clustering)<a class="headerlink" href="#graph-spectral-clustering" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_spectral_clustering.html"><span class="doc std std-doc">Spectral Clustering</span></a>: a diagram that represents data in an organized manner, each sample as a node with vertices indicating pairwise relationships between samples.</p>
<ul class="simple">
<li><p>for an undirected graph, vertices are bidirectional, i.e., the connection is symmetric, both ways with the same strength</p></li>
</ul>
</section>
<section id="gridded-data">
<h2><strong>Gridded Data</strong><a class="headerlink" href="#gridded-data" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Machine Learning Workflow Construction and Coding</span>: generally exhaustive, regularly spaced data over 2D or 3D, representing maps and models</p>
<ul class="simple">
<li><p>stored as a .csv comma delimited file, with <span class="math notranslate nohighlight">\(𝑛_𝑦\)</span> rows and <span class="math notranslate nohighlight">\(𝑛_𝑥\)</span> columns</p></li>
<li><p>may also be saved/loaded as also binary for a more compact, but not human readable file.</p></li>
<li><p>commonly visualized directly, for example, matplotlib’s imshow function, or as contour maps</p></li>
</ul>
</section>
<section id="hard-data">
<h2><strong>Hard Data</strong><a class="headerlink" href="#hard-data" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: data that has a high degree of certainty, usually from a direct measurement from the rock</p>
<ul class="simple">
<li><p>for example, well core-based and well log-based porosity and lithofacies</p></li>
</ul>
<p>In general, hard data has high resolution (small scale, volume support), but with poor coverage (measure only an extremely small proportion of the population, for example,</p>
<ul class="simple">
<li><p><em>Core coverage deepwater oil and gas</em> - well core only sample one five hundred millionth to one five billionth of a deepwater reservoir, assuming 3 inch diameter cores with 10% core coverage in vertical wells with 500 m to 1,500 m spacing</p></li>
<li><p><em>Core coverage mining grade control</em> - diamond drill hole cores sample one eight thousandth to one thirty thousandth of ore body, assuming HQ 63.5 mm diameter cores with 100% core coverage in vertical drill holes with 5 m to 10 m spacing</p></li>
</ul>
</section>
<section id="hermite-polynomials">
<h2><strong>Hermite Polynomials</strong><a class="headerlink" href="#hermite-polynomials" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_polynomial_regression.html"><span class="doc std std-doc">Polynomial Regression</span></a>: a family of orthogonal polynomials on the real number line.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Order</p></th>
<th class="head"><p>Hermite Polynomial <span class="math notranslate nohighlight">\(H_e(x)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0th Order</p></td>
<td><p><span class="math notranslate nohighlight">\(H_{e_0}(x) = 1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>1st Order</p></td>
<td><p><span class="math notranslate nohighlight">\(H_{e_1}(x) = x\)</span></p></td>
</tr>
<tr class="row-even"><td><p>2nd Order</p></td>
<td><p><span class="math notranslate nohighlight">\(H_{e_2}(x) = x^2 - 1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>3rd Order</p></td>
<td><p><span class="math notranslate nohighlight">\(H_{e_3}(x) = x^3 - 3x\)</span></p></td>
</tr>
<tr class="row-even"><td><p>4th Order</p></td>
<td><p><span class="math notranslate nohighlight">\(H_{e_4}(x) = x^4 - 6x^2 + 3\)</span></p></td>
</tr>
</tbody>
</table>
<p>These polynomials are orthogonal with respect to a weighting function,</p>
<div class="math notranslate nohighlight">
\[
𝑤(𝑥)=𝑒^{−\frac{𝑥^2}{2}}
\]</div>
<p>this is the standard Gaussian probability density function without the scaler, <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{2\pi}}\)</span>. The definition of orthogonality is stated as,</p>
<div class="math notranslate nohighlight">
\[ 
\int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 
\]</div>
<p>The Hermite polynomials are orthogonal over the interval <span class="math notranslate nohighlight">\([−\infty,\infty]\)</span> for the standard normal probability distribution.</p>
<p>By applying hermite polynomials instead of regular polynomials for polynomial basis expansion in polynomial regression were remove the multicollinearity between the predictor features,</p>
<ul class="simple">
<li><p>recall, independence of the predictor features is an assumption of the linear system applied in polynomial regression with the polynomial basis expansion</p></li>
</ul>
</section>
<section id="heuristic-algorithm">
<h2><strong>Heuristic Algorithm</strong><a class="headerlink" href="#heuristic-algorithm" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Cluster Analysis</span>: a shortcut solution to solve a difficult problem
a compromise of optimality and accuracy for speed and practicality.</p>
<ul class="simple">
<li><p>this general approach is common in machine learning, computer science and mathematical optimization, for example, the solution for k-mean clustering a <span class="math notranslate nohighlight">\(k^n\)</span> solution space is practically solved with an heuristic algorithm.</p></li>
</ul>
</section>
<section id="hierarchical-clustering">
<h2><strong>Hierarchical Clustering</strong><a class="headerlink" href="#hierarchical-clustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Cluster Analysis</span>: all cluster group assignments are determined iteratively, as opposed to an partitional clustering method that determine cluster groups all at once. Including,</p>
<ul class="simple">
<li><p><em>agglomerative hierarchical clustering</em> - start with <span class="math notranslate nohighlight">\(n\)</span> clusters, each data sample in its own cluster, and then iteratively merges clusters into larger clusters</p></li>
<li><p><em>divisive hierarchical clustering</em> - start with all data in one cluster, and then iteratively divide off new clusters</p></li>
<li><p>k-means clustering is partitional clustering, while the solution heuristic to find the solution is iterative, the solution is actually all at once</p></li>
<li><p>difficult to update, once a series of splits or mergers are made it is difficult to go back and modify the model</p></li>
</ul>
</section>
<section id="histogram">
<h2><strong>Histogram</strong><a class="headerlink" href="#histogram" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_univariate_analysis.html"><span class="doc std std-doc">Univariate Analysis</span></a>: a representation of the univariate statistical distribution with a plot of frequency over an exhaustive set of bins over the range of possible values. These are the steps to build a histogram,</p>
<ol class="arabic simple">
<li><p>Divide the continuous feature range of possible values into <span class="math notranslate nohighlight">\(K\)</span> equal size bins, <span class="math notranslate nohighlight">\(\delta x\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\Delta x = \left( \frac{x_{max} - x_{min}}{K} \right)
\]</div>
<p>or use available category labels for categorical features.</p>
<ol class="arabic simple" start="2">
<li><p>Count the number of samples (frequency) in each bin, <span class="math notranslate nohighlight">\(n_k\)</span>, \quad <span class="math notranslate nohighlight">\(\forall \quad k=1,\ldots,K\)</span>.</p></li>
<li><p>Plot the frequency vs. the bin label (use bin centroid if continuous)</p></li>
</ol>
<p>Note, histograms are typically plotted as a bar chart.</p>
</section>
<section id="hybrid-model">
<h2><strong>Hybrid Model</strong><a class="headerlink" href="#hybrid-model" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: system or process that includes a combination of both <em>deterministic model</em> and <em>stochastic model</em></p>
<ul class="simple">
<li><p>most geostatistical models are hybrid models</p></li>
<li><p>for example, additive deterministic trend models and stochastic residual models</p></li>
</ul>
</section>
<section id="independence-probability">
<h2><strong>Independence</strong> (probability)<a class="headerlink" href="#independence-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent if and only if the following relations are true,</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(P(A \cap B) = P(A) \cdot P(B)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(A|B) = P(A)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A) = P(B)\)</span></p></li>
</ol>
<p>If any of these are violated we suspect that there exists some form of relationship.</p>
</section>
<section id="indicator-transform-also-binary-transform">
<h2><strong>Indicator Transform</strong> (also Binary Transform)<a class="headerlink" href="#indicator-transform-also-binary-transform" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: indicator coding a random variable to a probability relative to a category or a threshold.</p>
<p>If <span class="math notranslate nohighlight">\(i(\bf{u}:z_k)\)</span> is an indicator for a categorical variable,</p>
<ul class="simple">
<li><p>what is the probability of a realization equal to a category?</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
i(\bf{u}; z_k) =
\begin{cases} 
    1, &amp; \text{if } Z(\bf{u}) = z_k \\
    0, &amp; \text{if } Z(\bf{u}) \ne z_k 
\end{cases}
\end{split}\]</div>
<p>for example,</p>
<ul class="simple">
<li><p>given threshold, <span class="math notranslate nohighlight">\(z_2 = 2\)</span>, and data at <span class="math notranslate nohighlight">\(\bf{u}_1\)</span>, <span class="math notranslate nohighlight">\(z(\bf{u}_1) = 2\)</span>, then <span class="math notranslate nohighlight">\(i(bf{u}_1; z_2) = 1\)</span></p></li>
<li><p>given threshold, <span class="math notranslate nohighlight">\(z_1 = 1\)</span>, and a RV away from data, <span class="math notranslate nohighlight">\(Z(\bf{u}_2)\)</span> then is calculated as <span class="math notranslate nohighlight">\(F^{-1}_{\bf{u}_2}(z_1)\)</span> of the RV as <span class="math notranslate nohighlight">\(i(\bf{u}_2; z_1) = 0.23\)</span></p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(i(\bf{u}:z_k)\)</span> is an indicator for a continuous variable,</p>
<ul class="simple">
<li><p>what is the probability of a realization less than or equal to a threshold?</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
i(\bf{u}; z_k) =
\begin{cases} 
    1, &amp; \text{if } Z(\bf{u}) \le z_k \\
    0, &amp; \text{if } Z(\bf{u}) &gt; z_k 
\end{cases}
\end{split}\]</div>
<p>for example,</p>
<ul class="simple">
<li><p>given threshold, <span class="math notranslate nohighlight">\(z_1 = 6\%\)</span>, and data at <span class="math notranslate nohighlight">\(\bf{u}_1\)</span>, <span class="math notranslate nohighlight">\(z(\bf{u}_1) = 8\%\)</span>, then <span class="math notranslate nohighlight">\(i(\bf{u}_1; z_1) = 0\)</span></p></li>
<li><p>given threshold, <span class="math notranslate nohighlight">\(z_4 = 18\%\)</span>, and a RV away from data, <span class="math notranslate nohighlight">\(Z(\bf{u}_2) = N\left[\mu = 16\%,\sigma = 3\%\right]\)</span> then <span class="math notranslate nohighlight">\(i(\bf{u}_2; z_4) = 0.75\)</span></p></li>
</ul>
<p>The indicator coding may be applied over an entire random function by indicator transform of all the random variables at each location.</p>
</section>
<section id="indicator-variogram">
<h2><strong>Indicator Variogram</strong><a class="headerlink" href="#indicator-variogram" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: varogram’s calculated and modelled from the <em>indicator transform</em> of spatial data and used for indicator kriging. The indicator variogram is,</p>
<div class="math notranslate nohighlight">
\[
\gamma_i(\mathbf{h}; z_k) = \frac{1}{2N(\mathbf{h})} 
\sum_{\alpha=1}^{N(\mathbf{h})} 
\left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h}; z_k) \right]^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(i(\mathbf{u}_\alpha; z_k)\)</span> and <span class="math notranslate nohighlight">\(i(\mathbf{u}_\alpha + \mathbf{h}; z_k)\)</span> are the indicator transforms for the <span class="math notranslate nohighlight">\(z_k\)</span> threshold at the tail location <span class="math notranslate nohighlight">\(\mathbf{u}_\alpha\)</span> and head location <span class="math notranslate nohighlight">\(\mathbf{u}_\alpha + \mathbf{h}\)</span> respectively.</p>
<ul class="simple">
<li><p>for hard data the indicator transform <span class="math notranslate nohighlight">\(i(\bf{u},z_k)\)</span> is either 0 or 1, in which case the <span class="math notranslate nohighlight">\(\left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h}; z_k) \right]^2\)</span> is equal to 0 when the values at head and tail are both <span class="math notranslate nohighlight">\(\le z_k\)</span> (for continuous features) or <span class="math notranslate nohighlight">\(= z_k\)</span>  (for categorical features), the same relative to the threshold, or 1 when they are different.</p></li>
<li><p>therefore, the indicator variogram is <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> the proportion of pairs that change! The indicator variogram can be related to probability of change over a lag distance, <span class="math notranslate nohighlight">\(h\)</span>.</p></li>
<li><p>the sill of an indicator variogram is the indicator variance calculated as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma_i^2 = p \cdot (1 - p)
\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is the proportion of 1’s (or zeros as the function is symmetric over proportion)</p>
</section>
<section id="inference-inferential-statistics">
<h2><strong>Inference, Inferential Statistics</strong><a class="headerlink" href="#inference-inferential-statistics" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: this is a big topic, but for the course I provide this simplified, functional definition, given a random sample from a population, describe the population, for example,</p>
<ul class="simple">
<li><p>given the well samples, describe the reservoir</p></li>
<li><p>given the drill hole samples, describe the ore body</p></li>
</ul>
</section>
<section id="inlier">
<h2><strong>Inlier</strong><a class="headerlink" href="#inlier" title="Permalink to this heading">#</a></h2>
<p>a regression model accuracy metric, the proportion of testing data within a margin, <span class="math notranslate nohighlight">\(\epsilon\)</span>, of the model predictions, <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>,</p>
<div class="math notranslate nohighlight">
\[ 
I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i) 
\]</div>
<p>given the indicator transform,</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
I(y_i, \hat{y}_i) = 
\begin{cases} 
1, &amp; \text{if } |y_i - \hat{y}_i| \leq \epsilon \\
0, &amp; \text{otherwise}
\end{cases} 
\end{split}\]</div>
<p>This is a useful, intuitive measure of accuracy, the proportion of training or testing data with predictions that are good enough.</p>
<ul class="simple">
<li><p>but, there is a choice of the size of the margin, <span class="math notranslate nohighlight">\(\epsilon\)</span>, that could be related to the accuracy required for the specific application</p></li>
</ul>
</section>
<section id="instance-based-learning">
<h2><strong>Instance-based Learning</strong><a class="headerlink" href="#instance-based-learning" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_knearest_neighbours.html"><span class="doc std std-doc">k-Nearest Neighbours</span></a>: also known as memory-based learning, compares new prediction problems (as set of predictors, <span class="math notranslate nohighlight">\(𝑥_1,\ldots,𝑥_𝑚\)</span>) with the cases observed in the training data.</p>
<ul class="simple">
<li><p>model requires access to the training data, acting as a library of observations</p></li>
<li><p>prediction directly from the training data</p></li>
<li><p>prediction complexity grows with the number of training data, <span class="math notranslate nohighlight">\(𝑛\)</span>, number of neighbors, <span class="math notranslate nohighlight">\(𝑘\)</span>, and number of features, <span class="math notranslate nohighlight">\(𝑚\)</span>.</p></li>
<li><p>a specific case of lazy learning</p></li>
</ul>
</section>
<section id="intersection-of-events-probability">
<h2><strong>Intersection of Events</strong> (probability)<a class="headerlink" href="#intersection-of-events-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: the intersection of outcomes, the probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is represented as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(A,B)
\]</div>
<p>under the assumption of independence of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> the probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is,</p>
<div class="math notranslate nohighlight">
\[
P(A,B) = P(A) \cdot P(B)
\]</div>
</section>
<section id="irreducible-error">
<h2><strong>Irreducible Error</strong><a class="headerlink" href="#irreducible-error" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: is error due to data limitations, including missing features and missing samples, for example, the full predictor feature space is not adequately sampled</p>
<ul class="simple">
<li><p>irreducible error is not impacted by model complexity, it is a limitation of the data</p></li>
<li><p>one of the three components of expected test square error, including model variance, model bias and irreducible error</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2
+
\]</div>
<div class="math notranslate nohighlight">
\[
E \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0, \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_e^2\)</span> is irreducible error.</p>
</section>
<section id="inertia-clustering">
<h2><strong>Inertia</strong> (clustering)<a class="headerlink" href="#inertia-clustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Cluster Analysis</span>: the k-means clustering loss function summarizing the difference between samples within the same group over all the groups,</p>
<div class="math notranslate nohighlight">
\[
I = \sum_{i=1}^{K} \sum_{x_j \in C_i} \| x_j - \mu_i \|^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is the total number of clusters, <span class="math notranslate nohighlight">\(C_i\)</span> represents the set of samples in the <span class="math notranslate nohighlight">\(i^{th}\)</span> cluster, <span class="math notranslate nohighlight">\(x_j\)</span> represents a data sample in cluster, <span class="math notranslate nohighlight">\(C_i\)</span>, <span class="math notranslate nohighlight">\(mu_i\)</span> is the prototype of cluster <span class="math notranslate nohighlight">\(C_i\)</span>,<span class="math notranslate nohighlight">\(\| x_j - \mu_i \|^2\)</span> is the squared Euclidean distance between sample <span class="math notranslate nohighlight">\(x_j\)</span> and the cluster prototype <span class="math notranslate nohighlight">\(\mu_i \)</span>. The samples and prototypes and distance calculations in mD space, with <span class="math notranslate nohighlight">\(1,\ldots,m\)</span> features.</p>
<ul class="simple">
<li><p>by minimizing inertia k-means clusters minimizes difference within groups while maximizing difference between groups</p></li>
</ul>
</section>
<section id="joint-probability">
<h2><strong>Joint Probability</strong><a class="headerlink" href="#joint-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: probability that considers more than one event occurring together, the probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is represented as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(A,B)
\]</div>
<p>or the probability of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span> is represented as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B \cap C) = P(A,B,C)
\]</div>
<p>under the assumption of independence of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span> the joint probability may be calculated as,</p>
<div class="math notranslate nohighlight">
\[
P(A,B,C) = P(A) \cdot P(B) \cdot P(C)
\]</div>
</section>
<section id="k-bins-discretization">
<h2><strong>K Bins Discretization</strong><a class="headerlink" href="#k-bins-discretization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: bin the range of the feature into K bins, then for each sample assignment of a value of 1 if the sample is within a bin and 0 if outsize the bin</p>
<ul class="simple">
<li><p>binning strategies include uniform width bins (uniform) and uniform number of data in each bin (quantile)</p></li>
<li><p>also known as one hot encoding</p></li>
</ul>
<p>Methods that require K bins discretization,</p>
<ul class="simple">
<li><p>basis expansion to work in a higher dimensional space</p></li>
<li><p>discretization of continuous features to categorical features for categorical methods such as naive Bayes classifier</p></li>
<li><p>histogram construction and Chi-square test for difference in distributions</p></li>
<li><p>mutual information binning</p></li>
</ul>
</section>
<section id="k-fold-cross-validation">
<h2><strong>K-fold Cross Validation</strong><a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: partitioning the data into K folds, and looping over the folds training the model with reminder of the data and testing the model with the data in the fold. Then aggregating the testing accuracy over all the folds.</p>
<ul class="simple">
<li><p>the train and test data split is based on K, for example, K = 4, is 25% testing for each fold and K = 5, is 20% testing for each fold</p></li>
<li><p>this is an improvement over cross validation that only applies one train and test split to build a single model. The K-fold approach allows testing of all data and the aggregation of accuracy over all the folds tends to smooth the accuracy vs. hyperparameter plot for more reliable hyperparameter tuning</p></li>
<li><p>k-fold cross validation may be applied to check model performance for estimation accuracy (most common) and uncertainty model goodness (<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0920410521006343">Maldonado-Cruz and Pyrcz, 2021</a>)</p></li>
</ul>
</section>
<section id="k-means-clustering">
<h2><strong>k-Means Clustering</strong><a class="headerlink" href="#k-means-clustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Cluster Analysis</span>: an unsupervised machine learning method for partitional clustering, group assignment to unlabeled data, where dissimilarity within clustered groups is mini minimized. The loss function that is minimized is,</p>
<div class="math notranslate nohighlight">
\[ 
I = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i ||
\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> is the cluster index, <span class="math notranslate nohighlight">\(\alpha\)</span> is the data sample index, <span class="math notranslate nohighlight">\(X\)</span> is the data sample and <span class="math notranslate nohighlight">\(\mu_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span> cluster prototype, <span class="math notranslate nohighlight">\(k\)</span> is the total number of clusters, and <span class="math notranslate nohighlight">\(|| X_m - \mu_m ||\)</span> is the Euclidean distance from a sample to the cluster prototype in <span class="math notranslate nohighlight">\(M\)</span> dimensional space calculated as,</p>
<div class="math notranslate nohighlight">
\[
|| X_{m,\alpha} - \mu_i || =  \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i} \right)^2 }
\]</div>
<p>Here is a summary of import aspects for k-means clustering,</p>
<ul class="simple">
<li><p><em>k</em> - is given as a model hyperparameter</p></li>
<li><p><em>exhaustive and mutually exclusive groups</em> - all data assigned to a single group</p></li>
<li><p><em>prototype method</em> - represents the training data with number of synthetic cases in the features space. For K-means clustering we assign and iteratively update <span class="math notranslate nohighlight">\(K\)</span> prototypes.</p></li>
<li><p><em>iterative solution</em> - the initial prototypes are assigned randomly in the feature space, the labels for each training sample are updated to the nearest prototype, then the prototypes are adjusted to the centroid of their assigned training data, repeat until there is no further update to the training data assignments.</p></li>
<li><p><em>unsupervised learning</em> - the training data are not labeled and are assigned <span class="math notranslate nohighlight">\(K\)</span> labels based on their proximity to the prototypes in the feature space. The idea is that similar things, proximity in feature space, should belong to the same cluster group.</p></li>
<li><p><em>feature weighting</em> - the procedure depends on the Euclidian distance between training samples and prototypes in feature space. Distance is treated as the ‘inverse’ of similarity. If the features have significantly different magnitudes, the feature(s) with the largest magnitudes and ranges will dominate the loss function and cluster groups will become anisotropic aligned orthogonal to the high range feature(s). While the common approach is to standardize / normalize the variables, by-feature weighting may be applied through unequal variances. Note, in this demonstration we normalize the features to range from 0.0 to 1.0.</p></li>
</ul>
</section>
<section id="k-nearest-neighbours">
<h2><strong>k-Nearest Neighbours</strong><a class="headerlink" href="#k-nearest-neighbours" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_knearest_neighbours.html"><span class="doc std std-doc">k-Nearest Neighbours</span></a>: a simple, interpretable and flexible, nonparametric predictive machine learning model based on a local weighting window applied to <span class="math notranslate nohighlight">\(k\)</span> nearest training data</p>
<p>The k-nearest neighbours approach is similar to a convolution approach for spatial interpolation. Convolution is the integral product of two functions, after one is reversed and shifted by <span class="math notranslate nohighlight">\(\Delta\)</span>.</p>
<ul class="simple">
<li><p>one interpretation is smoothing a function with weighting function, <span class="math notranslate nohighlight">\(𝑓(\Delta)\)</span>, is applied to calculate the weighted average of function, <span class="math notranslate nohighlight">\(𝑔(x)\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
(f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta 
\]</div>
<p>this easily extends into multidimensional</p>
<div class="math notranslate nohighlight">
\[
(f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \, d\Delta_x \, d\Delta_y \, d\Delta_z
\]</div>
<p>The choice of which function is shifted before integration does not change the result, the convolution operator has commutativity.</p>
<div class="math notranslate nohighlight">
\[ 
(f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta 
\]</div>
<div class="math notranslate nohighlight">
\[
(f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta 
\]</div>
<ul class="simple">
<li><p>if either function is reflected then convolution is equivalent to cross-correlation, measure of similarity between 2 signals as a function of displacement.</p></li>
<li><p>for k-nearest neighbours the use of <span class="math notranslate nohighlight">\(k\)</span> results in a locally adaptive window size, different from standard convolution</p></li>
</ul>
<p>K-nearest neighbours is an instance-based, lazy learning method, the model training is postponed until prediction is required, no precalculation of the model. i.e., prediction requires access to the data.</p>
<ul class="simple">
<li><p>to make new predictions that training data must be available</p></li>
</ul>
<p>The hyperparameters include,</p>
<ul class="simple">
<li><p><em>k number of nearest data</em> to utilize for prediction</p></li>
<li><p><em>data weighting</em>, for example uniform weighting with the local training data average, or inverse distance weighting</p></li>
</ul>
<p>Note, for the case of inverse distance weighting, the method is analogous to inverse distance weighted interpolation with a maximum number of local data constraint commonly applied for spatial interpolation.</p>
<ul class="simple">
<li><p>inverse distance is available in GeostatsPy for spatial mapping.</p></li>
</ul>
<p>Too find the k-nearest data a distance metric is needed,</p>
<ul class="simple">
<li><p>training data within the predictor feature space are ranked by distance (closest to farthest)</p></li>
<li><p>a variety of distance metrics may be applied, including:</p></li>
</ul>
<ol class="arabic simple">
<li><p>Euclidian distance</p></li>
</ol>
<p>\begin{equation}<br />
d_i = \sqrt{\sum_{\alpha = 1}^{m} \left(x_{\alpha,i} - x_{\alpha,0}\right)^2}
\end{equation}</p>
<ol class="arabic simple" start="2">
<li><p>Minkowski Distance - a general expression for distance with well-known Manhattan and Euclidean distances are special cases,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p \right)^{\frac{1}{p}}
\]</div>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(p=2\)</span>, this becomes the Euclidean distance</p></li>
<li><p>when <span class="math notranslate nohighlight">\(p=1\)</span> it becomes the Manhattan distance</p></li>
</ul>
</section>
<section id="kernel-trick-support-vector-machines">
<h2><strong>Kernel Trick</strong> (support vector machines)<a class="headerlink" href="#kernel-trick-support-vector-machines" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_support_vector_machines.html"><span class="doc std std-doc">Support Vector Machines</span></a>: we can incorporate our basis expansion in our method without ever needing to transform the training data to this higher dimensional space,</p>
<div class="math notranslate nohighlight">
\[
h(x)
\]</div>
<p>We only need the inner product over the predictor features,</p>
<div class="math notranslate nohighlight">
\[
h(x) \left( h(x') \right)^T = \langle h(x), h(x') \rangle
\]</div>
<p>Instead of the actual values in the transformed space, we just need the ‘similarity’ between all available training data in that transformed space!</p>
<ul class="simple">
<li><p>we training our support vector machines with only a similarity matrix between training data that will be projected to the higher dimensional space</p></li>
<li><p>we never actually need to calculate the training data values in the higher dimensional space</p></li>
</ul>
</section>
<section id="kriging">
<h2><strong>Kriging</strong><a class="headerlink" href="#kriging" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: spatial estimation approach that relies on linear weights that account for spatial continuity, data closeness and redundancy. The kriging estimate is,</p>
<div class="math notranslate nohighlight">
\[
z^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot z(\bf{u}_{\alpha}) + \left( 1.0 - \sum_{\alpha=1}^n \lambda_{\alpha} \right) \cdot m_z
\]</div>
<ul class="simple">
<li><p>the right term is the unbiasedness constraint, one minus the sum of the weights is applied to the global mean.</p></li>
</ul>
<p>In the case where the trend, <span class="math notranslate nohighlight">\(t(\bf{u})\)</span>, is removed, we now have a residual, <span class="math notranslate nohighlight">\(y(\bf{u})\)</span>,</p>
<div class="math notranslate nohighlight">
\[
y(\bf{u}) = z(\bf{u}) - t(\bf{u}) 
\]</div>
<p>the residual mean is zero so we can simplify our kriging estimate as,</p>
<div class="math notranslate nohighlight">
\[
y^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot y(\bf{u}_{\alpha}) 
\]</div>
<p>The simple kriging weights are calculated by solving a linear system of equations,</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^n \lambda_j C(\bf{u}_i,\bf{u}_j) = C(\bf{u},\bf{u}_i), \quad i=1,\ldots,n
\]</div>
<p>that may be represented with matrix notation as,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
C(\bf{u}_1,\bf{u}_1) &amp; C(\bf{u}_1,\bf{u}_2) &amp; \dots &amp; C(\bf{u}_1,\bf{u}_n) \\
C(\bf{u}_2,\bf{u}_1) &amp; C(\bf{u}_2,\bf{u}_2) &amp; \dots &amp; C(\bf{u}_2,\bf{u}_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
C(\bf{u}_n,\bf{u}_1) &amp; C(\bf{u}_n,\bf{u}_2) &amp; \dots &amp; C(\bf{u}_n,\bf{u}_n) \\
\end{bmatrix} \cdot 
\begin{bmatrix} 
\lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \\
\end{bmatrix} = 
\begin{bmatrix} 
C(\bf{u}_1,\bf{u}) \\ C(\bf{u}_2,\bf{u})  \\ \vdots \\ C(\bf{u}_n,\bf{u})  \\
\end{bmatrix}
\end{split}\]</div>
<p>This system may be derived by substituting the equation for kriging estimates into the equation for estimation variance, and then setting the partial derivative with respect to the weights to zero.</p>
<ul class="simple">
<li><p>we are optimizing the weights to minimize the estimation variance</p></li>
</ul>
<p>this system integrates the,</p>
<ul class="simple">
<li><p><em>spatial continuity</em> as quantified by the variogram (and covariance function to calculate the covariance, <span class="math notranslate nohighlight">\(C\)</span>, values)</p></li>
<li><p><em>redundancy</em> the degree of spatial continuity between all of the available data with themselves, <span class="math notranslate nohighlight">\(C(\bf{u}_i,\bf{u}_j)\)</span></p></li>
<li><p><em>closeness</em> the degree of spatial continuity between the available data and the estimation location, <span class="math notranslate nohighlight">\(C(\bf{u}_i,\bf{u})\)</span></p></li>
</ul>
<p>Kriging provides a measure of estimation accuracy known as kriging variance (a specific case of estimation variance).</p>
<div class="math notranslate nohighlight">
\[
\sigma^{2}_{E}(\bf{u}) = C(0) - \sum^{n}_{\alpha = 1} \lambda_{\alpha} C(\bf{u}_0 - \bf{u}_{\alpha})
\]</div>
<p>Kriging estimates are best in that they minimize the above estimation variance.</p>
<p>Properties of kriging estimates include,</p>
<ul class="simple">
<li><p><em>Exact interpolator</em> - kriging estimates with the data values at the data locations</p></li>
<li><p><em>Kriging variance</em> - a measure of uncertainty in a kriging estimate. Can be calculated before getting the sample information, as the kriging estimation variance is not dependent on the values of the data nor the kriging estimate, i.e. the kriging estimator is homoscedastic.</p></li>
<li><p><em>Spatial context</em> - kriging takes integrates spatial continuity, closeness and redundancy; therefore, kriging accounts for the configuration of the data and structural continuity of the feature being estimated.</p></li>
<li><p><em>Scale</em> - kriging by default assumes the estimate and data are at the same point support, i.e., mathematically represented as points in space with zero volume. Kriging may be generalized to account for the support volume of the data and estimate,</p></li>
<li><p><em>Multivariate</em> - kriging may be generalized to account for multiple secondary data in the spatial estimate with the cokriging system. We will cover this later.</p></li>
<li><p><em>Smoothing effect</em> - of kriging can be forecasted as the missing variance. The missing variance over local estimates is the kriging variance.</p></li>
</ul>
</section>
<section id="kriging-based-declustering">
<h2><strong>Kriging-based Declustering</strong><a class="headerlink" href="#kriging-based-declustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: a declustering method to assign weights to spatial samples based on local sampling density, such that the weighted statistics are likely more representative of the population. Data weights are assigned so that,</p>
<ul class="simple">
<li><p>samples in densely sampled areas receive less weight</p></li>
<li><p>samples in sparsely sampled areas receive more weight</p></li>
</ul>
<p>Kriging-based declustering proceeds as follows:</p>
<ol class="arabic simple">
<li><p>calculate and model the experimental variogram</p></li>
<li><p>apply kriging to calculate estimates over a high-resolution grid covering the area of interest</p></li>
<li><p>calculate the sum of the weights assigned to each data</p></li>
<li><p>assign data weights proportional to this sum of weights</p></li>
</ol>
<p>The weights are calculated as:</p>
<div class="math notranslate nohighlight">
\[
w(\bf{u}_j) =   n \cdot \frac{\sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_j}{\sum_{i=1}^n \left[ \sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_{j,ix,iy} \right]}
\]</div>
<p>where <span class="math notranslate nohighlight">\(nx\)</span> and <span class="math notranslate nohighlight">\(ny\)</span> are the number of cells in the grid, <span class="math notranslate nohighlight">\(n\)</span> is the number of data, and <span class="math notranslate nohighlight">\(\lambda_{j,ix,iy}\)</span> is the weight assigned to the <span class="math notranslate nohighlight">\(j\)</span> data at the <span class="math notranslate nohighlight">\(ix,iy\)</span> grid cell.</p>
<p>Here is an important point for kriging-based declustering,</p>
<ul class="simple">
<li><p>like polygonal declustering, kriging-based declustering is sensitive to the boundaries of the area of interest; therefore, the weights assigned to the data near the boundary of the area of interest may change radically as the area of interest is expanded or contracted</p></li>
</ul>
<p>Also, kriging-based declustering integrates the spatial continuity model from variogram model. Consider the following possible impacts of the variogram model on the declustering weights,</p>
<ul class="simple">
<li><p>if there is 100% relative nugget effect, there is no spatial continuity and therefore, all data receives equal weight. Note for the equation above this results in a divide by 0.0 error that must be checked for in the code.</p></li>
<li><p>geometric anisotropy may significantly impact the weights as data aligned over specific azimuths are assessed as closer or further in terms of covariance</p></li>
</ul>
</section>
<section id="kolmogorovs-3-probability-axioms">
<h2><strong>Kolmogorov’s 3 Probability Axioms</strong><a class="headerlink" href="#kolmogorovs-3-probability-axioms" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Probability Concepts</span>: these are Kolmogorov’s 3 axioms for valid probabilities,</p>
<ol class="arabic simple">
<li><p>Probability of an event is a non-negative number.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P(𝐴) \ge 0
\]</div>
<ol class="arabic simple" start="2">
<li><p>Probability of the entire sample space, all possible outcomes, <span class="math notranslate nohighlight">\(\Omega\)</span>, is one (unity), also known as probability closure.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P(\Omega) = 1
\]</div>
<ol class="arabic simple" start="3">
<li><p>Additivity of mutually exclusive events for unions.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P\left(⋃_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)
\]</div>
<p>e.g., probability of <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span> mutual exclusive events is, <span class="math notranslate nohighlight">\(P(A_1 + A_2) = P(A_1) + P(A_2)\)</span></p>
</section>
<section id="l-1-norm">
<h2><strong><span class="math notranslate nohighlight">\(L^1\)</span> Norm</strong><a class="headerlink" href="#l-1-norm" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_linear_regression.html"><span class="doc std std-doc">Linear Regression</span></a>: known as Manhattan norm or sum of absolute residual (SAR),</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n |\Delta y_i |
\]</div>
<p>also expressed as the mean absolute error (MAE),</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n |\Delta y_i |
\]</div>
<p>Minimization with <span class="math notranslate nohighlight">\(L^1\)</span> norm is known as minimum absolute difference.</p>
</section>
<section id="l-2-norm">
<h2><strong><span class="math notranslate nohighlight">\(L^2\)</span> Norm</strong><a class="headerlink" href="#l-2-norm" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_linear_regression.html"><span class="doc std std-doc">Linear Regression</span></a>: known as sum of square residual (SSR),</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \sqrt{\Delta y_i}
\]</div>
<p>also expressed as the mean square error (MSE),</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n \left( \Delta y_i \right)^2
\]</div>
<p>and the Euclidian norm,</p>
<div class="math notranslate nohighlight">
\[
\sqrt{ \sum_{i=1}^n \sqrt{\Delta y_i} }
\]</div>
<p>Minimization with <span class="math notranslate nohighlight">\(L^2\)</span> norm is known as the method of least squares.</p>
</section>
<section id="l-1-vs-l-2-norm">
<h2><strong><span class="math notranslate nohighlight">\(L^1\)</span> vs. <span class="math notranslate nohighlight">\(L^2\)</span> Norm</strong><a class="headerlink" href="#l-1-vs-l-2-norm" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_LASSO_regression.html"><span class="doc std std-doc">LASSO Regression</span></a>: the choice of <span class="math notranslate nohighlight">\(L^1\)</span> and <span class="math notranslate nohighlight">\(L^2\)</span> norm is important in machine learning. To explain this let’s compare the performance of <span class="math notranslate nohighlight">\(L^1\)</span> and <span class="math notranslate nohighlight">\(L^2\)</span> norms in loss functions while training model parameters.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head text-center"><p>Least Absolute Deviations (L1)</p></th>
<th class="head text-center"><p>Least Squares (L2)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Robustness*</p></td>
<td class="text-center"><p>Robust</p></td>
<td class="text-center"><p>Not very robust</p></td>
</tr>
<tr class="row-odd"><td><p>Solution Stability</p></td>
<td class="text-center"><p>Unstable solution</p></td>
<td class="text-center"><p>Stable solution</p></td>
</tr>
<tr class="row-even"><td><p>Number of Solutions</p></td>
<td class="text-center"><p>Possibly multiple solutions</p></td>
<td class="text-center"><p>Always one solution</p></td>
</tr>
<tr class="row-odd"><td><p>Feature Selection</p></td>
<td class="text-center"><p>Built-in feature selection</p></td>
<td class="text-center"><p>No feature selection</p></td>
</tr>
<tr class="row-even"><td><p>Output Sparsity</p></td>
<td class="text-center"><p>Sparse outputs</p></td>
<td class="text-center"><p>Non-sparse outputs</p></td>
</tr>
<tr class="row-odd"><td><p>Analytical Solutions</p></td>
<td class="text-center"><p>No analytical solutions</p></td>
<td class="text-center"><p>Analytical solutions</p></td>
</tr>
</tbody>
</table>
<p>Here’s some important points,</p>
<ul class="simple">
<li><p><em>robust</em> - resistant to outliers</p></li>
<li><p><em>unstable</em> - for small changes in training the trained model predictions may jump</p></li>
<li><p><em>multiple solutions</em> - different solution have similar or the same loss, resulting in solutions jumping with small changes to the training data</p></li>
<li><p><em>output sparsity</em> and <em>feature selection</em> - model parameters tend to 0.0</p></li>
<li><p><em>analytical solutions</em> - an analytical solution is available to solve for the optimum model parameters</p></li>
</ul>
</section>
<section id="l-1-or-l-2-normalizer">
<h2><strong><span class="math notranslate nohighlight">\(L^1\)</span> or <span class="math notranslate nohighlight">\(L^2\)</span> Normalizer</strong><a class="headerlink" href="#l-1-or-l-2-normalizer" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: is performed across features over individual samples to constrain the sum</p>
<p>The L1 Norm has the following constraint across samples,</p>
<div class="math notranslate nohighlight">
\[
\sum_{\alpha = 1}^m x^{\prime}_{i,\alpha} = 1.0, \quad i = 1, \ldots, n
\]</div>
<p>The L1 normalizer transform,</p>
<div class="math notranslate nohighlight">
\[
x^{\prime}_{i,\alpha} = \frac{x_{i,\alpha}}{\sum_{\alpha=1}^m x_{i,\alpha}}
\]</div>
<p>The L2 Norm has the following constraint across samples,</p>
<div class="math notranslate nohighlight">
\[
\sum_{\alpha = 1}^m \left( x^{\prime}_{i,\alpha} \right)^2 = 1.0, \quad i = 1, \ldots, n
\]</div>
<p>The L2 normalizer transform,</p>
<div class="math notranslate nohighlight">
\[
x^{\prime}_{i,\alpha} = \sqrt{\frac{(x_{i,\alpha})^2}{\sum_{\alpha=1}^m (x_{i,\alpha})^2}}
\]</div>
<p>Example, applied in text classification and clustering, and L1 for compositional data (sum 1.0 constraint)</p>
</section>
<section id="lasso-regression">
<h2><strong>LASSO Regression</strong><a class="headerlink" href="#lasso-regression" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_LASSO_regression.html"><span class="doc std std-doc">LASSO Regression</span></a>: linear regression with <span class="math notranslate nohighlight">\(L^1\)</span> regularization term and regularization hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m |b_{\alpha}|
\]</div>
<p>As a result, LASSO regression training integrates two and often competing goals to find the model parameters,</p>
<ul class="simple">
<li><p>find the model parameters that minimize the error with training data</p></li>
<li><p>minimize the slope parameters towards zero</p></li>
</ul>
<p>The only difference between LASSO and ridge regression is:</p>
<ul class="simple">
<li><p>for LASSO the shrinkage term is posed as an <span class="math notranslate nohighlight">\(\ell_1\)</span> penalty,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda \sum_{\alpha=1}^m |b_{\alpha}|
\]</div>
<ul class="simple">
<li><p>for ridge regression the shrinkage term is posed as an <span class="math notranslate nohighlight">\(\ell_2\)</span> penalty,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\lambda \sum_{\alpha=1}^m \left(b_{\alpha}\right)^2
\]</div>
<p>While both ridge regression and the LASSO shrink the model parameters (<span class="math notranslate nohighlight">\(b_{\alpha}, \alpha = 1,\ldots,m\)</span>) towards zero:</p>
<ul class="simple">
<li><p>LASSO parameters reach zero at different rates for each predictor feature as the lambda, <span class="math notranslate nohighlight">\(\lambda\)</span>, hyperparameter increases.</p></li>
<li><p>as a result LASSO provides a method for feature ranking and selection!</p></li>
</ul>
<p>The lambda, <span class="math notranslate nohighlight">\(\lambda\)</span>, hyperparameter controls the degree of fit of the model and may be related to the model bias-variance trade-off.</p>
<ul class="simple">
<li><p>for <span class="math notranslate nohighlight">\(\lambda \rightarrow 0\)</span> the prediction model approaches linear regression, there is lower model bias, but the model variance is higher</p></li>
<li><p>as <span class="math notranslate nohighlight">\(\lambda\)</span> increases the model variance decreases and the model bias increases</p></li>
<li><p>for <span class="math notranslate nohighlight">\(\lambda \rightarrow \infty\)</span> the coefficients all become 0.0 and the model is the training data response feature mean</p></li>
</ul>
</section>
<section id="lazy-learning">
<h2><strong>Lazy Learning</strong><a class="headerlink" href="#lazy-learning" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_knearest_neighbours.html"><span class="doc std std-doc">k-Nearest Neighbours</span></a>: model is a generalization of the training data and calculation is delayed until query is made of the model</p>
<ul class="simple">
<li><p>the model is the training data and selected hyperparameters, to make new predictions the training data must be available</p></li>
</ul>
<p>The opposite is eager learning.</p>
</section>
<section id="learning-rate-gradient-boosting">
<h2><strong>Learning Rate</strong> (gradient boosting)<a class="headerlink" href="#learning-rate-gradient-boosting" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_gradient_boosting.html"><span class="doc std std-doc">Gradient Boosting</span></a>: controls the rate of updating with each new model.</p>
<div class="math notranslate nohighlight">
\[
f_m = f_{m-1} - \rho_m \frac{\partial L(y_\alpha, F(X_\alpha))}{\partial F(X_\alpha)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho_m\)</span> is the learning rate, \frac{\partial L(y_\alpha, F(X_\alpha))}{\partial F(X_\alpha)} is the gradient, error, <span class="math notranslate nohighlight">\(f_{m-1}\)</span> is the previous estimate, and <span class="math notranslate nohighlight">\(f_m\)</span> is the new estimate.</p>
<p>Some salient points about learning rate,</p>
<ul class="simple">
<li><p>without learning rate, the boosting models learn too quickly and will have too high model variance</p></li>
<li><p>slow down learning for a more robust model, balanced to ensure good performance, too small rate will require very large number of models to reach convergence</p></li>
</ul>
</section>
<section id="likewise-deletion-mrmr">
<h2><strong>Likewise Deletion</strong> (MRMR)<a class="headerlink" href="#likewise-deletion-mrmr" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: removal of any sample with any missing feature values</p>
<ul class="simple">
<li><p>if missing feature values are not missing at random (MAR) this may impart a bias in the data</p></li>
<li><p>will result in a decrease in the effective data size and increase in model uncertainty</p></li>
</ul>
</section>
<section id="linear-regression">
<h2><strong>Linear Regression</strong><a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_linear_regression.html"><span class="doc std std-doc">Linear Regression</span></a>: a linear, parametric prediction model,</p>
<div class="math notranslate nohighlight">
\[
y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0
\]</div>
<p>The analytical solution for the model parameters, <span class="math notranslate nohighlight">\(b_1,\ldots,b_m,b_0\)</span>, is available for the L2 norm loss function, the errors are summed and squared known a least squares.</p>
<ul class="simple">
<li><p>we minimize the error, residual sum of squares (RSS) over the training data:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual response feature values and <span class="math notranslate nohighlight">\(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\)</span> are the model predictions, over the <span class="math notranslate nohighlight">\(\alpha = 1,\ldots,n\)</span> training data.</p>
<ul class="simple">
<li><p>this may be simplified as the sum of square error over the training data,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n (\Delta y_i)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta y_i\)</span> is actual response feature observation <span class="math notranslate nohighlight">\(y_i\)</span> minus the model prediction <span class="math notranslate nohighlight">\(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\)</span>, over the <span class="math notranslate nohighlight">\(i = 1,\ldots,n\)</span> training data.</p>
<p>There are important assumption with our linear regression model,</p>
<ul class="simple">
<li><p><em>Error-free</em> - predictor variables are error free, not random variables</p></li>
<li><p><em>Linearity</em> - response is linear combination of feature(s)</p></li>
<li><p><em>Constant Variance</em> - error in response is constant over predictor(s) value</p></li>
<li><p><em>Independence of Error</em> - error in response are uncorrelated with each other</p></li>
<li><p><em>No multicollinearity</em> - none of the features are redundant with other features</p></li>
</ul>
</section>
<section id="location-map">
<h2><strong>Location Map</strong><a class="headerlink" href="#location-map" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_plotting_data_models.html"><span class="doc std std-doc">Loading and Plotting Data and Models</span></a>: a data plot where the 2 axes are locations, e.g., <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, Easting and Northing, Latitude and Longitude, etc., to show the locations and magnitudes of the spatial data.</p>
<ul class="simple">
<li><p>often the data points are colored to represent the scale of feature to visualize the sampled feature over the area or volume of interest</p></li>
<li><p>advantage, visualize the data without any model that may bias our impression of the data</p></li>
<li><p>disadvantage, may be difficult to visualize large datasets and data in 3D</p></li>
</ul>
</section>
<section id="loss-function">
<h2><strong>Loss Function</strong><a class="headerlink" href="#loss-function" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_LASSO_regression.html"><span class="doc std std-doc">LASSO Regression</span></a>: the equation that is minimized to train the model parameters. For example, the loss function for linear regression includes residual sum of square, the <span class="math notranslate nohighlight">\(L^2\)</span> error norm,</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)^2
\]</div>
<p>for LASSO regression the loss function includes residual sum of square, the <span class="math notranslate nohighlight">\(L^2\)</span> error norm, plus a <span class="math notranslate nohighlight">\(L^1\)</span> regularization term,</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m |b_{\alpha}|
\]</div>
<p>for k-means clustering the loss function is,</p>
<div class="math notranslate nohighlight">
\[ 
I = \sum^k_{i=1} \sum_{\alpha \in C_i} \sqrt{ \sum_{j = 1}^m X_{\alpha,m} - \mu_{i,m} }
\]</div>
<p>The method to minimize loss functions depends on the type of norm,</p>
<ul class="simple">
<li><p>with <span class="math notranslate nohighlight">\(L^2\)</span> norms we apply differentiation to the loss function with respect to the model parameter and set it equal to zero</p></li>
<li><p>with <span class="math notranslate nohighlight">\(L^1\)</span> norms in our loss functions we lose access to an analytical solution and use iterative optimization, e.g., steepest descent</p></li>
</ul>
</section>
<section id="machine-learning-workflow-design">
<h2><strong>Machine Learning Workflow Design</strong><a class="headerlink" href="#machine-learning-workflow-design" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Machine Learning Workflow Construction and Coding</span>: is based on the following steps,</p>
<ol class="arabic simple">
<li><p><em>Specify the Goals</em> - for example,</p></li>
</ol>
<ul class="simple">
<li><p>build a numerical model</p></li>
<li><p>evaluate different recovery processes</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><em>Specify the Data</em> - what is available and what is missing?</p></li>
<li><p><em>Design a Set of Steps to Accomplish the Goal</em> - common steps include,</p></li>
</ol>
<ul class="simple">
<li><p>load data</p></li>
<li><p>format, check and clean data</p></li>
<li><p>run operation, including, statistical calculation, model or visualization</p></li>
<li><p>transfer function</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><em>Develop Documentation</em> - including implementation details, defense of decisions, metadata, limitations and future work</p></li>
<li><p><em>Flow</em> - data and information flow, learning while modeling with branches and loop backs</p></li>
<li><p><em>Uncertainty</em> - summarize all uncertainty sources, include methods to integrate uncertainty, defend the uncertainty models and aspects deemed certain</p></li>
</ol>
</section>
<section id="margin-support-vector-machines">
<h2><strong>Margin</strong> (support vector machines)<a class="headerlink" href="#margin-support-vector-machines" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_support_vector_machines.html"><span class="doc std std-doc">Support Vector Machines</span></a>: when the training data include overlapping categories it would not be possible, nor desirable, to develop a decision boundary that perfectly separates the categories for which this condition would hold,</p>
<div class="math notranslate nohighlight">
\[
y_i \left( x_i^T \beta + \beta_0 \right) \geq 0
\]</div>
<p>We need a model that allows for some misclassification.</p>
<div class="math notranslate nohighlight">
\[
y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i
\]</div>
<p>We introduce the concept of a margin, <span class="math notranslate nohighlight">\(𝑀\)</span>, and a distance from the margin (error, 𝜉_𝑖).</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M^2} + C \sum_{i=1}^N \xi_i \right)
\]</div>
<p>The loss function includes the margin term, <span class="math notranslate nohighlight">\(M\)</span>, and hence attempts to minimize margin while minimizing classification error weighted by hyperparameter, <span class="math notranslate nohighlight">\(C\)</span>.</p>
</section>
<section id="marginal-probability">
<h2><strong>Marginal Probability</strong><a class="headerlink" href="#marginal-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: probability that considers only one event occurring, the probability of <span class="math notranslate nohighlight">\(A\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(A)
\]</div>
<p>marginal probabilities may be calculated from joint probabilities through the process of marginalization,</p>
<div class="math notranslate nohighlight">
\[
P(A) = \int_{-\infty}^{\infty} P(A,B) dB
\]</div>
<p>where we integrate over all cases of the other event, <span class="math notranslate nohighlight">\(B\)</span>, to remove its influence. Given discrete possible cases of event <span class="math notranslate nohighlight">\(B\)</span> we can simply sum the probabilities over all possible cases of <span class="math notranslate nohighlight">\(B\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(A) = \sum_{i=1}^{k_B} P(A,B) dB
\]</div>
</section>
<section id="matrix-scatter-plots">
<h2><strong>Matrix Scatter Plots</strong><a class="headerlink" href="#matrix-scatter-plots" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_multivariate_analysis.html"><span class="doc std std-doc">Multivariate Analysis</span></a>: composite plot including the combinatorial of all pair-wise scatter plots for all features.</p>
<ul class="simple">
<li><p>given <span class="math notranslate nohighlight">\(m\)</span> features, there are <span class="math notranslate nohighlight">\(m \times m\)</span> scatter plots</p></li>
<li><p>the scatter plots are ordered, y-axis feature from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> over the rows and x-axis feature from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> over the columns</p></li>
<li><p>the diagonal is the features plotted with themselves and are often replaced with feature histograms or probability density functions</p></li>
</ul>
<p>We use matrix scatter plots to,</p>
<ul class="simple">
<li><p>look for bivariate linear or nonlinear structures</p></li>
<li><p>look for bivariate homoscedasticity (constant conditional variance) and heteroscedasticity (conditional variance changes with value)</p></li>
<li><p>look for bivariate constraints, such as sum constraints with compositional data</p></li>
</ul>
<p>Remember, the other features are marginalized, this is not a full m-D visualization.</p>
</section>
<section id="maximum-relevance-minimum-redundancy-mrmr">
<h2><strong>Maximum Relevance Minimum Redundancy</strong> (MRMR)<a class="headerlink" href="#maximum-relevance-minimum-redundancy-mrmr" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: a mutual information-based approach for feature ranking that accounts for feature relevance and redundancy.</p>
<ul class="simple">
<li><p>one example is a relevance minus redundancy summary,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
MRMR = max \left[ frac{1}{|S|} \sum_{X_i \in S} I(X_i,Y) - \frac{1}{|S|^2} \sum_{X_i \in S} \sum_{X_j, i \ne j} I(X_i,X_j) \right] 
\]</div>
<p>where <span class="math notranslate nohighlight">\(𝑆\)</span> is the predictor feature subset and <span class="math notranslate nohighlight">\(|𝑆|\)</span> is the number of features in the subset <span class="math notranslate nohighlight">\(𝑆\)</span>.</p>
</section>
<section id="metropolis-hastings-mcmc-sampler">
<h2><strong>Metropolis-Hastings MCMC Sampler</strong><a class="headerlink" href="#metropolis-hastings-mcmc-sampler" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html"><span class="doc std std-doc">Bayesian Linear Regression</span></a>: The basic steps of the Metropolis-Hastings MCMC Sampler:</p>
<p>For <span class="math notranslate nohighlight">\(\ell = 1, \ldots, L\)</span>:</p>
<ol class="arabic simple">
<li><p>Assign random values for the initial sample of model parameters, <span class="math notranslate nohighlight">\(\beta(\ell = 1) = b_1(\ell = 1)\)</span>, <span class="math notranslate nohighlight">\(b_0(\ell = 1)\)</span> and <span class="math notranslate nohighlight">\(\sigma^2(\ell = 1)\)</span>.</p></li>
<li><p>Propose new model parameters based on a proposal function, <span class="math notranslate nohighlight">\(\beta^{\prime} = b_1\)</span>, <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
<li><p>Calculate probability of acceptance of the new proposal, as the ratio of the posterior probability of the new model parameters given the data to the previous model parameters given the data multiplied by the probability of the old step given the new step divided by the probability of the new step given the old.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X) }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right)
\]</div>
<ol class="arabic simple" start="4">
<li><p>Apply Monte Carlo simulation to conditionally accept the proposal, if accepted, <span class="math notranslate nohighlight">\(\ell = \ell + 1\)</span>, and sample <span class="math notranslate nohighlight">\(\beta(\ell) = \beta^{\prime}\)</span></p></li>
<li><p>Go to step 2.</p></li>
</ol>
</section>
<section id="minkowski-distance">
<h2><strong>Minkowski Distance</strong><a class="headerlink" href="#minkowski-distance" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_knearest_neighbours.html"><span class="doc std std-doc">k-Nearest Neighbours</span></a>: a general expression for distance with well-known Manhattan and Euclidean distances are special cases,</p>
<div class="math notranslate nohighlight">
\[
d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p \right)^{\frac{1}{p}}
\]</div>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(p=2\)</span>, this becomes the Euclidean distance</p></li>
<li><p>when <span class="math notranslate nohighlight">\(p=1\)</span> it becomes the Manhattan distance</p></li>
</ul>
</section>
<section id="missing-feature-values">
<h2><strong>Missing Feature Values</strong><a class="headerlink" href="#missing-feature-values" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_imputation.html"><span class="doc std std-doc">Feature Imputation</span></a>: null values in the data table, samples that do not have values for all features</p>
<p>There are many causes of missing feature values, for example,</p>
<ol class="arabic simple">
<li><p>sampling cost, e.g., low permeability test takes too long</p></li>
<li><p>rock rheology sample filter, e.g., can’t recover the mudstone samples</p></li>
<li><p>sampling to reduce uncertainty and maximize profitability instead of statistical representativity, dual purpose samples for information and production</p></li>
</ol>
<p>Missing data consequences, more than reducing the amount of training and testing data, missing data, if not completely at random may result in,</p>
<ul class="simple">
<li><p>biased sample statistics resulting in biased model training and testing</p></li>
<li><p>biased models with biased predictions with potentially no indication of the bias</p></li>
</ul>
</section>
<section id="missing-at-random-mar">
<h2><strong>Missing at Random</strong> (MAR)<a class="headerlink" href="#missing-at-random-mar" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_imputation.html"><span class="doc std std-doc">Feature Imputation</span></a>: missing feature values are distributed randomly, uniform coverage over the predictor feature space, i.e., all values have likelihood to be missing, and no correlation between missing feature values.</p>
<p>This is typically not the case as missing data often has a confounding feature, for example,</p>
<ol class="arabic simple">
<li><p>sampling cost, e.g., low permeability test takes too long</p></li>
<li><p>rock rheology sample filter, e.g., can’t recover the mudstone samples</p></li>
<li><p>sampling to reduce uncertainty and maximize profitability instead of statistical representativity, dual purpose samples for information and production</p></li>
</ol>
<p>Missing data consequences, more than reducing the amount of training and testing data, missing data, if not completely at random may result in,</p>
<ul class="simple">
<li><p>biased sample statistics resulting in biased model training and testing</p></li>
<li><p>biased models with biased predictions with potentially no indication of the bias</p></li>
</ul>
</section>
<section id="model-bias">
<h2><strong>Model Bias</strong><a class="headerlink" href="#model-bias" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: is error due to insufficient complexity and flexibility to fit the natural setting</p>
<ul class="simple">
<li><p>increasing model complexity usually results in decreasing model bias</p></li>
<li><p><em>model bias variance trade-off</em> - as complexity increases, model variance increases and model bias decreases</p></li>
<li><p>one of the three components of expected test square error, including model variance, model bias and irreducible error</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2
+
\]</div>
<div class="math notranslate nohighlight">
\[
E \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0, \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\left(E [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2\)</span> is model bias.</p>
</section>
<section id="model-bias-variance-trade-off">
<h2><strong>Model-Bias Variance Trade-off</strong><a class="headerlink" href="#model-bias-variance-trade-off" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: as complexity increases, model variance increases and model bias decreases.</p>
<ul class="simple">
<li><p>as model variance and model bias are both components of expected test square error, the balancing of model bias and model variance results in an optimum level of complexity to minimize the testing error</p></li>
</ul>
</section>
<section id="model-checking">
<h2><strong>Model Checking</strong><a class="headerlink" href="#model-checking" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: is a critical last step for any spatial modeling workflow. Here are the critical aspects of model checking,</p>
<ol class="arabic simple">
<li><p><em>Model Inputs</em> - data and statistics integration</p></li>
</ol>
<ul class="simple">
<li><p>check the model to ensure the model inputs are honored in the models, generally checked over all the realizations, for example, the output histograms and matches the input histogram over the realizations</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><em>Accurate Spatial Estimates</em> - ability of the model to accurately predict away from the available sample data, over a variety of configurations, with accuracy</p></li>
</ol>
<ul class="simple">
<li><p>by cross validation, withholding some of the data, check the model’s ability to predict</p></li>
<li><p>generally, summarized with a truth vs. predicted cross plot and measures such as mean square error</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
MSE = \frac{1}{n} \sum_{\alpha = 1}^{n} \left(z^{*}(\bf{u}_{\alpha}) - z(\bf{u}_{\alpha}) \right)^2
\]</div>
<ol class="arabic simple" start="3">
<li><p><em>Accurate and Precise Uncertainty Models</em> - uncertainty model is fair given the amount of information available and various sources of uncertainty</p></li>
</ol>
<ul class="simple">
<li><p>also checked through cross validation, withholding some of the data, but by checking the proportion of the data in specific probability intervals</p></li>
<li><p>summarized with a proportion of withheld data in interval vs. the probability interval</p></li>
<li><p>points on the 45 degree line indicate accurate and precise uncertainty model</p></li>
<li><p>points above the 45 degree line indicate accurate and imprecise uncertainty model, uncertainty is too wide</p></li>
<li><p>points below the 45 degree line indicate inaccurate uncertainty model, uncertainty is too narrow or model is biased</p></li>
</ul>
</section>
<section id="model-complexity-or-flexibility">
<h2><strong>Model Complexity or Flexibility</strong><a class="headerlink" href="#model-complexity-or-flexibility" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the ability of a model to fit to data and to be interpreted.</p>
<p>A variety of concepts may be used to describe model complexity,</p>
<ul class="simple">
<li><p>the number of features, predictor variables are in the model, dimensionality of the model, usually resulting in more model parameters</p></li>
<li><p>the number of parameters, the order applied for each term, e.g. linear, quadratic, thresholds</p></li>
<li><p>the format of the model, i.e., a compact equation with polynomial regression vs. nested conditional statements with decision tree vs. thousands of weights and bias model parameters for a neural network</p></li>
<li><p>For example, more complexity with a high order polynomial, larger decision trees etc.</p></li>
</ul>
<p>In general, more complicated or flexible models are more difficult to interpret,</p>
<ul class="simple">
<li><p>linear regression and the associated model parameters can be analyzed and even applied for feature ranking, while support vector machines with radial basis functions are a linear model in the nD high dimensional space</p></li>
</ul>
</section>
<section id="model-generalization">
<h2><strong>Model Generalization</strong><a class="headerlink" href="#model-generalization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the ability of a model to predict away from training data.</p>
<ul class="simple">
<li><p>the model learns the structure in the data and does not just memorize the training data</p></li>
</ul>
<p>Models that do not generalize well,</p>
<ul class="simple">
<li><p>overfit models have high accuracy at training data and low accuracy away from training data, demonstrated with low testing accuracy</p></li>
<li><p>underfit models are too simple or inflexible for the natural phenomenon and have low training and testing accuracy</p></li>
</ul>
</section>
<section id="model-hyperparameters">
<h2><strong>Model Hyperparameters</strong><a class="headerlink" href="#model-hyperparameters" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: constrain the model complexity. Hyperparameters are tuned to maximize accuracy with the withheld testing data to prevent model overfit.</p>
<p>For a set of polynomial models from <span class="math notranslate nohighlight">\(4^{th}\)</span> to <span class="math notranslate nohighlight">\(1^{st}\)</span> order,</p>
<div class="math notranslate nohighlight">
\[
y = b_4 \cdot x^4 + b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0
\]</div>
<div class="math notranslate nohighlight">
\[
y = b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0
\]</div>
<div class="math notranslate nohighlight">
\[
y = b_2 \cdot x^2 + b_1 \cdot x + b_0
\]</div>
<div class="math notranslate nohighlight">
\[
y = b_1 \cdot x + b_0
\]</div>
<p>the choice of polynomial order is the hyperparameter, i.e., the first order model is most simple and the fourth order model is most complicated.</p>
</section>
<section id="model-parameters">
<h2><strong>Model Parameters</strong><a class="headerlink" href="#model-parameters" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: trainable coefficients for a machine learning model that control the fit to the training data.</p>
<p>For a polynomial model,</p>
<div class="math notranslate nohighlight">
\[
y = b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0
\]</div>
<p><span class="math notranslate nohighlight">\(b_3\)</span>, <span class="math notranslate nohighlight">\(b_2\)</span>, <span class="math notranslate nohighlight">\(b_1\)</span>, and <span class="math notranslate nohighlight">\(b_0\)</span> are model parameters.</p>
<ul class="simple">
<li><p><em>training model parameters</em> - model parameters are calculated by optimization to minimize error and regularization terms over the training data through analytical solution or iterative solution, e.g., gradient descent optimization</p></li>
</ul>
</section>
<section id="model-regularization">
<h2>Model Regularization<a class="headerlink" href="#model-regularization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_ridge_regression.html"><span class="doc std std-doc">Ridge Regression</span></a>: adding information to prevent overfit (or underfit), improve model generalization.</p>
<ul class="simple">
<li><p>this information is known as a regularization term</p></li>
<li><p>this represents a penalty for complexity that is tuned with a regularization hyperparameter</p></li>
</ul>
<p>Consider the ridge regression loss function,</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda \sum_{j=1}^m b_{\alpha}^2\)</span> is the regularization term and <span class="math notranslate nohighlight">\(\lambda\)</span> is the regularization hyperparameter.</p>
<p>The concept of regularization is quite general and choices in machine learning architecture, such as,</p>
<ul class="simple">
<li><p>use of receptive fields for convolutional neural networks (CNNs)</p></li>
<li><p>the choice to limit decision trees to a maximum number of levels.</p></li>
</ul>
<p>There are a couple of useful perspectives on model regularization,</p>
<ul class="simple">
<li><p><em>Occam’s razor</em> - regularization tunes model complexity to the simplest effective solution</p></li>
<li><p><em>Bayesian perspective</em> - regularization is imposing a prior on the solution.</p></li>
</ul>
</section>
<section id="model-variance">
<h2><strong>Model Variance</strong><a class="headerlink" href="#model-variance" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: is error due to sensitivity to the dataset</p>
<ul class="simple">
<li><p>increasing model complexity usually results in increasing model variance</p></li>
<li><p>ensemble machine learning, for example, model bagging reduce model variance by averaging over multiple estimators trained on bootstrap realizations of the dataset</p></li>
<li><p><em>model bias variance trade-off</em> - as complexity increases, model variance increases and model bias decreases</p></li>
<li><p>one of the three components of expected test square error, including model variance, model bias and irreducible error</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2
+
\]</div>
<div class="math notranslate nohighlight">
\[
E \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0, \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(E \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0, \ldots, x_m,^0) \right] \right)^2 \right]\)</span> is model variance.</p>
</section>
<section id="momentum-optimization">
<h2><strong>Momentum</strong> (optimization)<a class="headerlink" href="#momentum-optimization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_LASSO_regression.html"><span class="doc std std-doc">LASSO Regression</span></a>: update the previous step with the new step, momentum, <span class="math notranslate nohighlight">\(\lambda\)</span>, is the weight applied to the previous step while <span class="math notranslate nohighlight">\(1 - \lambda\)</span> is the weight applied to the current step,</p>
<div class="math notranslate nohighlight">
\[ 
\left( \left( r \cdot \nabla L \right)_{t-1} \right)^m = \lambda \cdot r \cdot \nabla L_{t-2} + (1 - \lambda) \cdot r \cdot \nabla L_{t-1}
\]</div>
<ul class="simple">
<li><p>the gradients calculated from the partial derivatives of the loss function for each model parameter have noise.  Momentum smooths out, reduces the impact of this noise.</p></li>
<li><p>momentum helps the solution proceed down the general slope of the loss function, rather than oscillating in local ravines or dimples</p></li>
</ul>
</section>
<section id="markov-chain-monte-carlo-mcmc">
<h2><strong>Markov Chain Monte Carlo</strong> (MCMC)<a class="headerlink" href="#markov-chain-monte-carlo-mcmc" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html"><span class="doc std std-doc">Bayesian Linear Regression</span></a>: a set of algorithms to sample from a probability distribution such that the samples match the distribution statistics.</p>
<ul class="simple">
<li><p><em>Markov</em> - screening assumption, the next sample is only dependent on the previous sample</p></li>
<li><p><em>Chain</em> - the samples form a sequence often demonstrating a transition from burn-in chain with inaccurate statistics and equilibrium chain with accurate statistics</p></li>
<li><p><em>Monte Carlo</em> - use of Monte Carlo simulation, random sampling from a statistical distribution</p></li>
</ul>
<p>Why is this useful?</p>
<ul class="simple">
<li><p>we often don’t have the target distribution, it is unknown</p></li>
<li><p>but we can sample with the correct frequencies with other form of information such as conditional probability density functions, Gibbs sampler, or the likelihood ratios of the candidate next sample and the current sample, Metropolis-Hastings</p></li>
</ul>
</section>
<section id="metropolis-hastings-sampling-mcmc">
<h2><strong>Metropolis-Hastings Sampling</strong> (MCMC)<a class="headerlink" href="#metropolis-hastings-sampling-mcmc" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html"><span class="doc std std-doc">Bayesian Linear Regression</span></a>: a set of algorithms to sample from a probability distribution such that the samples match the distribution statistics, based on,</p>
<ul class="simple">
<li><p>the likelihood ratios of the candidate next sample and the current sample</p></li>
<li><p>a rejection sampler based on this likelihood ratio</p></li>
</ul>
<p>Since only the ratio of likelihood is required, the system is simplified as the evidence term cancels out from the Bayesian probability</p>
<p>Here’s the basic steps of the Metropolis-Hastings MCMC Sampler:</p>
<p>For <span class="math notranslate nohighlight">\(\ell = 1, \ldots, L\)</span>:</p>
<ol class="arabic simple">
<li><p>Assign random values for the initial sample of model parameters, <span class="math notranslate nohighlight">\(\beta(\ell = 1) = b_1(\ell = 1)\)</span>, <span class="math notranslate nohighlight">\(b_0(\ell = 1)\)</span> and <span class="math notranslate nohighlight">\(\sigma^2(\ell = 1)\)</span>.</p></li>
<li><p>Propose new model parameters based on a proposal function, <span class="math notranslate nohighlight">\(\beta^{\prime} = b_1\)</span>, <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
<li><p>Calculate probability of acceptance of the new proposal, as the ratio of the posterior probability of the new model parameters given the data to the previous model parameters given the data multiplied by the probability of the old step given the new step divided by the probability of the new step given the old.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X) }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right)
\]</div>
<ol class="arabic simple" start="4">
<li><p>Apply Monte Carlo simulation to conditionally accept the proposal, if accepted, <span class="math notranslate nohighlight">\(\ell = \ell + 1\)</span>, and sample <span class="math notranslate nohighlight">\(\beta(\ell) = \beta^{\prime}\)</span></p></li>
<li><p>Go to step 2.</p></li>
</ol>
</section>
<section id="monte-carlo-simulation-mcs">
<h2><strong>Monte Carlo Simulation (MCS)</strong><a class="headerlink" href="#monte-carlo-simulation-mcs" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html"><span class="doc std std-doc">Bayesian Linear Regression</span></a>: a random sample from a statistical distribution, random variable. The steps for MCS are:</p>
<ol class="arabic simple">
<li><p>model the feature cumulative distribution function, <span class="math notranslate nohighlight">\(F_x(x)\)</span></p></li>
<li><p>draw random value from a uniform [0,1] distribution, this is a random cumulative probability value, known as a p-value, <span class="math notranslate nohighlight">\(p^{\ell}\)</span></p></li>
<li><p>apply the inverse of the cumulative distribution function to calculate the associated realization</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
x^{\ell} = F_x^{-1} (p^{\ell})
\]</div>
<ol class="arabic simple" start="4">
<li><p>repeat to calculate enough realizations for the subsequent analysis</p></li>
</ol>
<p>Monte Carlo simulation is the basic building block of stochastic simulation workflows, for example,</p>
<ul class="simple">
<li><p><em>Monte Carlo simulation workflows</em> - apply Monte Carlo simulation many over all features to the transfer function to calculate a realization of the decision criteria, repeated for many realizations, to propagate uncertainty through a transfer function</p></li>
<li><p><em>Bootstrap</em> - applies Monte Carlo simulation to acquire realizations of the data to calculate uncertainty in sample statistics or ensembles of prediction models for ensemble-based machine learning</p></li>
<li><p><em>Monte Carlo methods</em> - applies Monte Carlo simulation to speed up an expensive calculation with a limited random sample that converges on the solution as the number of random samples increases</p></li>
</ul>
</section>
<section id="monte-carlo-simulation-workflow">
<h2><strong>Monte Carlo Simulation Workflow</strong><a class="headerlink" href="#monte-carlo-simulation-workflow" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html"><span class="doc std std-doc">Bayesian Linear Regression</span></a>: a convenient stochastic workflow for propagating uncertainty through a transfer function through sampling with Monte Carlo Simulation (MCS). The workflow includes the following steps,</p>
<ol class="arabic simple">
<li><p>Model all the input features’ distributions, cumulative distribution functions,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
F_{x_1}(x_1), \quad F_{x_2}(x_2), \quad \dots \quad , F_{x_m}(x_m) 
\]</div>
<ol class="arabic simple" start="2">
<li><p>Monte Carlo simulate a realizations for all the inputs,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
x_1^{\ell}, \quad x_2^{\ell}, \quad \ldots \quad , x_m^{\ell}
\]</div>
<ol class="arabic simple" start="3">
<li><p>Apply to the transfer function to get a realization of the transfer function output, often the <em>decision criteria</em></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
y^{\ell} = f \left(x_1^{\ell},x_2^{\ell}, \quad \ldots \quad, x_m^{\ell} \right)
\]</div>
<ol class="arabic simple" start="4">
<li><p>Repeat steps 1-3 to calculate enough realizations to model the transfer function output distribution.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
F_y(y)
\]</div>
</section>
<section id="multiplication-rule-probability">
<h2><strong>Multiplication Rule</strong> (probability)<a class="headerlink" href="#multiplication-rule-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: we can calculate the joint probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as the product of the conditional probability of <span class="math notranslate nohighlight">\(B\)</span> given <span class="math notranslate nohighlight">\(A\)</span> with the marginal probability of <span class="math notranslate nohighlight">\(A\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = P(A,B) = P(B|A) \cdot P(A)
\]</div>
<p>The multiplication rule is derived as a simple manipulation of the definition of conditional probability, in this case,</p>
<div class="math notranslate nohighlight">
\[
P(B|A) = \frac{P(A,B)}{P(A)}
\]</div>
</section>
<section id="mutual-information">
<h2><strong>Mutual Information</strong><a class="headerlink" href="#mutual-information" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: a generalized approach that quantifies the mutual dependence between two features.</p>
<ul>
<li><p>quantifies the amount of information gained from observing one feature about the other</p></li>
<li><p>avoids any assumption about the form of the relationship (e.g. no assumption of linear relationship)</p>
<p>units are Shannons or bits</p>
</li>
<li><p>compares the joint probabilities to the product of the marginal probabilities</p></li>
<li><ul class="simple">
<li><p>summarizes the difference between the joint <span class="math notranslate nohighlight">\(P(x,y)\)</span> and the product of the marginals <span class="math notranslate nohighlight">\(P(x)\cdot P(y)\)</span>, integrated over all <span class="math notranslate nohighlight">\(x \in 𝑋\)</span> and <span class="math notranslate nohighlight">\(y \in Y\)</span>,</p></li>
</ul>
</li>
</ul>
<p>For discrete or binned continuous features <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, mutual information is calculated as:</p>
<div class="math notranslate nohighlight">
\[
I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right)
\]</div>
<p>recall, given independence between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y)
\]</div>
<p>therefore if the two features are independent then the <span class="math notranslate nohighlight">\(log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) = 0\)</span></p>
<p>The joint probability <span class="math notranslate nohighlight">\(P_{X,Y}(x,y)\)</span> is a weighting term on the sum and enforces closure.</p>
<ul class="simple">
<li><p>parts of the joint distribution with greater density have greater impact on the mutual information metric</p></li>
</ul>
<p>For continuous (and nonbinned) features we can applied the integral form.</p>
<div class="math notranslate nohighlight">
\[
I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x) \cdot P_Y(y)} \right) dx dy
\]</div>
</section>
<section id="mutually-exclusive-events-probability">
<h2><strong>Mutually Exclusive Events</strong> (probability)<a class="headerlink" href="#mutually-exclusive-events-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: the events do not intersect, i.e., do not have any common outcomes. We represent this as,</p>
<ul class="simple">
<li><p>using set notation, we state events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are mutually exclusive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset
\]</div>
<ul class="simple">
<li><p>and the probability for mutually exclusive as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(A,B) = 0.0
\]</div>
</section>
<section id="multidimensional-scaling">
<h2><strong>Multidimensional Scaling</strong><a class="headerlink" href="#multidimensional-scaling" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_multidimensional_scaling.html"><span class="doc std std-doc">Multidimensional Scaling</span></a>: a method in inferential statistics / information visualization for exploring / visualizing the similarity (conversely the difference) between individual samples from a high dimensional dataset in a low dimensional space.</p>
<p>Multidimensional scaling (MDS) projects the <span class="math notranslate nohighlight">\(m\)</span> dimensional data to <span class="math notranslate nohighlight">\(p\)</span> dimensions such that <span class="math notranslate nohighlight">\(p &lt;&lt; m\)</span>.</p>
<ul class="simple">
<li><p>while attempting to preserve the pairwise dissimilarity between the data samples</p></li>
<li><p>ideally we are able to project to <span class="math notranslate nohighlight">\(p=2\)</span> to easily explore the relationships between the samples</p></li>
</ul>
<p>While principal component analysis (PCA) operates with the covariance matrix, multidimensional scaling operates with the distance or dissimilarity matrix. For multidimensional scaling,</p>
<ul class="simple">
<li><p>you don’t need to know the actual feature values, just the distance or dissimilarity between the samples</p></li>
<li><p>as with any distance in feature space, we consider feature standardization to ensure that features with larger variance do not dominate the calculation</p></li>
<li><p>we may work with a variety of dissimilarity measures</p></li>
</ul>
<p>Comparison between multidimensional scaling and principal component analysis,</p>
<ul class="simple">
<li><p>principal component analysis takes the covariance matrix (<span class="math notranslate nohighlight">\(m \times m\)</span>) between all the features and finds the linear, orthogonal rotation such that the <em>variance is maximized</em> over the ordered principle components</p></li>
<li><p>multidimensional scaling takes the matrix of the pairwise distances (<span class="math notranslate nohighlight">\(n \times n\)</span>) between all the samples in feature space and finds the nonlinear projection such that the <em>error in the pairwise distances is minimized</em></p></li>
</ul>
<p>Some have suggest that visualizing data or models in a multidimensional scaling space is visualizing the space of uncertainty.</p>
</section>
<section id="naive-bayes">
<h2><strong>Naive Bayes</strong><a class="headerlink" href="#naive-bayes" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_naive_Bayes.html"><span class="doc std std-doc">Naive Bayes</span></a>: the application of the assumption of conditional independence to simplify the classification prediction problem from the perspective of Bayesian updating, based on the conditional probability of a category, <span class="math notranslate nohighlight">\(k\)</span>, given <span class="math notranslate nohighlight">\(n\)</span> features, <span class="math notranslate nohighlight">\(x_1, \dots , x_n\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3 | x_4, \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k)  (x_{n} | C_k) P(C_k)
\]</div>
<p>The likelihood, conditional probability with the joint conditional is difficult, likely impossible to calculate. It requires information about the joint relationship between <span class="math notranslate nohighlight">\(x_1, \dots , x_n\)</span> features. As <span class="math notranslate nohighlight">\(n\)</span> increases this requires a lot of data to inform the joint distribution.</p>
<p>With the naive Bayes approach we make the ‘naive’ assumption that the features are all <em>conditionally independent</em>*. This entails,</p>
<div class="math notranslate nohighlight">
\[
P(x_i | x_{i+1}, \ldots , x_n, C_k) = P(x_i | C_k)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span> features.</p>
<p>We can now solve for the needed conditional probability as:</p>
<div class="math notranslate nohighlight">
\[
P(C_k | x_1, \dots , x_n) = \frac{P(C_k) \prod_{i=1}^{n} P(x_i | C_k)}{P(x_1, \dots , x_n)}
\]</div>
<p>We only need the prior, <span class="math notranslate nohighlight">\(P(C_k)\)</span>, and a set of conditionals, <span class="math notranslate nohighlight">\(P(x_i | C_k)\)</span>, for all predictor features, <span class="math notranslate nohighlight">\(i = 1,\ldots,n\)</span> and all categories, <span class="math notranslate nohighlight">\(k = 1,\ldots,K\)</span>.</p>
<p>The evidence term, <span class="math notranslate nohighlight">\(P(x_1, \dots , x_n)\)</span>, is only based on the features <span class="math notranslate nohighlight">\(x_1, \dots , x_n\)</span>; therefore, is a constant over the categories <span class="math notranslate nohighlight">\(k = 1,\ldots,n\)</span>.</p>
<ul class="simple">
<li><p>it ensures closure - probabilities over all categories sum to one</p></li>
<li><p>we simply standardize the numerators to sum to one over the categories</p></li>
</ul>
<p>The naive Bayes approach is:</p>
<ul class="simple">
<li><p>simple to understand, builds on fundamental Bayesian statistics</p></li>
<li><p>practical even with small datasets since with the conditional independence we only need to estimate simple conditional distributions</p></li>
</ul>
</section>
<section id="ndarray">
<h2><strong>ndarray</strong><a class="headerlink" href="#ndarray" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Machine Learning Workflow Construction and Coding</span>: Numpy’s convenient class for working with grids, exhaustive, regularly spaced data over 2D or 3D, representing maps and models, due to,</p>
<ul class="simple">
<li><p>convenient data structure to store, access, manipulate gridded data</p></li>
<li><p>built in methods to load from a variety of file types, Python classes</p></li>
<li><p>built in methods to calculate multidimensional summary statistics</p></li>
<li><p>built in methods for data queries, filters</p></li>
<li><p>built in methods for data manipulation, cleaning, reformatting</p></li>
<li><p>built in attributes to store information about the nD array, for example, size and shape</p></li>
</ul>
</section>
<section id="nonparametric-model">
<h2><strong>Nonparametric Model</strong><a class="headerlink" href="#nonparametric-model" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a model that makes no assumption about the functional form, shape of the natural setting.</p>
<ul class="simple">
<li><p>learns the shape from the training data, more flexibility to fit a variety of shapes for natural systems</p></li>
<li><p>less risk that the model is a poor fit for the natural settings than with parametric models</p></li>
</ul>
<p>Typically need a lot more data for an accurate estimate of nonparametric models,</p>
<ul class="simple">
<li><p>nonparametric often have many trainable parameters, i.e., nonparametric models are actually parametric rich!</p></li>
</ul>
</section>
<section id="norm">
<h2><strong>Norm</strong><a class="headerlink" href="#norm" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_linear_regression.html"><span class="doc std std-doc">Linear Regression</span></a>: norm of a vector maps vector values to a summary measure <span class="math notranslate nohighlight">\([𝟎,\infty)\)</span>, that indicates size or length.</p>
<p>To train our models to training data, we require a single summary measure of mismatch with the training data, training error. The error is observed at each training data location,</p>
<div class="math notranslate nohighlight">
\[
\Delta y_i = y_i - \hat{y}_i, \quad \forall \quad i = 1,\ldots,n
\]</div>
<p>as an error vector. We need a single value to summarize over all training data, that we can minimize!</p>
</section>
<section id="normalization">
<h2><strong>Normalization</strong><a class="headerlink" href="#normalization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: a distribution rescaling that can be thought of as shifting, and stretching or squeezing of a univariate distribution (e.g., <em>histogram</em>) to a minimum of 0.0 and a maximum of 1.0.</p>
<ul class="simple">
<li><p>this is a shift and stretch / squeeze of the original property distribution assumes no shape change, rank preserving</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y_i = \frac{x_i - min(x)}{max(x) - min(x)}, \quad \forall \quad i, \ldots, n
\]</div>
<p>Methods that require standardization and min/max normalization:</p>
<ul class="simple">
<li><p>k-means clustering, k-nearest neighbour regression</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> coefficient’s for feature ranking</p></li>
<li><p>artificial neural networks forward transform of predictor features and back transform of response features to improve activation function sensitivity</p></li>
</ul>
</section>
<section id="normalized-histogram">
<h2><strong>Normalized Histogram</strong><a class="headerlink" href="#normalized-histogram" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_univariate_analysis.html"><span class="doc std std-doc">Univariate Analysis</span></a>: is a representation of the univariate statistical distribution with a plot of probability over an exhaustive set of bins over the range of possible values. These are the steps to build a normalized histogram,</p>
<ol class="arabic simple">
<li><p>Divide the continuous feature range of possible values into <span class="math notranslate nohighlight">\(K\)</span> equal size bins, <span class="math notranslate nohighlight">\(\delta x\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\Delta x = \left( \frac{x_{max} - x_{min}}{K} \right)
\]</div>
<p>or use available categories for categorical features.</p>
<ol class="arabic simple" start="2">
<li><p>Count the number of samples (frequency) in each bin, <span class="math notranslate nohighlight">\(n_k\)</span>, <span class="math notranslate nohighlight">\(\forall k=1,\ldots,K\)</span> and divide each by the total number of data, <span class="math notranslate nohighlight">\(n\)</span>, to calculate the probability of each bin,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
p_k = \frac{n_k}{n}, \forall \quad k = 1,\ldots,L
\]</div>
<ol class="arabic simple" start="4">
<li><p>Plot the probability vs. the bin label (use bin centroid if continuous)</p></li>
</ol>
<p>Note, normalized histograms are typically plotted as a bar chart.</p>
</section>
<section id="one-hot-encoding">
<h2><strong>One Hot Encoding</strong><a class="headerlink" href="#one-hot-encoding" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: bin the range of the feature into K bins, then for each sample assignment of a value of 1 if the sample is within a bin and 0 if outsize the bin</p>
<ul class="simple">
<li><p>binning strategies include uniform width bins (uniform) and uniform number of data in each bin (quantile)</p></li>
<li><p>also known as K bins discretization</p></li>
</ul>
<p>Methods that require K bins discretization,</p>
<ul class="simple">
<li><p>basis expansion to work in a higher dimensional space</p></li>
<li><p>discretization of continuous features to categorical features for categorical methods such as naive Bayes classifier</p></li>
<li><p>histogram construction and Chi-square test for difference in distributions</p></li>
<li><p>mutual information binning</p></li>
</ul>
</section>
<section id="out-of-bag-sample">
<h2><strong>Out-of-Bag Sample</strong><a class="headerlink" href="#out-of-bag-sample" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_ensemble_trees.html"><span class="doc std std-doc">Bagging Tree and Random Forest</span></a>: with bootstrap resampling of the data, it can be shown that about <span class="math notranslate nohighlight">\(\frac{2}{3}\)</span> of the data will be included (in expectation). For bagging-based ensemble prediction models,</p>
<ul class="simple">
<li><p>therefore are <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span> of the data (in expectation) unused in training each model realization, these are know as out-of-bag observations</p></li>
<li><p>for every response feature observation, <span class="math notranslate nohighlight">\(y_{\alpha}\)</span>, there are <span class="math notranslate nohighlight">\(\frac{B}{3}\)</span> out-of-bag predictions, <span class="math notranslate nohighlight">\(y^{*,b}_{\alpha}\)</span></p></li>
<li><p>we can aggregate this ensemble of prediction realizations, average for regression or mode for classification, to calculate a single out-of-bag prediction, <span class="math notranslate nohighlight">\(y^{*}_{\alpha} = \sum_{\alpha = 1}^{\frac{B}{3}} y^{*,b}_{\alpha}\)</span></p></li>
<li><p>from these single out-of-bag predictions over all data, the out-of-bag mean square error (MSE) is calculated as,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
MSE_{OOB} = \sum_{\alpha = 1}^{\frac{B}{3}} \left[ y^{*}_{\alpha} - y_{\alpha} \right]^2
\]</div>
<p>For bagging-based ensemble predictive machine learning, there is no need to perform training and testing splits, hyperparameter tuning can be applied with out-of-bag MSE.</p>
<ul class="simple">
<li><p>this is equivalent to random train and test split that may not be fair, same difficulty as the planned use of the model</p></li>
<li><p>this freezes the test proportion at about <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span></p></li>
</ul>
</section>
<section id="overfit-model">
<h2><strong>Overfit Model</strong><a class="headerlink" href="#overfit-model" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a machine learning model that is fit to data noise or data idiosyncrasies</p>
<ul class="simple">
<li><p>increased complexity will generally decrease error with respect to the training dataset but, may result in increase error with testing data</p></li>
<li><p>over the region of model complexity with rising testing error and falling training error</p></li>
</ul>
<p>Issues of an overfit machine learning model,</p>
<ul class="simple">
<li><p>more model complexity and flexibility than can be justified with the available data, data accuracy, frequency and coverage</p></li>
<li><p>high accuracy in training, but low accuracy in testing representing real-world use away from training data cases, indicating poor ability of the model to generalize</p></li>
</ul>
</section>
<section id="parameters-statistics">
<h2><strong>Parameters</strong> (statistics)<a class="headerlink" href="#parameters-statistics" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a summary measure of a population</p>
<ul class="simple">
<li><p>for example, population mean, population standard deviation</p></li>
</ul>
<p>We very rarely have access to actual population parameters, in general we infer population parameters with available sample statistics</p>
</section>
<section id="parameters-machine-learning">
<h2><strong>Parameters</strong> (machine learning)<a class="headerlink" href="#parameters-machine-learning" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: trainable coefficients for a machine learning model that control the fit to the training data</p>
<ul class="simple">
<li><p>model parameters are calculated by optimization to minimize error over the training data through, analytical solution, or iterative solution, e.g., gradient descent optimization</p></li>
</ul>
</section>
<section id="parametric-model">
<h2><strong>Parametric Model</strong><a class="headerlink" href="#parametric-model" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a model that makes an assumption about the functional form, shape of the natural system.</p>
<ul class="simple">
<li><p>we gain simplicity and advantage of only a few parameters</p></li>
<li><p>for is a linear model we only have <span class="math notranslate nohighlight">\(m+1\)</span> model parameters</p></li>
</ul>
<p>There is a risk that our model is quite different than the natural setting, resulting in a poor model, for example, a linear model applied to a nonlinear phenomenon.</p>
</section>
<section id="partial-correlation-coefficient">
<h2><strong>Partial Correlation Coefficient</strong><a class="headerlink" href="#partial-correlation-coefficient" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_multivariate_analysis.html"><span class="doc std std-doc">Multivariate Analysis</span></a>: a method to calculate the correlation between <span class="math notranslate nohighlight">\(𝑿\)</span> and <span class="math notranslate nohighlight">\(𝒀\)</span> after controlling for the influence of <span class="math notranslate nohighlight">\(𝒁_𝟏,\ldots,𝒁_(𝒎−𝟐)\)</span> other features on both <span class="math notranslate nohighlight">\(𝑿\)</span> and <span class="math notranslate nohighlight">\(𝑌\)</span>. Note, I use <span class="math notranslate nohighlight">\(m-2\)</span> to account for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> removed.</p>
<p>For <span class="math notranslate nohighlight">\(\rho_(𝑋,𝑌.𝑍_1,…,𝑍_(𝑚−2) )\)</span>,</p>
<ol class="arabic simple">
<li><p>perform linear, least-squares regression to predict <span class="math notranslate nohighlight">\(𝑿\)</span> from <span class="math notranslate nohighlight">\(𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}\)</span>. <span class="math notranslate nohighlight">\(𝑿\)</span> is regressed on the predictors to calculate the estimate, <span class="math notranslate nohighlight">\(𝑿^∗\)</span>.</p></li>
<li><p>perform linear, least-squares regression to predict <span class="math notranslate nohighlight">\(𝒀\)</span> from <span class="math notranslate nohighlight">\(𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}\)</span>. <span class="math notranslate nohighlight">\(𝒀\)</span> is regressed on the predictors to calculate the estimate, 𝒀^∗</p></li>
<li><p>calculate the residuals in Step #1, <span class="math notranslate nohighlight">\(𝑿 − 𝑿^∗\)</span>, where <span class="math notranslate nohighlight">\(𝑿^∗=𝒇(𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})\)</span>, linear regression model</p></li>
<li><p>calculate the residuals in Step #2, <span class="math notranslate nohighlight">\(𝒀 − 𝒀^∗\)</span>, where <span class="math notranslate nohighlight">\(𝒀^∗=𝒇(𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})\)</span>, linear regression model</p></li>
<li><p>calculate the correlation coefficient between the residuals from Steps #3 and #4, <span class="math notranslate nohighlight">\(\rho_{𝑿 −𝑿^∗,𝒀 − 𝒀^∗}\)</span></p></li>
</ol>
<p>Assumptions of Partial Correlation, for <span class="math notranslate nohighlight">\(𝝆_(𝑿,𝒀.𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐})\)</span>,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(𝑿,𝒀,𝒁_𝟏,\ldots,𝒁_{𝒎−𝟐}\)</span> have linear relationships, i.e., all pairwise relationships are linear</p></li>
<li><p>no outliers for any of the univariate distributions (univariate outliers) and pairwise relationships (bivariate outliers). Partial correlation is very sensitive to outliers like regular correlation.</p></li>
<li><p>Gaussian distributed, univariate and pairwise bivariate distributions Gaussian distributed. Bivariate should be linearly related and homoscedastic.</p></li>
</ul>
</section>
<section id="partitional-clustering">
<h2><strong>Partitional Clustering</strong><a class="headerlink" href="#partitional-clustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Cluster Analysis</span>: all cluster group assignments are determined at once, as opposed to an agglomerative hierarchical clustering method that starts with <span class="math notranslate nohighlight">\(n\)</span> clusters and then iteratively merges clusters into larger clusters</p>
<ul class="simple">
<li><p>k-means clustering is partitional clustering, while the solution heuristic to find the solution is iterative, the solution is actually all at once</p></li>
<li><p>easy to update, for example, by modifying the prototype locations and recalculating the group assignments</p></li>
</ul>
</section>
<section id="polygonal-declustering">
<h2><strong>Polygonal Declustering</strong><a class="headerlink" href="#polygonal-declustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: a declustering method to assign weights to spatial samples based on local sampling density, such that the weighted statistics are likely more representative of the population. Data weights are assigned so that,</p>
<ul class="simple">
<li><p>samples in densely sampled areas receive less weight</p></li>
<li><p>samples in sparsely sampled areas receive more weight</p></li>
</ul>
<p>Polygonal declustering proceeds as follows:</p>
<ol class="arabic simple">
<li><p>Split up the area of interest with Voronoi polygons. These are constructed by intersected perpendicular bisectors between adjacent data points. The polygons group the area of interest by nearest data point</p></li>
<li><p>Assign weight to each datum proportional to the area of the associated Voronoi polygon</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
w(\bf{u}_j) = n \cdot \frac{A_j}{\sum_{j=1}^n}
\]</div>
<p>where <span class="math notranslate nohighlight">\(w(\bf{u}_j)\)</span> is the weight for the <span class="math notranslate nohighlight">\(j\)</span> data. Note, the sum of the weights is <span class="math notranslate nohighlight">\(n\)</span>; therefore, <span class="math notranslate nohighlight">\(w(\bf{u}_j)\)</span> is nominal weight of 1.0, sample density if the data were equally spaced over the area of interest.</p>
<p>Here are some highlights for polygonal declustering,</p>
<ul class="simple">
<li><p>polygonal declustering is sensitive to the boundaries of the area of interest; therefore, the weights assigned to the data near the boundary of the area of interest may change radically as the area of interest is expanded or contracted</p></li>
<li><p>polygonal declustering is the same as the Theissen polygon method for calculation of precipitation averages developed by Afred H. Thiessen in 1911, <span id="id2">[]</span></p></li>
</ul>
</section>
<section id="polynomial-regression">
<h2><strong>Polynomial Regression</strong><a class="headerlink" href="#polynomial-regression" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_polynomial_regression.html"><span class="doc std std-doc">Polynomial Regression</span></a>: application of polynomial basis expansion to the predictor features before linear regression,</p>
<div class="math notranslate nohighlight">
\[
y = \sum_{l=1}^{k} \sum_{j=1}^{m} \beta_{j,l} h_l (X_j) + \beta_0
\]</div>
<p>where the h transforms over training data, <span class="math notranslate nohighlight">\(𝑖=1,\ldots,n\)</span>,</p>
<div class="math notranslate nohighlight">
\[ 
h_1(x_i) = x_i, \quad h_2(x_i) = x_i^2, \quad h_3(x_i) = x_i^3, \quad h_4(x_i) = x_i^4, \dots, h_k(x_i) = x_i^k 
\]</div>
<p>up to the specified order <span class="math notranslate nohighlight">\(𝑘\)</span>.</p>
<p>For example, with a single predictor feature, <span class="math notranslate nohighlight">\(𝑚 = 1\)</span>, up to the <span class="math notranslate nohighlight">\(4^{th}\)</span> order,</p>
<div class="math notranslate nohighlight">
\[
y = \beta_{1,1} X + \beta_{1,2} X^2 + \beta_{1,3} X^3 + \beta_{1,4} X^4 + \beta_0
\]</div>
<p>After the <span class="math notranslate nohighlight">\(𝒉_𝒍\)</span>, <span class="math notranslate nohighlight">\(𝑙=1,\ldots,𝑘\)</span> transforms, over the <span class="math notranslate nohighlight">\(𝑗=1,\dots,𝑚\)</span> predictor features we have the same linear equation and the ability to utilize the previously discussed analytical solution.</p>
<ul class="simple">
<li><p>we are assuming linearity after application of our basis expansion.</p></li>
</ul>
<p>Now the model parameters, <span class="math notranslate nohighlight">\(\beta_(𝒍,𝒊)\)</span>, relate to a transformed version of the initial predictor feature, <span class="math notranslate nohighlight">\(𝒉_𝒍 (𝑿_𝒋)\)</span>.</p>
<ul class="simple">
<li><p>we lose the ability to interpret the coefficients, for example, what is 𝑝𝑒𝑟𝑚𝑒𝑎𝑏𝑖𝑙𝑖𝑡𝑦<span class="math notranslate nohighlight">\(^4\)</span>?</p></li>
<li><p>generally, significantly higher model variance, i.e., may have unstable interpolation and especially extrapolation</p></li>
</ul>
<p>Polynomial regression model assumptions,</p>
<ul class="simple">
<li><p><em>error-free</em> - predictor features basis expansions are error free, not random variables</p></li>
<li><p><em>constant variance</em> - error in response is constant over predictor(s) value</p></li>
<li><p><em>linearity</em> - response is linear combination of basis features</p></li>
<li><p><em>polynomial</em> - relationships between 𝑋 and Y is polynomial</p></li>
<li><p><em>independence of error</em> - error in response are uncorrelated with each other</p></li>
<li><p>no multicollinearity* - none of the basis feature expansions are linearly redundant with other features</p></li>
</ul>
</section>
<section id="population">
<h2><strong>Population</strong><a class="headerlink" href="#population" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: exhaustive, finite list of property of interest over area of interest.</p>
<ul class="simple">
<li><p>for example, exhaustive set of porosity measures at every location within a reservoir</p></li>
</ul>
<p>Generally, the entire population is not generally accessible and we use a limited sample to make inference concerning the population</p>
</section>
<section id="power-law-average">
<h2><strong>Power Law Average</strong><a class="headerlink" href="#power-law-average" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: a general form for averaging based scale up, aggregation of smaller scale measures in a larger volume into a single value representative of the larger volume</p>
<div class="math notranslate nohighlight">
\[
\overline{x}_p = \left(\frac{1}{n}\sum_{i=1}^n x_i^p \right)^{\frac{1}{p}}
\]</div>
<ul class="simple">
<li><p>useful to calculate effective permeability where flow is not parallel nor perpendicular to distinct permeability layers</p></li>
<li><p>flow simulation may be applied to calibrate (calculate the appropriate power for power law averaging)</p></li>
</ul>
</section>
<section id="precision-classification-accuracy-metric">
<h2><strong>Precision</strong> (classification accuracy metric)<a class="headerlink" href="#precision-classification-accuracy-metric" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_naive_Bayes.html"><span class="doc std std-doc">Naive Bayes</span></a>: a categorical classification prediction model measure of accuracy, a single summary metric for each <span class="math notranslate nohighlight">\(k\)</span> category from the confusion matrix.</p>
<ul class="simple">
<li><p>the ratio of true positives divided by all positives, true positives + false positives</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Precision_k = \frac{ n_{k,\text{true  positives}} }{ n_{k,\text{true  positives}} + n_{k,\text{false  positives}}} = \frac{ n_{k,\text{true  positives}} }{ n_{k, \text{all  positives}} }
\]</div>
</section>
<section id="prediction-interval">
<h2><strong>Prediction Interval</strong><a class="headerlink" href="#prediction-interval" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_linear_regression.html"><span class="doc std std-doc">Linear Regression</span></a>: the uncertainty in the next prediction represented as a range, lower and upper bound, based on a specified probability interval known as the confidence level.</p>
<p>We communicate confidence intervals like this,</p>
<ul class="simple">
<li><p>there is a 95% probability (or 19 times out of 20) that the true reservoir NTG is between 13% and 17%, given, predictor feature values, <span class="math notranslate nohighlight">\(𝑋_1=𝑥_1,\ldots,𝑋_𝑚=𝑥_𝑚\)</span>.</p></li>
</ul>
<p>Is the uncertainty in our prediction, for prediction intervals we integrate,</p>
<ul class="simple">
<li><p>uncertainty in the model <span class="math notranslate nohighlight">\(𝐸{\hat{𝑌}|𝑋=𝑥}\)</span></p></li>
<li><p>error in the model, conditional distribution <span class="math notranslate nohighlight">\(\hat{Y}|X=x\)</span></p></li>
</ul>
</section>
<section id="prediction-predictive-statistics">
<h2><strong>Prediction, Predictive Statistics</strong><a class="headerlink" href="#prediction-predictive-statistics" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: estimate the next sample(s) given assumptions about or a model of the population</p>
<ul class="simple">
<li><p>for example, given our model of the reservoir, predict the next well (pre-drill assessment) sample, e.g., porosity, permeability, production rate, etc.</p></li>
</ul>
</section>
<section id="predictor-feature">
<h2><strong>Predictor Feature</strong><a class="headerlink" href="#predictor-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the input feature for a predictive machine learning model. We can generalize a predictive machine learning model as,</p>
<div class="math notranslate nohighlight">
\[
y = \hat{f}(x_1,\ldots,x_m) + \epsilon
\]</div>
<p>where the response feature is <span class="math notranslate nohighlight">\(y\)</span>, the predictor features are <span class="math notranslate nohighlight">\(x_1,\ldots,x_m\)</span>, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is model error</p>
<ul class="simple">
<li><p>traditional statistics uses the term independent variable</p></li>
</ul>
</section>
<section id="predictor-feature-space">
<h2><strong>Predictor Feature Space</strong><a class="headerlink" href="#predictor-feature-space" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: refers to the predictor features and does not include the response feature(s), i.e.,</p>
<ul class="simple">
<li><p>all possible combinations of predictor features for which we need to make predictions</p></li>
<li><p>may be referred to as predictor feature space.</p></li>
</ul>
<p>Typically, we train and test our machines’ predictions over the predictor feature space.</p>
<ul class="simple">
<li><p>the space is typically a hypercuboid with each axis representing a predictor feature and extending from the minimum to maximum, over the range of each predictor feature</p></li>
<li><p>more complicated shapes of predictor feature space are possible, e.g., we could mask or remove subsets with poor data coverage.</p></li>
</ul>
</section>
<section id="primary-data">
<h2><strong>Primary Data</strong><a class="headerlink" href="#primary-data" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: data samples for the feature of interest, the target feature for building a model, for example,</p>
<ul class="simple">
<li><p>porosity measures from cores and logs used to build a full 3D porosity model. Any samples of porosity are the primary data</p></li>
<li><p>as opposed to secondary feature, e.g., if we have facies data to help predict porosity, the facies data are secondary data</p></li>
</ul>
</section>
<section id="principal-component-analysis">
<h2><strong>Principal Component Analysis</strong><a class="headerlink" href="#principal-component-analysis" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_PCA.html"><span class="doc std std-doc">Principal Component Analysis</span></a>: one of a variety of methods for dimensional reduction, transform the data to a lower dimension</p>
<ul class="simple">
<li><p>given features, <span class="math notranslate nohighlight">\(𝑋_1,\dots,𝑋_𝑚\)</span> we would require <span class="math notranslate nohighlight">\({m \choose 2}=\frac{𝑚 \cdot (𝑚−1)}{2}\)</span> scatter plots to visualize just the two-dimensional scatter plots.</p></li>
<li><p>once we have 4 or more variables understanding our data gets very hard.</p></li>
<li><p>recall the curse of dimensionality, impact inference, modeling and visualization.</p></li>
</ul>
<p>One solution, is to find a good lower dimensional, <span class="math notranslate nohighlight">\(𝑝\)</span>,  representation of the original dimensions <span class="math notranslate nohighlight">\(𝑚\)</span></p>
<p>Benefits of Working in a Reduced Dimensional Representation:</p>
<ol class="arabic simple">
<li><p>data storage / Computational Time</p></li>
<li><p>easier visualization</p></li>
<li><p>also takes care of multicollinearity</p></li>
</ol>
<p>Salient points of principal component analysis,</p>
<ul class="simple">
<li><p><em>orthogonal transformation</em> - convert a set of observations into a set of linearly uncorrelated variables known as principal components, the transformation retains pairwise distance, i.e., is a rotation</p></li>
<li><p><em>number of principal components (<span class="math notranslate nohighlight">\(k\)</span>) available</em> - is min⁡(<span class="math notranslate nohighlight">\(𝑛−1,𝑚\)</span>), limited by the variables/features, <span class="math notranslate nohighlight">\(𝑚\)</span>, and the number of data</p></li>
</ul>
<p>Components are ordered,</p>
<ul class="simple">
<li><p>first component describes the larges possible variance / accounts for as much variability as possible</p></li>
<li><p>next component describes the largest possible remaining variance</p></li>
<li><p>up to the maximum number of principal components</p></li>
</ul>
<p>Eigenvalues and eigenvectors-based,</p>
<p>Calculate the data covariance matrix, the pairwise covariance for the combinatorial of features and then calculate the eigenvectors and eigenvalues from the covariance matrix,</p>
<ul class="simple">
<li><p>the eigenvalues are the variance explained for each component.</p></li>
<li><p>the eigenvectors of the data covariance matrix are the principal components.</p></li>
</ul>
</section>
<section id="probability-density-function-pdf">
<h2><strong>Probability Density Function</strong> (PDF)<a class="headerlink" href="#probability-density-function-pdf" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_univariate_analysis.html"><span class="doc std std-doc">Univariate Analysis</span></a>: a representation of a statistical distribution with a function, <span class="math notranslate nohighlight">\(f(x)\)</span>, of probability density over the range of all possible feature values, <span class="math notranslate nohighlight">\(x\)</span>. These are the concepts for PDFs,</p>
<ul class="simple">
<li><p>non-negativity constraint, the density cannot be negative,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
0.0 \le f(x)
\]</div>
<ul class="simple">
<li><p>for continuous features the density may be &gt; 1.0, because density is a measure of likelihood and not of probability</p></li>
<li><p>integrate density over a range of <span class="math notranslate nohighlight">\(x\)</span> to calculate probability,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
0 \le \int_a^b f(x) dx = P(a \le x \le b) \le 1.0
\]</div>
<ul class="simple">
<li><p>probability closure, the sum of the area under the PDF curve is equal to 1.0,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\int_{-infty}^{\infty} f(x) dx = 1.0
\]</div>
<p>Nonparametric PDFs are calculated with kernels (usual a small Gaussian distribution) that is summed over all data; therefore, there is an implicitly scale (smoothness) parameter when calculating a PDF.</p>
<ul class="simple">
<li><p>To large of kernels will smooth out important information about the univariate distribution</p></li>
<li><p>Too narrow will result in an overly noisy PDF that is difficult to interpret.</p></li>
</ul>
<p>This is analogous to the choice of bin size for a histogram or normalized histogram.</p>
<p>Parametric PDFs are possible but require model fitting to the data, the steps are,</p>
<ol class="arabic simple">
<li><p>Select a parametric distribution, e.g., Gaussian, log normal, etc.</p></li>
<li><p>Calculate the parameters for the parametric distribution based on the available data, by methods such as least squares or maximum likelihood.</p></li>
</ol>
</section>
<section id="probability-non-negativity-normalization">
<h2><strong>Probability Non-negativity, Normalization</strong><a class="headerlink" href="#probability-non-negativity-normalization" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Probability Concepts</span>: fundamental constraints on probability including,</p>
<ol class="arabic simple">
<li><p>Bounded, <span class="math notranslate nohighlight">\(0.0 \le P(A) \le 1.0\)</span></p></li>
<li><p>Closure, <span class="math notranslate nohighlight">\(P(\Omega) = 1.0\)</span></p></li>
<li><p>Null Sets, <span class="math notranslate nohighlight">\(P(\emptyset) = 0.0\)</span></p></li>
</ol>
</section>
<section id="probability-of-acceptance-mcmc">
<h2><strong>Probability of Acceptance</strong> (MCMC)<a class="headerlink" href="#probability-of-acceptance-mcmc" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_Bayesian_linear_regression.html"><span class="doc std std-doc">Bayesian Linear Regression</span></a>: applied in a rejection sampler as the likelihood of a candidate sample being added to the sample.</p>
<ul class="simple">
<li><p>conditional acceptance is performed by Monte Carlo simulation,</p></li>
<li><p>sequentially sampling from conditional distributions</p></li>
</ul>
<p>The acceptance rule is,</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(𝑃(𝑎𝑐𝑐𝑒𝑝𝑡) \ge 1\)</span>, accept – accept</p></li>
<li><p>if <span class="math notranslate nohighlight">\(𝑃(𝑎𝑐𝑐𝑒𝑝𝑡) \lt 1\)</span>, conditionally accept, draw <span class="math notranslate nohighlight">\(𝑝 ∼ U[0,1]\)</span>, and accept if <span class="math notranslate nohighlight">\(𝑝 \le 𝛼\)</span></p></li>
</ul>
</section>
<section id="probability-operators">
<h2><strong>Probability Operators</strong><a class="headerlink" href="#probability-operators" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: common probability operators that are essential to working with probability and uncertainty problems,</p>
<p><em>Union of Events</em> - the union of outcomes, the probability of <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> is calculated with the probability addition rule,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = P(A) + P(B) - P(A,B)
\]</div>
<p><em>Intersection of Events</em> - the intersection of outcomes, the probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is represented as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(A,B)
\]</div>
<p>only under the assumption of independence of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> can it be calculate from the probabilities of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as,</p>
<div class="math notranslate nohighlight">
\[
P(A,B) = P(A) \cdot P(B)
\]</div>
<p>if there is dependence between <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> then we need the conditional probability, <span class="math notranslate nohighlight">\(P(A|B)\)</span> instead of the marginal, <span class="math notranslate nohighlight">\(P(A)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P(A,B) = P(A|B) \cdot P(B)
\]</div>
<p><em>Complimentary Events</em> - is the NOT operator for probability, if we define <span class="math notranslate nohighlight">\(A\)</span> then <span class="math notranslate nohighlight">\(A\)</span> compliment, <span class="math notranslate nohighlight">\(A^c\)</span> is not <span class="math notranslate nohighlight">\(A\)</span> and we have this resulting closure relationship,</p>
<div class="math notranslate nohighlight">
\[
P(A) + P(A^c) = 1.0
\]</div>
<p>complimentary events may be considered for beyond univariate problems, for example consider this bivariate closure,</p>
<div class="math notranslate nohighlight">
\[
P(A|B) + P(A^c|B) = 1.0
\]</div>
<p>Note, the given term must be the same.</p>
<p><em>Mutually Exclusive Events</em> - the events that do not intersect or do not have any common outcomes. We represent this with set notation as,</p>
<div class="math notranslate nohighlight">
\[
\{x: x \in A \text{ and } x \in B \} = \emptyset
\]</div>
<p>and the joint probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as,</p>
<div class="math notranslate nohighlight">
\[
P(A \cap B) = P(A,B) = 0
\]</div>
</section>
<section id="probability-perspectives">
<h2><strong>Probability Perspectives</strong><a class="headerlink" href="#probability-perspectives" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: the 3 primary perspectives for calculating probability:</p>
<ol class="arabic simple">
<li><p><em>Long-term frequencies</em> - probability as ratio of outcomes, requires repeated observations of an experiment. The basis for <em>frequentist probability</em>.</p></li>
<li><p><em>Physical tendencies or propensities</em> - probability from knowledge about or modeling the system, e.g., we could know the probability of a heads outcome from a coin toss without the experiment.</p></li>
<li><p><em>Degrees of belief</em> - reflect our certainty about a result, very flexible, assign probability to anything, and updating with new information. The basis for <em>Bayesian probability</em>.</p></li>
</ol>
</section>
<section id="prototype-clustering">
<h2><strong>Prototype</strong> (clustering)<a class="headerlink" href="#prototype-clustering" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Cluster Analysis</span>: represent the sample data with set of points in the feature space.</p>
<ul class="simple">
<li><p>prototypes are typically not actual samples</p></li>
<li><p>sample data are often assigned to the nearest (Euclidean) distance prototype</p></li>
</ul>
</section>
<section id="qualitative-features">
<h2><strong>Qualitative Features</strong><a class="headerlink" href="#qualitative-features" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: information about quantities that you cannot directly measure, require interpretation of measurement, and are described with words (not numbers), for example,</p>
<ul class="simple">
<li><p>rock type = sandstone</p></li>
<li><p>zonation = bornite-chalcopyrite-gold higher grade copper zone</p></li>
</ul>
</section>
<section id="quantitative-features">
<h2><strong>Quantitative Features</strong><a class="headerlink" href="#quantitative-features" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: features that can be measured and represented by numbers, for example,</p>
<ul class="simple">
<li><p>age = 10 Ma (millions of years)</p></li>
<li><p>porosity = 0.134 (fraction of volume is void space)</p></li>
<li><p>saturation = 80.5% (volume percentage)</p></li>
</ul>
<p>Like <em>qualitative features</em>, there is often the requirement for interpretation, for example, total porosity may be measured but should be converted to effective porosity through interpretation or a model</p>
</section>
<section id="r-2-also-coefficient-of-determination">
<h2><strong><span class="math notranslate nohighlight">\(r^2\)</span></strong> (also coefficient of determination)<a class="headerlink" href="#r-2-also-coefficient-of-determination" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_linear_regression.html"><span class="doc std std-doc">Linear Regression</span></a>: the proportion of variance explained by the model in linear regression</p>
<p>This works only for linear models, where:</p>
<div class="math notranslate nohighlight">
\[
\sigma^2_{tot} = \sigma^2_{reg} + \sigma^2_{res}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2_{tot}\)</span> is variance of response feature training, <span class="math notranslate nohighlight">\(y_i\)</span>, <span class="math notranslate nohighlight">\(\sigma^2_{reg}\)</span> is variance of the model predictions, <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2_{res}\)</span> is the variance of the errors, <span class="math notranslate nohighlight">\(\Delta y_i\)</span>.</p>
<ul class="simple">
<li><p>for linear regression, <span class="math notranslate nohighlight">\(r^2 = \left( \rho_{x,y} \right)^2\)</span></p></li>
</ul>
<p>For nonlinear models this will not likely hold, then <span class="math notranslate nohighlight">\(\frac{\sigma^2_{𝑟𝑒𝑔}}{\sigma^2_{𝑡𝑜𝑡}}\)</span> may exceed <span class="math notranslate nohighlight">\([0,1]\)</span>, for our nonlinear models regression models we will use more robust measures, e.g. mean square error (MSE)</p>
</section>
<section id="random-forest">
<h2><strong>Random Forest</strong><a class="headerlink" href="#random-forest" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_ensemble_trees.html"><span class="doc std std-doc">Bagging Tree and Random Forest</span></a>: a ensemble prediction model that is based on the standard bagging approach, specifically,</p>
<ul class="simple">
<li><p>with decision tree</p></li>
<li><p>with diversification of the individual trees by restricting each split to consider a <span class="math notranslate nohighlight">\(p\)</span> random subset of the <span class="math notranslate nohighlight">\(𝑚\)</span> available predictors</p></li>
</ul>
<p>There are various methods to calculate <span class="math notranslate nohighlight">\(p\)</span> from <span class="math notranslate nohighlight">\(m\)</span> available features,</p>
<div class="math notranslate nohighlight">
\[
p = \sqrt{m}
\]</div>
<p>is common. Note, if <span class="math notranslate nohighlight">\(p = m\)</span> then random forest is tree bagging.</p>
<p>More comments on the benefit of ensemble model diversification,</p>
<ul class="simple">
<li><p>the reduction in model variance by ensemble estimation, as represented by standard error in the mean,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma_{\overline{x}}^2 = 
fracc{\sigma_{s}^2}{n}
\]</div>
<p>is under the assumption that the samples are uncorrelated. One issue with tree bagging is the trees in the ensemble may be highly correlated.</p>
<ul class="simple">
<li><p>this occurs when there is a dominant predictor feature as it will always be applied to the top split(s), the result is all the trees in the ensemble are very similar (i.e. correlated)</p></li>
<li><p>with highly correlated trees, there is significantly less reduction in model variance with the ensemble</p></li>
<li><p>this forces each tree in the ensemble to evolve in dissimilar, decorrelated, manner</p></li>
</ul>
</section>
<section id="realization">
<h2><strong>Realization</strong><a class="headerlink" href="#realization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: an outcome from a <em>random variable</em> or a joint outcome from a <em>random function</em>.</p>
<ul class="simple">
<li><p>an outcome from a random variable, <span class="math notranslate nohighlight">\(X\)</span>, (or joint set of outcomes from a random function)</p></li>
<li><p>represented with lower case, e.g., <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p>for spatial settings it is common to include a location vector, <span class="math notranslate nohighlight">\(\bf{u}\)</span>, to describe the location, e.g., <span class="math notranslate nohighlight">\(x(\bf{u})\)</span>, as <span class="math notranslate nohighlight">\(X(\bf{u})\)</span></p></li>
<li><p>resulting from simulation, e.g., Monte Carlo simulation, sequential Gaussian simulation, a method to sample (jointly) from the RV (RF)</p></li>
<li><p>in general, we assume all realizations are equiprobable, i.e., have the same probability of occurrence</p></li>
</ul>
</section>
<section id="realizations-uncertainty">
<h2><strong>Realizations</strong> (uncertainty)<a class="headerlink" href="#realizations-uncertainty" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: multiple spatial, subsurface models calculated by stochastic simulation by holding input parameters and model choices constant and only changing the random number seed</p>
<ul class="simple">
<li><p>these models represent spatial uncertainty</p></li>
<li><p>for example, hold the porosity mean constant and observe changes in porosity away from the wells over multiple realizations</p></li>
</ul>
</section>
<section id="reasons-to-learn-some-coding">
<h2><strong>Reasons to Learn Some Coding</strong><a class="headerlink" href="#reasons-to-learn-some-coding" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_workflow_construction.html"><span class="doc std std-doc">Machine Learning Workflow Construction and Coding</span></a>: Professor Pyrcz’s reasons for all scientists and engineers to learn some coding,</p>
<ul class="simple">
<li><p><em>Transparency</em> – no compiler accepts hand waiving! Coding forces your logic to be uncovered for any other scientist or engineer to review.</p></li>
<li><p><em>Reproducibility</em> – run it and get an answer, hand it over to a peer, they run it and they get the same answer. This is a principle of the scientific method.</p></li>
<li><p><em>Quantification</em> – programs need numbers and drive us from qualitative to quantitative. Feed the program and discover new ways to look at the world.</p></li>
<li><p><em>Open-source</em> – leverage a world of brilliance. Check out packages, snippets and be amazed with what great minds have freely shared.</p></li>
<li><p><em>Break Down Barriers</em> – don’t throw it over the fence. Sit at the table with the developers and share more of your subject matter expertise for a better deployed product.</p></li>
<li><p><em>Deployment</em> – share your code with others and multiply your impact. Performance metrics or altruism, your good work benefits many others.</p></li>
<li><p><em>Efficiency</em> – minimize the boring parts of the job. Build a suite of scripts for automation of common tasks and spend more time doing science and engineering!</p></li>
<li><p><em>Always Time to Do it Again!</em> – how many times did you only do it once? It probably takes 2-4 times as long to script and automate a workflow.                           Usually, worth it.</p></li>
</ul>
<p>Be Like Us – it will change you. Users feel limited, programmers truly harness the power of their applications and hardware.</p>
</section>
<section id="recall-classification-accuracy-metric">
<h2><strong>Recall</strong> (classification accuracy metric)<a class="headerlink" href="#recall-classification-accuracy-metric" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_naive_Bayes.html"><span class="doc std std-doc">Naive Bayes</span></a>: a categorical classification prediction model measure of accuracy, a single summary metric for each <span class="math notranslate nohighlight">\(k\)</span> category from the confusion matrix.</p>
<ul class="simple">
<li><p>the ratio of true positives divided by all cases of the category in the testing dataset</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Recall_k = \frac{ n_{k, \text{true  positives}} }{n_k}
\]</div>
</section>
<section id="recursive-feature-elimination">
<h2><strong>Recursive Feature Elimination</strong><a class="headerlink" href="#recursive-feature-elimination" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: a method works by recursively removing features and building a model with the remaining features.</p>
<ul class="simple">
<li><p>build a model with all features, calculate a feature ranking metric, e.g., coefficient or feature importance, depending on which is available with the modeling method</p></li>
<li><p>remove the feature with the lowest feature importance and rebuild the model</p></li>
</ul>
<p>repeat the process until only one feature remains</p>
<p>Any model predictive model could be used,</p>
<ul class="simple">
<li><p>the method assigns rank <span class="math notranslate nohighlight">\(1,\ldots,𝑚\)</span> for all features as reverse order of removal, i.e., last remaining feature is most important and first removed is least important</p></li>
</ul>
</section>
<section id="reservoir-modeling-workflow">
<h2><strong>Reservoir Modeling Workflow</strong><a class="headerlink" href="#reservoir-modeling-workflow" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_workflow_construction.html"><span class="doc std std-doc">Machine Learning Workflow Construction and Coding</span></a>: the following is the common geostatistical reservoir modeling workflow:</p>
<ol class="arabic simple">
<li><p>Integrate all available information to build multiple subsurface scenarios and realizations to sample the uncertainty space</p></li>
<li><p>Apply all the models to the transfer function to sample the decision criteria</p></li>
<li><p>Assemble the distribution of the decision criteria</p></li>
<li><p>Make the optimum reservoir development decisions accounting for this uncertainty model</p></li>
</ol>
</section>
<section id="response-feature">
<h2><strong>Response Feature</strong><a class="headerlink" href="#response-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: output feature for a predictive machine learning model. We can generalize a predictive machine learning model as,</p>
<div class="math notranslate nohighlight">
\[
y = \hat{f}(x_1,\ldots,x_m) + \epsilon
\]</div>
<p>where the response feature is <span class="math notranslate nohighlight">\(y\)</span>, the predictor features are <span class="math notranslate nohighlight">\(x_1,\ldots,x_m\)</span>, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is model error</p>
<ul class="simple">
<li><p>traditional statistics uses the term dependent variable</p></li>
</ul>
</section>
<section id="ridge-regression-tikhonov-regularization">
<h2><strong>Ridge Regression</strong> (Tikhonov Regularization)<a class="headerlink" href="#ridge-regression-tikhonov-regularization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_ridge_regression.html"><span class="doc std std-doc">Ridge Regression</span></a>: a linear, parametric prediction model,</p>
<div class="math notranslate nohighlight">
\[
y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0
\]</div>
<p>The analytical solution for the model parameters, <span class="math notranslate nohighlight">\(b_1,\ldots,b_m,b_0\)</span>, is available for the L2 norm loss function, the errors are summed and squared known a least squares.</p>
<ul class="simple">
<li><p>we minimize a loss function including the error, residual sum of squares (RSS) over the training data and a regularization term:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0) \right)^2 + \lambda \sum_{\alpha = 1}^m b_{\alpha}^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual response feature values and <span class="math notranslate nohighlight">\(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\)</span> are the model predictions, over the <span class="math notranslate nohighlight">\(\alpha = 1,\ldots,n\)</span> training data, and <span class="math notranslate nohighlight">\(\lambda \sum_{\alpha = 1}^m b_{\alpha}^2\)</span> is the shrinkage penalty.</p>
<p>With ridge regression we add a hyperparameter, <span class="math notranslate nohighlight">\(\lambda\)</span>, to our minimization, with a shrinkage penalty term, <span class="math notranslate nohighlight">\(\sum_{j=1}^m b_{\alpha}^2\)</span>.</p>
<p>As a result, ridge regression training integrates two and often competing goals to find the model parameters,</p>
<ul class="simple">
<li><p>find the model parameters that minimize the error with training data</p></li>
<li><p>minimize the slope parameters towards zero</p></li>
</ul>
<p>Note: lambda does not include the intercept, <span class="math notranslate nohighlight">\(b_0\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(\lambda\)</span> is a hyperparameter that controls the degree of fit of the model and may be related to the model bias-variance trade-off.</p>
<ul class="simple">
<li><p>for <span class="math notranslate nohighlight">\(\lambda \rightarrow 0\)</span> the solution approaches linear regression, there is no bias (relative to a linear model fit), but the model variance is likely higher</p></li>
<li><p>as <span class="math notranslate nohighlight">\(\lambda\)</span> increases the model variance decreases and the model bias increases, the model becomes simpler</p></li>
<li><p>for <span class="math notranslate nohighlight">\(\lambda \rightarrow \infty\)</span> the model parameters <span class="math notranslate nohighlight">\(b_1,\ldots,b_m\)</span> shrink to 0.0 and the model predictions approaches the training data response feature mean</p></li>
</ul>
</section>
<section id="sample">
<h2><strong>Sample</strong><a class="headerlink" href="#sample" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the set of values, locations that have been measured</p>
<ul class="simple">
<li><p>for example, 1,000 porosity measures from well-logs over the wells in the reservoir</p></li>
<li><p>or 1,000,000 acoustic impedance measurements over a 1000 x 1000 2D grid for a reservoir unit of interest</p></li>
</ul>
</section>
<section id="scenarios-uncertainty">
<h2><strong>Scenarios</strong> (uncertainty)<a class="headerlink" href="#scenarios-uncertainty" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: multiple spatial, subsurface models calculated by stochastic simulation by changing the input parameters or other modeling choices to represent the uncertainty due to inference of model parameters and model choices</p>
<ul class="simple">
<li><p>for example, model three porosity input distribution, porosity mean low, mid and high, and vary the input distribution to calculate new subsurface models</p></li>
</ul>
</section>
<section id="secondary-data">
<h2><strong>Secondary Data</strong><a class="headerlink" href="#secondary-data" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: data samples for another feature, not the feature of interest, the target feature for building a model, but are used to improve the prediction of the target feature.</p>
<ul class="simple">
<li><p>requires a model of the relationship between the primary and secondary data</p></li>
</ul>
<p>For example, samples in space of,</p>
<ul class="simple">
<li><p>acoustic impedance (secondary data) to support calculation of a model of porosity, the feature of interest</p></li>
<li><p>porosity (secondary data) to support calculation of a model of permeability, the feature of interest</p></li>
</ul>
</section>
<section id="seismic-data">
<h2><strong>Seismic Data</strong><a class="headerlink" href="#seismic-data" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: indirect measurement with remote sensing, reflection seismic applies acoustic source(s) and receivers (geophones) to map acoustic reflections with high coverage and generally low resolution. Some more details,</p>
<ul class="simple">
<li><p>seismic reflections (amplitude) data are inverted to rock properties, e.g., acoustic impedance, consistent with and positionally anchored with well sonic logs</p></li>
<li><p>provides framework, bounding surfaces for extents and shapes of reservoirs along with soft information on reservoir properties, e.g., porosity and facies.</p></li>
</ul>
</section>
<section id="shapley-value">
<h2><strong>Shapley Value</strong><a class="headerlink" href="#shapley-value" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: model-based, local (for a single prediction) and global (over a suit of predictions) feature importance by learning contribution of each feature to the prediction.</p>
<p>A explainable machine learning method to support complicated models are often required but have low interpretability.</p>
<p>Two choices to improve model interpretability,</p>
<ol class="arabic simple">
<li><p>reduce the complexity of the models, but may also reduce model accuracy</p></li>
<li><p>develop improved, agnostic (for any model) model diagnostics, i.e., Shapley value</p></li>
</ol>
<p>Shapley value is a cooperative game theory approach that,</p>
<ul class="simple">
<li><p>for allocating resources between players based on a summarization of marginal contributions, i.e., dividing up payment between players</p></li>
<li><p>calculates the contribution of each predictor feature to push the response prediction away from the mean value of the response</p></li>
<li><p>marginal contributions and Shapley values are in units of the response feature</p></li>
<li><p>in the units of the response feature</p></li>
</ul>
</section>
<section id="simpson-s-paradox">
<h2><strong>Simpson’s Paradox</strong><a class="headerlink" href="#simpson-s-paradox" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_multivariate_analysis.html"><span class="doc std std-doc">Multivariate Analysis</span></a>: data trend reverses or disappears when groups are combined (or separated). Often observed in correlation analysis when grouping data, for example,</p>
<ul class="simple">
<li><p>groups each have a negative correlation, but the whole has a positive correlation</p></li>
</ul>
</section>
<section id="soft-data">
<h2><strong>Soft Data</strong><a class="headerlink" href="#soft-data" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: data that has a high degree of uncertainty, such that data uncertainty must be integrated into the model</p>
<ul class="simple">
<li><p>for example, probability density function for local porosity calibrated from acoustic impedance</p></li>
</ul>
<p>Soft data integration requires workflows like <em>indicator kriging</em>, <em>indicator simulation</em> and <em>p-field simulation</em> or workflows that randomize the data with standard simulation methods that assume hard data like <em>sequential Gaussian simulation</em>.</p>
<ul class="simple">
<li><p>soft data integration is an advanced topic and a focus of ongoing research, but is often to done with standard, subsurface modeling software packages</p></li>
</ul>
</section>
<section id="spatial-sampling-biased">
<h2><strong>Spatial Sampling</strong> (biased)<a class="headerlink" href="#spatial-sampling-biased" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: sample such that the sample statistics are not representative of the population parameters. For example,</p>
<ul class="simple">
<li><p>the sample mean is not the same as the population mean</p></li>
<li><p>the sample variance is not the same as the population variance</p></li>
</ul>
<p>Of course, the population parameters are not accessible, so we cannot directly calculate sampling bias, i.e., the difference between the sample statistics and the population parameters. Methods we can use to check for biased sampling,</p>
<ul class="simple">
<li><p>evaluate the samples for preferential sampling, clustering, filtering, etc.</p></li>
<li><p>apply <em>declustering</em> and check the results for a major change in the summary statistics, this is using declustering diagnostically</p></li>
</ul>
</section>
<section id="spatial-sampling-clustered">
<h2><strong>Spatial Sampling</strong> (clustered)<a class="headerlink" href="#spatial-sampling-clustered" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: spatial samples with locations preferentially selected, i.e., clustered, resulting in biased statistics,</p>
<ul class="simple">
<li><p>typically spatial samples are clustered in locations with higher value samples, e.g., high porosity and permeability, good quality shale for unconventional reservoirs, low acoustic impedance indicating higher porosity, etc.</p></li>
</ul>
<p>Of course, the population parameters are not accessible, so we cannot directly calculate sampling bias, i.e., the difference between the sample statistics and the population parameters. Methods we can use to check for biased sampling,</p>
<ul class="simple">
<li><p>evaluate the samples for preferential sampling, clustering, filtering, etc.</p></li>
<li><p>apply <em>declustering</em> and check the results for a major change in the summary statistics, this is using declustering diagnostically</p></li>
</ul>
</section>
<section id="spatial-sampling-common-practice">
<h2><strong>Spatial Sampling</strong> (common practice)<a class="headerlink" href="#spatial-sampling-common-practice" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: sample locations are selected to,</p>
<p><em>Reduce uncertainty</em> - by answering questions, for example,</p>
<ul class="simple">
<li><p>how far does the contaminant plume extend? – sample peripheries</p></li>
<li><p>where is the fault? – drill based on seismic interpretation</p></li>
<li><p>what is the highest mineral grade? – sample the best part</p></li>
<li><p>who far does the reservoir extend? – offset drilling</p></li>
</ul>
<p><em>Directly maximize net present value</em> - while collecting information, for example,</p>
<ul class="simple">
<li><p>maximize production rates</p></li>
<li><p>maximize tonnage of mineral extracted</p></li>
</ul>
<p>In other words, often our samples are dual purpose, e.g., wells that are drilled for exploration and appraisal information are subsequently utilized for production.</p>
</section>
<section id="spatial-sampling-representative">
<h2><strong>Spatial Sampling</strong> (representative)<a class="headerlink" href="#spatial-sampling-representative" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Data Preparation</span>: if we are sampling for representativity, i.e., the sample set and resulting sample statistics are representative of the population, by sampling theory we have 2 options:</p>
<p><em>Random sampling</em> - each potential sample from the population is equally likely to be sampled as samples are collected. This includes,</p>
<ul class="simple">
<li><p>selecting a specific location has no impact on the selection of subsequent locations.</p></li>
<li><p>assumption that the population size that is much larger than the sample size; therefore, significant correlation between samples is not imposed due to without replacement sampling (the constraint that you can only sample a location once). Note, generally this is not an issue for the subsurface due to the sparsely sampled massive populations</p></li>
</ul>
<p><em>Regular sampling</em> - sampling at equal space or time intervals. While random sampling is preferred, regular sampling is robust as long as,</p>
<ul class="simple">
<li><p>the regular sampling intervals do not align with natural periodicity in the data, for example, the crests are systematically samples resulting in biased high sample statistics</p></li>
</ul>
</section>
<section id="spectral-clustering">
<h2><strong>Spectral Clustering</strong><a class="headerlink" href="#spectral-clustering" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_spectral_clustering.html"><span class="doc std std-doc">Spectral Clustering</span></a>: a partitional clustering method that utilizes the spectrum, eigenvalues and eigenvectors, of a matrix that represents the pairwise relationships between the data.</p>
<ul class="simple">
<li><p>dimensionality reduction from data samples pairwise relationships characterized by the graph Laplacian matrix</p></li>
<li><p>eigenvalues, eigenvectors are equivalent to principal component analysis dimensionality reduction by linear, orthogonal feature projection and rotation to best describe the variance</p></li>
</ul>
<p>Advantages of spectral clustering,</p>
<ul class="simple">
<li><p>the ability to encode pairwise relationships, integrate expert knowledge.</p></li>
<li><p>eigenvalues provide useful information on the number of clusters, based on the degree of ‘cutting’ required to make k clusters</p></li>
<li><p>lower dimensional representation for the sample data pairwise relationships</p></li>
<li><p>the resulting eigenvalues and eigenvectors can be interpreted, eigenvalues describe the amount of connection for each number of groups and eigenvectors are grouped to form the clusters</p></li>
</ul>
</section>
<section id="standardization">
<h2><strong>Standardization</strong><a class="headerlink" href="#standardization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: a distribution rescaling that can be thought of as shifting, and stretching or squeezing of a univariate distribution (e.g., <em>histogram</em>) to a mean of 0.0 and a variance of 1.0.</p>
<div class="math notranslate nohighlight">
\[
y_i = \frac{1}{\sigma_x}(x_i - \overline{x}), \quad \forall \quad i, \ldots, n
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{x}\)</span> and <span class="math notranslate nohighlight">\(\sigma_x\)</span> are the original mean and variance.</p>
<ul class="simple">
<li><p>this is a shift and stretch / squeeze of the original property distribution</p></li>
<li><p>assumes no shape change, rank preserving</p></li>
</ul>
</section>
<section id="stochastic-gradient-based-optimization">
<h2><strong>Stochastic Gradient-based Optimization</strong><a class="headerlink" href="#stochastic-gradient-based-optimization" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_LASSO_regression.html"><span class="doc std std-doc">LASSO Regression</span></a>: a method to solve for model parameters by iteratively minimizing the loss function. Stochasticity and improve computational efficiency are added to gradient descent through the use of batches,</p>
<ul class="simple">
<li><p>a batch is a random subset of the training data with specified size, <span class="math notranslate nohighlight">\(n_{batch}\)</span></p></li>
<li><p>resulting in stochastic approximations of the loss function gradient, that are faster to calculate</p></li>
<li><p>batches reduce accuracy in the gradient descent, but speed up the calculation and can perform more steps, often faster than gradient descent</p></li>
<li><p>increase <span class="math notranslate nohighlight">\(𝑛_{𝑏𝑎𝑡𝑐ℎ}\)</span> for more accuracy of gradient estimation, and decrease <span class="math notranslate nohighlight">\(𝑛_{𝑏𝑎𝑡𝑐ℎ}\)</span> to speed up the steps</p></li>
</ul>
<p>Robbins-Siegmund (1971) Theorem - converge to global minimum for convex loss functions and either a global or local minimum for nonconvex loss functions.</p>
<p>The steps include,</p>
<ol class="arabic simple">
<li><p>start with random model parameters</p></li>
<li><p>select a random subset of training data, <span class="math notranslate nohighlight">\(n_{batch}\)</span></p></li>
<li><p>calculate the loss function and loss function gradient for the model parameters over the random batch,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ 
\nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha}, b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} 
\]</div>
<ol class="arabic simple" start="3">
<li><p>update the parameter estimate by stepping down slope / gradient,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ 
\hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1))
\]</div>
<p>where <span class="math notranslate nohighlight">\(r\)</span> is the learning rate/step size, <span class="math notranslate nohighlight">\(\hat{b}(1,𝑡)\)</span>, is the current model parameter estimate and <span class="math notranslate nohighlight">\(\hat{b}(1,𝑡+1)\)</span> is the updated parameter estimate.</p>
</section>
<section id="stochastic-model">
<h2><strong>Stochastic Model</strong><a class="headerlink" href="#stochastic-model" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: system or process that is uncertain and is represented by multiple models, <em>realizations</em> and <em>scenarios</em> constrained by statistics,</p>
<ul class="simple">
<li><p>for example, data-driven models that integrate uncertainty like geostatistical simulation models</p></li>
</ul>
<p>Advantages:</p>
<ul class="simple">
<li><p>speed</p></li>
<li><p>uncertainty assessment</p></li>
<li><p>report significance, confidence / prediction intervals</p></li>
<li><p>honor many types of data</p></li>
<li><p>data-driven approaches</p></li>
</ul>
<p>Disadvantages:</p>
<ul class="simple">
<li><p>limited physics used</p></li>
<li><p>statistical model assumptions / simplification</p></li>
</ul>
<p>For the alternative to stochastic models see <em>deterministic models</em>.</p>
</section>
<section id="statistics-practice">
<h2><strong>Statistics</strong> (practice)<a class="headerlink" href="#statistics-practice" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: the theory and practice for collecting, organizing, and interpreting data, as well as drawing conclusions and making decisions.</p>
</section>
<section id="statistics-measurement">
<h2><strong>Statistics</strong> (measurement)<a class="headerlink" href="#statistics-measurement" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: summary measure of a sample, for example,</p>
<ul class="simple">
<li><p>sample mean - <span class="math notranslate nohighlight">\(\overline{x}\)</span></p></li>
<li><p>sample standard deviation - <span class="math notranslate nohighlight">\(s\)</span>,</p></li>
</ul>
<p>we use statistics as estimates of the model parameters that summarize the population (<em>inference</em>)</p>
</section>
<section id="statistical-distribution">
<h2><strong>Statistical Distribution</strong><a class="headerlink" href="#statistical-distribution" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_univariate_analysis.html"><span class="doc std std-doc">Univariate Analysis</span></a>: for a feature a description of the probability of occurrence over the range of possible values. We represent the univariate statistical distribution with,</p>
<ul class="simple">
<li><p><em>histogram</em></p></li>
<li><p><em>normalized histogram</em></p></li>
<li><p><em>probability density function</em> (PDF)</p></li>
<li><p><em>cumulative distribution function</em> (CDF)</p></li>
</ul>
<p>What do we learn from a statistical distribution? For example,</p>
<ul class="simple">
<li><p>what is the minimum and maximum?</p></li>
<li><p>do we have a lot of low values?</p></li>
<li><p>do we have a lot of high values?</p></li>
<li><p>do we have outliers, and any other values that don’t make sense and need explaining?</p></li>
</ul>
</section>
<section id="support-vector-support-vector-machines">
<h2><strong>Support Vector</strong> (support vector machines)<a class="headerlink" href="#support-vector-support-vector-machines" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_support_vector_machines.html"><span class="doc std std-doc">Support Vector Machines</span></a>: training data within the margin or misclassified and update the support vector machines classification model.</p>
<ul class="simple">
<li><p>with a support vector machines model, training data well within the correct region, are not support vectors, and have no impact on the model</p></li>
</ul>
</section>
<section id="support-vector-machines">
<h2><strong>Support Vector Machines</strong><a class="headerlink" href="#support-vector-machines" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_support_vector_machines.html"><span class="doc std std-doc">Support Vector Machines</span></a>: a predictive, binary classification machine learning method that is a good classification method when there is poor separation of groups.</p>
<ul class="simple">
<li><p>projects the original predictor features to higher dimensional space and then applies a linear, plane or hyperplane,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
𝑓(𝑥) = 𝑥^𝑇 \beta +\beta_0
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a vector and together with <span class="math notranslate nohighlight">\(\beta\)</span> are the hyperplane model parameters, while <span class="math notranslate nohighlight">\(x\)</span> is the matrix of predictor features, all are in the high dimensional space.</p>
<div class="math notranslate nohighlight">
\[
𝐺(𝑥)=\text{𝑠𝑖𝑔𝑛}\left( 𝑓(𝑥) \right)
\]</div>
<p><span class="math notranslate nohighlight">\(𝑓(𝑥)\)</span> is proportional to the signed distance from the decision boundary, and <span class="math notranslate nohighlight">\(𝐺(𝑥)\)</span> is the side of the decision boundary, <span class="math notranslate nohighlight">\(−\)</span> one side and <span class="math notranslate nohighlight">\(+\)</span> the other, <span class="math notranslate nohighlight">\(f(x) = 0\)</span> is on the decision boundary.</p>
<p>We represent the constraint, all data of each category must be on the correct side of the boundary, by,</p>
<div class="math notranslate nohighlight">
\[
y_i \left( x_i^T \beta + \beta_0 \right) \geq 0
\]</div>
<p>where this holds if the categories, <span class="math notranslate nohighlight">\(y_i\)</span>, are -1 or 1. We need a model that allows for some misclassification,</p>
<div class="math notranslate nohighlight">
\[
y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i
\]</div>
<p>We introduce the concept of a margin, <span class="math notranslate nohighlight">\(𝑀\)</span>, and a distance from the margin, the error as <span class="math notranslate nohighlight">\(\xi_i\)</span>. Now we can pose our loss function as,</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M^2} + C \sum_{i=1}^N \xi_i \right)
\]</div>
<p>subject to, <span class="math notranslate nohighlight">\(\xi_i \geq 0, \quad y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i\)</span>.</p>
<p>This is the support vector machine loss function in the higher dimensional space, where 𝛽,𝛽_0 are the multilinear model parameters.</p>
<p>Training the support vector machine, by finding the model parameters of the plane to maximize the margin, <span class="math notranslate nohighlight">\(M\)</span>, while minimizing the error, <span class="math notranslate nohighlight">\(\sum_{i=1}^N \xi_i\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(𝑪\)</span> hyperparameter weights the sum of errors, <span class="math notranslate nohighlight">\(xi_𝑖\)</span>, higher <span class="math notranslate nohighlight">\(𝐶\)</span>, will result in reduced margin, <span class="math notranslate nohighlight">\(M\)</span>, and lead to overfit</p></li>
<li><p>smaller margin, fewer data used to constrain the boundary, known as support vectors</p></li>
<li><p>training data well within the correct side of the boundary have no influence</p></li>
</ul>
<p>Here are some key aspects of support vector machines,</p>
<ul class="simple">
<li><p>known as support vector machines, and not machine, because with a new kernel you get a new machine</p></li>
<li><p>there are many kernels available including polynomial and radial basis functions</p></li>
</ul>
<p>The primary hyperparameter is <span class="math notranslate nohighlight">\(C\)</span>, the cost of</p>
<p>Hyperparameters are related to the choice of kernel, for example,</p>
<ul class="simple">
<li><p><em>polynomial</em> - polynomial order</p></li>
<li><p><em>radial basis function</em> - <span class="math notranslate nohighlight">\(\gamma\)</span> inversely proportional to the distance influence of the training data</p></li>
</ul>
</section>
<section id="tabular-data">
<h2><strong>Tabular Data</strong><a class="headerlink" href="#tabular-data" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Machine Learning Workflow Construction and Coding</span>: data table with rows for each sample and columns for each feature</p>
<p>Pandas’ DataFrames are a convenient class for working with tabular data, due to,</p>
<ul class="simple">
<li><p>convenient data structure to store, access, manipulate tabular data</p></li>
<li><p>built-in methods to load data from a variety of file types, Python classes and even directly from Excel</p></li>
<li><p>built-in methods to calculate summary statistics and visualize data</p></li>
<li><p>built-in methods for data queries, sort, data filters</p></li>
<li><p>built-in methods  for data manipulation, cleaning, reformatting</p></li>
<li><p>built-in attributes to store information about the data, e.g. size, number nulls and null value</p></li>
</ul>
</section>
<section id="training-and-testing-splits">
<h2><strong>Training and Testing Splits</strong><a class="headerlink" href="#training-and-testing-splits" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: for model cross validation, prior to predictive model training withhold a proportion of the data as testing data.</p>
<ul class="simple">
<li><p>training data are applied to train the model parameters, while withheld testing data are applied to tune the model hyperparameter</p></li>
<li><p>hyperparameter tuning is selecting the hyperparameter combination that minimizes the error norm over the withheld testing data</p></li>
</ul>
<p>The most common approach is random selection, this may not be fair testing,</p>
<ul class="simple">
<li><p>the range of testing difficulty is similar to the real-world use of the model</p></li>
<li><p>too easy – testing cases are the same or almost the same as training cases, random sampling is often too easy</p></li>
<li><p>too hard – testing cases are very different from the training cases, the model is expected to severely extrapolate</p></li>
</ul>
<p>Alternative methods such as k-fold cross validation provide the opportunity for testing over all available data but require,</p>
<ul class="simple">
<li><p>the training k predictive machine learning models over the hyperparameter combinations</p></li>
<li><p>aggregation of the testing error over the k models for selection of the optimum hyperparameters, hyperparameter tuning</p></li>
</ul>
<p>Also, there are alternative workflow that include, training, validation and testing subsets of the data</p>
</section>
<section id="transfer-function-reservoir-modeling-workflow">
<h2><strong>Transfer Function</strong> (reservoir modeling workflow)<a class="headerlink" href="#transfer-function-reservoir-modeling-workflow" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: calculation applied to the spatial, subsurface model realizations and scenarios to calculate a decision criterion, a metric that is used to support decision making representing value, and health, environment and safety. Example transfer functions include,</p>
<ul class="simple">
<li><p><em>transport and bioattenuation</em> - numerical simulation to model soil contaminant concentrations over time during a pump and treat operation</p></li>
<li><p><em>volumetric calculation</em> - for total oil-in-place to calculate resource in place</p></li>
<li><p><em>heterogeneity metric</em> - as an indicator of recovery factor to estimate reserves from resources</p></li>
<li><p><em>flow simulation</em> - for pre-drill production forecast for a planned well</p></li>
<li><p><em>Whittle’s pit optimization</em> - to calculate mineral resources and ultimate pit shell</p></li>
</ul>
</section>
<section id="uncertainty-modeling">
<h2><strong>Uncertainty Modeling</strong><a class="headerlink" href="#uncertainty-modeling" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: calculation of the range of possible values for a feature at a location or jointly over many locations at the sample time. Some considerations,</p>
<ul class="simple">
<li><p>quantification of the limitation in the precision of our samples and model predictions</p></li>
<li><p>uncertainty is a model, there is no objective uncertainty</p></li>
<li><p>uncertainty is caused by our ignorance</p></li>
<li><p>uncertainty is caused by sparse sampling, measurement error and bias, and heterogeneity</p></li>
</ul>
<p>we represent uncertainty by multiple models, scenarios and realizations:</p>
<ul class="simple">
<li><p>Scenarios - multiple spatial, subsurface models calculated by stochastic simulation by changing the input parameters or other modeling choices to represent the uncertainty due to inference of model parameters and model choices</p></li>
<li><p>Realizations - multiple spatial, subsurface models calculated by stochastic simulation by holding input parameters and model choices constant and only changing the random number seed</p></li>
</ul>
</section>
<section id="underfit-model">
<h2><strong>Underfit Model</strong><a class="headerlink" href="#underfit-model" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a machine learning model that too simple, too low complexity and flexibility, to fit the natural phenomenon resulting in very high model bias.</p>
<ul class="simple">
<li><p>underfit models often approach the response feature global mean</p></li>
<li><p>underfit models have high error over training and testing data</p></li>
<li><p>increased complexity will generally decrease error with respect to the training and testing dataset</p></li>
<li><p>over the region of model complexity with falling training and testing error</p></li>
</ul>
<p>Issues of an underfit machine learning model,</p>
<ul class="simple">
<li><p>more model complexity and flexibility is insufficient given the available data, data accuracy, frequency and coverage</p></li>
<li><p>low accuracy in training and testing representing real-world use away from training data cases, indicating poor ability of the model to generalize</p></li>
</ul>
</section>
<section id="union-of-events-probability">
<h2><strong>Union of Events</strong> (probability)<a class="headerlink" href="#union-of-events-probability" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: the union of outcomes, the probability of <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> is calculated with the probability addition rule,</p>
<div class="math notranslate nohighlight">
\[
P(A \cup B) = P(A) + P(B) - P(A,B)
\]</div>
</section>
<section id="univariate-parameters">
<h2><strong>Univariate Parameters</strong><a class="headerlink" href="#univariate-parameters" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_univariate_analysis.html"><span class="doc std std-doc">Univariate Analysis</span></a>: summary measures based on one feature measured over the population</p>
</section>
<section id="univariate-statistics">
<h2><strong>Univariate Statistics</strong><a class="headerlink" href="#univariate-statistics" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_univariate_analysis.html"><span class="doc std std-doc">Univariate Analysis</span></a>: summary measures based on one feature measured over the samples</p>
</section>
<section id="unsupervised-learning">
<h2><strong>Unsupervised Learning</strong><a class="headerlink" href="#unsupervised-learning" title="Permalink to this heading">#</a></h2>
<p><span class="xref myst">Cluster Analysis</span>: learning patterns in data from unlabeled data.</p>
<ul class="simple">
<li><p>no response features, <span class="math notranslate nohighlight">\(𝑌\)</span>, instead only predictor features, <span class="math notranslate nohighlight">\(𝑋_1,ldots,𝑋_𝑚\)</span></p></li>
<li><p>machine learns by mimicry a compact representation of the data</p></li>
<li><p>captures patterns as feature projections, group assignments, neural network latent features, etc.</p></li>
<li><p>focus on inference of the population, the natural system, instead of prediction of response features</p></li>
</ul>
<p>In this course we use the terms inferential and predictive machine learning, all the covered inferential machine learning methods are unsupervised.</p>
</section>
<section id="variable-also-feature">
<h2><strong>Variable</strong> (also feature)<a class="headerlink" href="#variable-also-feature" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: any property measured or observed in a study, for example,</p>
<ul class="simple">
<li><p>porosity, permeability, mineral concentrations, saturations, contaminant concentration</p></li>
<li><p>in data mining / machine learning this is known as a <em>feature</em></p></li>
<li><p>measure often requires significant analysis, interpretation, etc.</p></li>
</ul>
</section>
<section id="variance-inflation-factor-vif">
<h2><strong>Variance Inflation Factor</strong> (VIF)<a class="headerlink" href="#variance-inflation-factor-vif" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_ranking.html"><span class="doc std std-doc">Feature Ranking</span></a>: a measure of linear multicollinearity between a predictor feature (<span class="math notranslate nohighlight">\(X_i\)</span>) a nd all other predictor features (<span class="math notranslate nohighlight">\(X_j, \forall j \ne i\)</span>).</p>
<p>First we calculate a linear regression for a predictor feature given all the other predictor features.</p>
<div class="math notranslate nohighlight">
\[
X_i = \sum_{j, j \ne i}^m X_j + \epsilon
\]</div>
<p>From this model we determine the coefficient of determination, <span class="math notranslate nohighlight">\(R^2\)</span>, known as variance explained.</p>
<p>Then we calculate the Variance Inflation Factor as:</p>
<div class="math notranslate nohighlight">
\[
VIF = \frac{1}{1 - R^2}
\]</div>
</section>
<section id="volume-variance-relations">
<h2><strong>Volume-Variance Relations</strong><a class="headerlink" href="#volume-variance-relations" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_feature_transformations.html"><span class="doc std std-doc">Feature Transformations</span></a>: as the <em>volume support</em> (scale) increases the variance reduces</p>
<p>Predicting volume-variance relations is central to handling multiple scales of data and models. Some general observations and assumptions,</p>
<ul class="simple">
<li><p>the mean does not change as the volume support, scale changes. Only the variance changes</p></li>
<li><p>there may be shape change (we will not tackle that here). Best practice is to check shape change empirically. It is common to assume no shape change (<em>affine correction</em>) or to use a shape change model (indirect lognormal correction).</p></li>
<li><p>the variance reduction in the distribution is inversely proportional to the range of spatial continuity. Variance reduces faster (over smaller volume increase) for shorter spatial continuity ranges.</p></li>
</ul>
<p>Over common changes in scale this impact may be significant; therefore, it is not appropriate to ignore volume-variance relations,</p>
<ul class="simple">
<li><p>we don’t do this scale up, change in volume support perfectly, and this is why it is still called the missing scale. We rarely have enough data to model this rigorously</p></li>
<li><p>we need a model to predict this change in variance with change in volume support</p></li>
</ul>
<p>There are some change in volume support, scale models,</p>
<p><em>Empirical</em> - build a small scale, high resolution model and scale it up numerically. For example, calculate a high resolution model of permeability, apply flow simulation to calculate effective permeability over <span class="math notranslate nohighlight">\(v\)</span> scale blocks</p>
<p><em>Power Law Averaging</em> - there is a flexible approach known as power law averaging.</p>
<div class="math notranslate nohighlight">
\[
z_V = \left[ \frac{1}{n} \sum z_v^{\omega} \right] ^{\frac{1}{\omega}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\omega\)</span> is the power of averaging. For example:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\omega = 1\)</span> is a regular linear averaging</p></li>
<li><p><span class="math notranslate nohighlight">\(\omega = -1\)</span> is a harmonic averaging</p></li>
<li><p><span class="math notranslate nohighlight">\(\omega = 0\)</span> is a geometric averaging (this is proved in the limit as <span class="math notranslate nohighlight">\(\omega \rightarrow 0\)</span>)</p></li>
</ul>
<p>How to calculate <span class="math notranslate nohighlight">\(\omega\)</span>?</p>
<ul class="simple">
<li><p>for some cases we know from theory the correct <span class="math notranslate nohighlight">\(\omega\)</span> value, for example, for flow orthogonal to beds we select <span class="math notranslate nohighlight">\(\omega = -1.0\)</span> to scale up permeability</p></li>
<li><p>flow simulation may be applied to numerically scale up permeability and then to back-calculate a calibrated <span class="math notranslate nohighlight">\(\omega\)</span></p></li>
</ul>
<p><em>Model</em> - directly adjust the statistics for change in scale. For example, under the assumption of linear averaging and a stationary variogram and variance:</p>
<div class="math notranslate nohighlight">
\[
f = 1 - \frac{\overline{\gamma}(v,v)}{\sigma^2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is variance reduction factor,</p>
<div class="math notranslate nohighlight">
\[
f = \frac{D^2(v,V)}{D^2(\cdot,V)} = \frac{D^2(v,V)}{\sigma^2}
\]</div>
<p>in other words, <span class="math notranslate nohighlight">\(f\)</span> is the ratio of the variance at scale <span class="math notranslate nohighlight">\(v\)</span> to the variance at the original data point support scale based on,</p>
<ul class="simple">
<li><p>the variogram model</p></li>
<li><p>the scale of the data, <span class="math notranslate nohighlight">\(\cdot\)</span> and the scale of <span class="math notranslate nohighlight">\(v\)</span></p></li>
</ul>
</section>
<section id="venn-diagrams">
<h2><strong>Venn Diagrams</strong><a class="headerlink" href="#venn-diagrams" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_probability.html"><span class="doc std std-doc">Probability Concepts</span></a>: a plot, visual tool for communicating probability. What do we learn from a Venn diagram?</p>
<ul class="simple">
<li><p>size of regions <span class="math notranslate nohighlight">\(\propto\)</span> probability of occurrence</p></li>
<li><p>proportion of <span class="math notranslate nohighlight">\(\Omega\)</span>, all possible outcomes represented by a box, i.e., probability of <span class="math notranslate nohighlight">\(1.0\)</span></p></li>
<li><p>overlap <span class="math notranslate nohighlight">\(\propto\)</span> probability of joint occurrence</p></li>
</ul>
<p>Venn diagrams are an excellent tool to visualize marginal, joint and conditional probability.</p>
</section>
<section id="well-log-data">
<h2><strong>Well Log Data</strong><a class="headerlink" href="#well-log-data" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: as a much cheaper method to sample wells that does not interrupt drilling operations, well logs are very common over the wells. Often all wells have various well logs available. For example,</p>
<ul class="simple">
<li><p>gamma ray on pilot vertical wells to assess the locations and quality of shales for targeting (landing) horizontal wells</p></li>
<li><p>neutron porosity to assess location high porosity reservoir sands</p></li>
<li><p>gamma ray in drill holes to map thorium mineralization</p></li>
</ul>
<p>Well log data are critical to support subsurface resource interpretations. Once anchored by core data they provide the essential coverage and resolution to model the entire reservoir concept / framework for prediction, for example,</p>
<ul class="simple">
<li><p>well log data calibrated by core data collocated with well log data are used to map the critical stratigraphic layers, including reservoir and seal units</p></li>
<li><p>well logs are applied to depth correct features inverted from seismic that have location imprecision due to uncertainty in the rock velocity over the volume of interest</p></li>
</ul>
</section>
<section id="weak-learner">
<h2><strong>Weak Learner</strong><a class="headerlink" href="#weak-learner" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_gradient_boosting.html"><span class="doc std std-doc">Gradient Boosting</span></a>: the prediction model performs only marginally better than random</p>
<div class="math notranslate nohighlight">
\[
𝑌 = \hat{f}_𝑘(𝑋_1,\ldots,𝑋_𝑚)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{f}_𝑘\)</span> is the <span class="math notranslate nohighlight">\(𝑘^{th}\)</span> weak learner, <span class="math notranslate nohighlight">\(𝑋_1,\ldots,𝑋_𝑚\)</span> are the predictor features, <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is the prediction of the response feature.</p>
<p>The term weak predictor is often used, and specifically the term weak classifier for the case of classification models.</p>
</section>
<section id="well-log-data-image-logs">
<h2><strong>Well Log Data, Image Logs</strong><a class="headerlink" href="#well-log-data-image-logs" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="MachineLearning_concepts.html"><span class="doc std std-doc">Machine Learning Concepts</span></a>: a special case of <em>well logs</em> where the well logs are repeated at various azimuthal intervals within the well bore resulting in a 2D (unwrapped) image instead of a 1D line along the well bore. For example, Fullbore formation MicroImager (FMI) with:</p>
<ul class="simple">
<li><p>with 80% bore hole coverage</p></li>
<li><p>0.2 inch (0.5 cm) resolution vertical and horizontal</p></li>
<li><p>30 inch (79 cm) depth of investigation</p></li>
</ul>
<p>can be applied to observe lithology change, bed dips and sedimentary structures.</p>
</section>
<section id="comments">
<h2>Comments<a class="headerlink" href="#comments" title="Permalink to this heading">#</a></h2>
<p>This was a basic introduction to geostatistics. If you would like more on these fundamental concepts I recommend the Introduction, Modeling Principles and Modeling Prerequisites chapters from my text book, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a>{cite}`pyrcz2014’.</p>
<p>I hope this is helpful,</p>
<p><em>Michael</em></p>
</section>
<section id="the-author">
<h2>The Author:<a class="headerlink" href="#the-author" title="Permalink to this heading">#</a></h2>
<p>Michael Pyrcz, Professor, The University of Texas at Austin
<em>Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions</em></p>
<p>With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers’ and geoscientists’ impact in subsurface resource development.</p>
<p>For more about Michael check out these links:</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?<a class="headerlink" href="#want-to-work-together" title="Permalink to this heading">#</a></h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I’d be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz&#37;&#52;&#48;austin&#46;utexas&#46;edu">mpyrcz<span>&#64;</span>austin<span>&#46;</span>utexas<span>&#46;</span>edu</a>.</p></li>
</ul>
<p>I’m always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="conclusions.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Conclusions</p>
      </div>
    </a>
    <a class="right-next"
       href="references.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-machine-learning-concepts">Motivation for Machine Learning Concepts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjacency-matrix-spectral-clustering"><strong>Adjacency Matrix</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#addition-rule-probability"><strong>Addition Rule</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#affine-correction"><strong>Affine Correction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#affinity-matrix-spectral-clustering"><strong>Affinity Matrix</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging-models"><strong>Bagging Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-expansion"><strong>Basis Expansion</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-function"><strong>Basis Function</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-probability"><strong>Bayes’ Theorem</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-probability"><strong>Bayesian Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-updating-for-classification"><strong>Bayesian Updating for Classification</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression"><strong>Bayesian Linear Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#big-data"><strong>Big Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#big-data-analytics"><strong>Big Data Analytics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-transform-also-indicator-transform"><strong>Binary Transform</strong> (also Indicator Transform)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boosting-models"><strong>Boosting Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrap"><strong>Bootstrap</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-feature"><strong>Categorical Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-nominal-feature"><strong>Categorical Nominal Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-ordinal-feature"><strong>Categorical Ordinal Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#causation"><strong>Causation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cell-based-declustering"><strong>Cell-based Declustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cognitive-biases"><strong>Cognitive Biases</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complimentary-events-probability"><strong>Complimentary Events</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity"><strong>Computational Complexity</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability"><strong>Conditional Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-interval"><strong>Confidence Interval</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix"><strong>Confusion Matrix</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-feature"><strong>Continuous Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-interval-feature"><strong>Continuous, Interval Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-ratio-feature"><strong>Continuous, Ratio Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuously-differentiable"><strong>Continuously Differentiable</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution"><strong>Convolution</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-data"><strong>Core Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation"><strong>Correlation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance"><strong>Covariance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation"><strong>Cross Validation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-function-cdf"><strong>Cumulative Distribution Function</strong> (CDF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curse-of-dimensionality"><strong>Curse of Dimensionality</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-data-aspects"><strong>Data</strong> (data aspects)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-convexity"><strong>Data Convexity</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataframe"><strong>DataFrame</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-analytics"><strong>Data Analytics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation"><strong>Data Preparation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#degree-matrix-spectral-clustering"><strong>Degree Matrix</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dbscan-for-density-based-clustering"><strong>DBSCAN for Density-based Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-criteria"><strong>Decision Criteria</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree"><strong>Decision Tree</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#declustering"><strong>Declustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#declustering-statistics"><strong>Declustering</strong> (statistics)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#density-connected-dbscan"><strong>Density-Connected</strong> (DBSCAN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#density-based-cluster-dbscan"><strong>Density-based Cluster</strong> (DBSCAN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#density-reachable-dbscan"><strong>Density-Reachable</strong> (DBSCAN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-model"><strong>Deterministic Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction"><strong>Dimensionality Reduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#directly-density-reachable-dbscan"><strong>Directly Density Reachable</strong> (DBSCAN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-feature"><strong>Discrete Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution-transformations"><strong>Distribution Transformations</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eager-learning"><strong>Eager Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation"><strong>Estimation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#f1-score-classification-accuracy-metric"><strong>f1-score</strong> (classification accuracy metric)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-also-variable"><strong>Feature</strong> (also variable)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering"><strong>Feature Engineering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance"><strong>Feature Importance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-imputation"><strong>Feature Imputation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-projection"><strong>Feature Projection</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-space"><strong>Feature Space</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-ranking"><strong>Feature Ranking</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-transformations"><strong>Feature Transformations</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourth-paradigm"><strong>Fourth Paradigm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-probability"><strong>Frequentist Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-anamorphosis"><strong>Gaussian Anamorphosis</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gibbs-sampler-mcmc"><strong>Gibbs Sampler</strong> (MCMC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-boosting-models"><strong>Gradient Boosting Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-laplacian-spectral-clustering"><strong>Graph Laplacian</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geostatistics"><strong>Geostatistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-based-optimization"><strong>Gradient-based Optimization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-spectral-clustering"><strong>Graph</strong> (spectral clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gridded-data"><strong>Gridded Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hard-data"><strong>Hard Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hermite-polynomials"><strong>Hermite Polynomials</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#heuristic-algorithm"><strong>Heuristic Algorithm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-clustering"><strong>Hierarchical Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#histogram"><strong>Histogram</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hybrid-model"><strong>Hybrid Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-probability"><strong>Independence</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indicator-transform-also-binary-transform"><strong>Indicator Transform</strong> (also Binary Transform)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indicator-variogram"><strong>Indicator Variogram</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-inferential-statistics"><strong>Inference, Inferential Statistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inlier"><strong>Inlier</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instance-based-learning"><strong>Instance-based Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intersection-of-events-probability"><strong>Intersection of Events</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#irreducible-error"><strong>Irreducible Error</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inertia-clustering"><strong>Inertia</strong> (clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-probability"><strong>Joint Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-bins-discretization"><strong>K Bins Discretization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation"><strong>K-fold Cross Validation</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering"><strong>k-Means Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbours"><strong>k-Nearest Neighbours</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-trick-support-vector-machines"><strong>Kernel Trick</strong> (support vector machines)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kriging"><strong>Kriging</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kriging-based-declustering"><strong>Kriging-based Declustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kolmogorovs-3-probability-axioms"><strong>Kolmogorov’s 3 Probability Axioms</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-1-norm"><strong><span class="math notranslate nohighlight">\(L^1\)</span> Norm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-2-norm"><strong><span class="math notranslate nohighlight">\(L^2\)</span> Norm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-1-vs-l-2-norm"><strong><span class="math notranslate nohighlight">\(L^1\)</span> vs. <span class="math notranslate nohighlight">\(L^2\)</span> Norm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-1-or-l-2-normalizer"><strong><span class="math notranslate nohighlight">\(L^1\)</span> or <span class="math notranslate nohighlight">\(L^2\)</span> Normalizer</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression"><strong>LASSO Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lazy-learning"><strong>Lazy Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-gradient-boosting"><strong>Learning Rate</strong> (gradient boosting)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likewise-deletion-mrmr"><strong>Likewise Deletion</strong> (MRMR)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression"><strong>Linear Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#location-map"><strong>Location Map</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function"><strong>Loss Function</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-workflow-design"><strong>Machine Learning Workflow Design</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#margin-support-vector-machines"><strong>Margin</strong> (support vector machines)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-probability"><strong>Marginal Probability</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-scatter-plots"><strong>Matrix Scatter Plots</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-relevance-minimum-redundancy-mrmr"><strong>Maximum Relevance Minimum Redundancy</strong> (MRMR)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metropolis-hastings-mcmc-sampler"><strong>Metropolis-Hastings MCMC Sampler</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minkowski-distance"><strong>Minkowski Distance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-feature-values"><strong>Missing Feature Values</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-at-random-mar"><strong>Missing at Random</strong> (MAR)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-bias"><strong>Model Bias</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-bias-variance-trade-off"><strong>Model-Bias Variance Trade-off</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-checking"><strong>Model Checking</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-complexity-or-flexibility"><strong>Model Complexity or Flexibility</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-generalization"><strong>Model Generalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-hyperparameters"><strong>Model Hyperparameters</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parameters"><strong>Model Parameters</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-regularization">Model Regularization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-variance"><strong>Model Variance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum-optimization"><strong>Momentum</strong> (optimization)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo-mcmc"><strong>Markov Chain Monte Carlo</strong> (MCMC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metropolis-hastings-sampling-mcmc"><strong>Metropolis-Hastings Sampling</strong> (MCMC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-simulation-mcs"><strong>Monte Carlo Simulation (MCS)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-simulation-workflow"><strong>Monte Carlo Simulation Workflow</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiplication-rule-probability"><strong>Multiplication Rule</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-information"><strong>Mutual Information</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutually-exclusive-events-probability"><strong>Mutually Exclusive Events</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multidimensional-scaling"><strong>Multidimensional Scaling</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes"><strong>Naive Bayes</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ndarray"><strong>ndarray</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nonparametric-model"><strong>Nonparametric Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#norm"><strong>Norm</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization"><strong>Normalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-histogram"><strong>Normalized Histogram</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding"><strong>One Hot Encoding</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#out-of-bag-sample"><strong>Out-of-Bag Sample</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfit-model"><strong>Overfit Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-statistics"><strong>Parameters</strong> (statistics)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-machine-learning"><strong>Parameters</strong> (machine learning)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-model"><strong>Parametric Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-correlation-coefficient"><strong>Partial Correlation Coefficient</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partitional-clustering"><strong>Partitional Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#polygonal-declustering"><strong>Polygonal Declustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-regression"><strong>Polynomial Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#population"><strong>Population</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#power-law-average"><strong>Power Law Average</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-classification-accuracy-metric"><strong>Precision</strong> (classification accuracy metric)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-interval"><strong>Prediction Interval</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-predictive-statistics"><strong>Prediction, Predictive Statistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-feature"><strong>Predictor Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-feature-space"><strong>Predictor Feature Space</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#primary-data"><strong>Primary Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis"><strong>Principal Component Analysis</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-function-pdf"><strong>Probability Density Function</strong> (PDF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-non-negativity-normalization"><strong>Probability Non-negativity, Normalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-of-acceptance-mcmc"><strong>Probability of Acceptance</strong> (MCMC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-operators"><strong>Probability Operators</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-perspectives"><strong>Probability Perspectives</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prototype-clustering"><strong>Prototype</strong> (clustering)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qualitative-features"><strong>Qualitative Features</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantitative-features"><strong>Quantitative Features</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-2-also-coefficient-of-determination"><strong><span class="math notranslate nohighlight">\(r^2\)</span></strong> (also coefficient of determination)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest"><strong>Random Forest</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#realization"><strong>Realization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#realizations-uncertainty"><strong>Realizations</strong> (uncertainty)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reasons-to-learn-some-coding"><strong>Reasons to Learn Some Coding</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-classification-accuracy-metric"><strong>Recall</strong> (classification accuracy metric)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-feature-elimination"><strong>Recursive Feature Elimination</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reservoir-modeling-workflow"><strong>Reservoir Modeling Workflow</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#response-feature"><strong>Response Feature</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-tikhonov-regularization"><strong>Ridge Regression</strong> (Tikhonov Regularization)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample"><strong>Sample</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenarios-uncertainty"><strong>Scenarios</strong> (uncertainty)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#secondary-data"><strong>Secondary Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seismic-data"><strong>Seismic Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shapley-value"><strong>Shapley Value</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simpson-s-paradox"><strong>Simpson’s Paradox</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#soft-data"><strong>Soft Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-sampling-biased"><strong>Spatial Sampling</strong> (biased)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-sampling-clustered"><strong>Spatial Sampling</strong> (clustered)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-sampling-common-practice"><strong>Spatial Sampling</strong> (common practice)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spatial-sampling-representative"><strong>Spatial Sampling</strong> (representative)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-clustering"><strong>Spectral Clustering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization"><strong>Standardization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-based-optimization"><strong>Stochastic Gradient-based Optimization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-model"><strong>Stochastic Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-practice"><strong>Statistics</strong> (practice)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-measurement"><strong>Statistics</strong> (measurement)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-distribution"><strong>Statistical Distribution</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-support-vector-machines"><strong>Support Vector</strong> (support vector machines)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machines"><strong>Support Vector Machines</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-data"><strong>Tabular Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-testing-splits"><strong>Training and Testing Splits</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-function-reservoir-modeling-workflow"><strong>Transfer Function</strong> (reservoir modeling workflow)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-modeling"><strong>Uncertainty Modeling</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underfit-model"><strong>Underfit Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#union-of-events-probability"><strong>Union of Events</strong> (probability)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-parameters"><strong>Univariate Parameters</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-statistics"><strong>Univariate Statistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning"><strong>Unsupervised Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-also-feature"><strong>Variable</strong> (also feature)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-inflation-factor-vif"><strong>Variance Inflation Factor</strong> (VIF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#volume-variance-relations"><strong>Volume-Variance Relations</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#venn-diagrams"><strong>Venn Diagrams</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#well-log-data"><strong>Well Log Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weak-learner"><strong>Weak Learner</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#well-log-data-image-logs"><strong>Well Log Data, Image Logs</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">Comments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-author">The Author:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-work-together">Want to Work Together?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Michael J. Pyrcz
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024 CC-BY-SA 4.0.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>